<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://longzl2015.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"我们没有找到任何搜索结果: ${query}","hits_stats":"找到约${hits}条结果（用时${time}ms）"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="spark 提交任务后，Linux中java 进程说明[TOC] 一、简述当使用 spark 提交任务后，可在hadoop集群中的一台Linux里 使用 ps -ef | grep jdk1.8 可查看到对应的任务进程信息。 注、本例中使用的是单机模式 二、进程分析123456hadoop   110334 110332  0 13:40 ?        00:00:00 &#x2F;bin&amp;#">
<meta property="og:type" content="article">
<meta property="og:title" content="spark 提交任务后，Linux中java 进程说明">
<meta property="og:url" content="https://longzl2015.github.io/2018/06/04/spark/Spark%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91/spark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E5%90%8E%20linux%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E8%AF%B4%E6%98%8E/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="spark 提交任务后，Linux中java 进程说明[TOC] 一、简述当使用 spark 提交任务后，可在hadoop集群中的一台Linux里 使用 ps -ef | grep jdk1.8 可查看到对应的任务进程信息。 注、本例中使用的是单机模式 二、进程分析123456hadoop   110334 110332  0 13:40 ?        00:00:00 &#x2F;bin&amp;#">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2018-06-04T15:22:09.000Z">
<meta property="article:modified_time" content="2020-02-27T09:16:07.860Z">
<meta property="article:author" content="zhoul">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://longzl2015.github.io/2018/06/04/spark/Spark%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91/spark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E5%90%8E%20linux%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E8%AF%B4%E6%98%8E/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>spark 提交任务后，Linux中java 进程说明 | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">190</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">92</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">351</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2018/06/04/spark/Spark%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91/spark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E5%90%8E%20linux%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E8%AF%B4%E6%98%8E/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          spark 提交任务后，Linux中java 进程说明
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-06-04 23:22:09" itemprop="dateCreated datePublished" datetime="2018-06-04T23:22:09+08:00">2018-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/" itemprop="url" rel="index">
                    <span itemprop="name">源码解析</span>
                  </a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2018/06/04/spark/Spark%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91/spark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E5%90%8E%20linux%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E8%AF%B4%E6%98%8E/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2018/06/04/spark/Spark执行逻辑/spark提交任务后 linux中的线程说明/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="spark-提交任务后，Linux中java-进程说明"><a href="#spark-提交任务后，Linux中java-进程说明" class="headerlink" title="spark 提交任务后，Linux中java 进程说明"></a>spark 提交任务后，Linux中java 进程说明</h1><p>[TOC]</p>
<h2 id="一、简述"><a href="#一、简述" class="headerlink" title="一、简述"></a>一、简述</h2><p>当使用 spark 提交任务后，可在hadoop集群中的一台Linux里 使用 <code>ps -ef | grep jdk1.8</code> 可查看到对应的任务进程信息。</p>
<p>注、本例中使用的是单机模式</p>
<h2 id="二、进程分析"><a href="#二、进程分析" class="headerlink" title="二、进程分析"></a>二、进程分析</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop   110334 110332  0 13:40 ?        00:00:00 &#x2F;bin&#x2F;bash -c .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx1024m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;tmp &#39;-XX:MaxPermSize&#x3D;2048m&#39; &#39;-XX:PermSize&#x3D;512m&#39; -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class &#39;org.apache.spark.ml.alogrithm.SmartRules&#39; --jar hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_zl&#x2F;lib2&#x2F;cmpt&#x2F;xxxxx-workflow-component-0.3.2-20180320-1101.jar --arg &#39;hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_3.5&#x2F;proc&#x2F;1&#x2F;11&#x2F;92&#x2F;submit_SmartRules_37Client.json&#39; --properties-file &#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;__spark_conf__&#x2F;__spark_conf__.properties 1&gt; &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;stdout 2&gt; &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;stderr</span><br><span class="line">hadoop   110891 110334 99 13:40 ?        00:00:34 .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx1024m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;tmp -XX:MaxPermSize&#x3D;2048m -XX:PermSize&#x3D;512m -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class org.apache.spark.ml.alogrithm.SmartRules --jar hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_zl&#x2F;lib2&#x2F;cmpt&#x2F;xxxxx-workflow-component-0.3.2-20180320-1101.jar --arg hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_3.5&#x2F;proc&#x2F;1&#x2F;11&#x2F;92&#x2F;submit_SmartRules_37Client.json --properties-file &#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;__spark_conf__&#x2F;__spark_conf__.properties</span><br><span class="line">hadoop   111013 111010  0 13:40 ?        00:00:00 &#x2F;bin&#x2F;bash -c .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;tmp &#39;-Dspark.ui.port&#x3D;0&#39; &#39;-Dspark.driver.port&#x3D;37011&#39; -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002 -XX:OnOutOfMemoryError&#x3D;&#39;kill %p&#39; org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 1 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;__app__.jar 1&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;stdout 2&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;stderr</span><br><span class="line">hadoop   111567 111013 99 13:40 ?        00:00:32 .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;tmp -Dspark.ui.port&#x3D;0 -Dspark.driver.port&#x3D;37011 -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002 -XX:OnOutOfMemoryError&#x3D;kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 1 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;__app__.jar</span><br><span class="line">hadoop   111619 111616  0 13:40 ?        00:00:00 &#x2F;bin&#x2F;bash -c .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;tmp &#39;-Dspark.ui.port&#x3D;0&#39; &#39;-Dspark.driver.port&#x3D;37011&#39; -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003 -XX:OnOutOfMemoryError&#x3D;&#39;kill %p&#39; org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 2 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;__app__.jar 1&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;stdout 2&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;stderr</span><br><span class="line">hadoop   112178 111619 99 13:40 ?        00:00:50 .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;tmp -Dspark.ui.port&#x3D;0 -Dspark.driver.port&#x3D;37011 -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003 -XX:OnOutOfMemoryError&#x3D;kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 2 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;__app__.jar</span><br></pre></td></tr></table></figure>



<p>下面逐条解释上面java进程： </p>
<h3 id="2-1-进程一"><a href="#2-1-进程一" class="headerlink" title="2.1 进程一"></a>2.1 进程一</h3><p>进程一表示使用bash -c 启动进程二，并将进程二的信息重定向到指定位置。这里直接说重定向命令参数，其他参数见进程二说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop   110334 110332  0 13:40 ?        00:00:00 &#x2F;bin&#x2F;bash -c .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx1024m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;tmp &#39;-XX:MaxPermSize&#x3D;2048m&#39; &#39;-XX:PermSize&#x3D;512m&#39; -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class &#39;org.apache.spark.ml.alogrithm.SmartRules&#39; --jar hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_zl&#x2F;lib2&#x2F;cmpt&#x2F;xxxxx-workflow-component-0.3.2-20180320-1101.jar --arg &#39;hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_3.5&#x2F;proc&#x2F;1&#x2F;11&#x2F;92&#x2F;submit_SmartRules_37Client.json&#39; --properties-file &#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;__spark_conf__&#x2F;__spark_conf__.properties 1&gt; &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;stdout 2&gt; &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;stderr</span><br></pre></td></tr></table></figure>

<h4 id="信息重定向命令"><a href="#信息重定向命令" class="headerlink" title="信息重定向命令"></a>信息重定向命令</h4><blockquote>
<p>1&gt; /home/hadoop/hadoop-2.7.3/logs/userlogs/application_1519271509270_0745/container_1519271509270_0745_01_000001/stdout</p>
<p> 2&gt; /home/hadoop/hadoop-2.7.3/logs/userlogs/application_1519271509270_0745/container_1519271509270_0745_01_000001/stderr</p>
</blockquote>
<h3 id="2-2-进程二：-ApplicationMaster"><a href="#2-2-进程二：-ApplicationMaster" class="headerlink" title="2.2 进程二： ApplicationMaster"></a>2.2 进程二： ApplicationMaster</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop   110891 110334 99 13:40 ?        00:00:34 .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx1024m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;tmp -XX:MaxPermSize&#x3D;2048m -XX:PermSize&#x3D;512m -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class org.apache.spark.ml.alogrithm.SmartRules --jar hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_zl&#x2F;lib2&#x2F;cmpt&#x2F;xxxxx-workflow-component-0.3.2-20180320-1101.jar --arg hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_3.5&#x2F;proc&#x2F;1&#x2F;11&#x2F;92&#x2F;submit_SmartRules_37Client.json --properties-file &#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;__spark_conf__&#x2F;__spark_conf__.properties</span><br></pre></td></tr></table></figure>

<p>该进程有 进程一 产生，参数配置和一近乎相当</p>
<h4 id="1-jdk-8u161-linux-x64-tar-gz-jdk1-8-0-161-bin-java"><a href="#1-jdk-8u161-linux-x64-tar-gz-jdk1-8-0-161-bin-java" class="headerlink" title="1. ./jdk-8u161-linux-x64.tar.gz/jdk1.8.0_161/bin/java"></a>1. ./jdk-8u161-linux-x64.tar.gz/jdk1.8.0_161/bin/java</h4><p>使用 jdk1.8 运行</p>
<h4 id="2-server"><a href="#2-server" class="headerlink" title="2. -server"></a>2. -server</h4><p> java 有2种启动方式 client 和 server 启动方式 ，client模式启动比较快，但运行时性能和内存管理效率不如server模式，通常用于客户端应用程序。相反，server模式启动比client慢，但可获得更高的运行性能。<br>在 windows上，缺省的虚拟机类型为client模式，如果要使用 server模式，就需要在启动虚拟机时加-server参数，以获得更高性能，对服务器端应用，推荐采用server模式，尤其是多个CPU的系统。在 Linux，Solaris上缺省采用server模式。 </p>
<h4 id="3-Xmx1024m"><a href="#3-Xmx1024m" class="headerlink" title="3. -Xmx1024m"></a>3. -Xmx1024m</h4><p>设置虚拟机内存堆的最大可用大小</p>
<h4 id="4-Djava-io-tmpdir"><a href="#4-Djava-io-tmpdir" class="headerlink" title="4. -Djava.io.tmpdir"></a>4. -Djava.io.tmpdir</h4><p>设置java 临时目录 为 <code>/home/hadoop/tmp/nm-local-dir/usercache/xxxxx/appcache/application_1519271509270_0745/container_1519271509270_0745_01_000001/tmp</code></p>
<h4 id="5-XX-MaxPermSize-2048m-XX-PermSize-512m"><a href="#5-XX-MaxPermSize-2048m-XX-PermSize-512m" class="headerlink" title="5. -XX:MaxPermSize=2048m -XX:PermSize=512m"></a>5. -XX:MaxPermSize=2048m -XX:PermSize=512m</h4><p>-XX:PermSize=64M JVM初始分配的非堆内存</p>
<p>-XX:MaxPermSize=128M JVM最大允许分配的非堆内存，按需分配</p>
<h4 id="6-Dspark-yarn-app-container-log-dir"><a href="#6-Dspark-yarn-app-container-log-dir" class="headerlink" title="6. -Dspark.yarn.app.container.log.dir"></a>6. -Dspark.yarn.app.container.log.dir</h4><p>设置容器 日志目录 为 <code>/home/hadoop/hadoop-2.7.3/logs/userlogs/application_1519271509270_0745/container_1519271509270_0745_01_000001</code></p>
<h4 id="7-org-apache-spark-deploy-yarn-ApplicationMaster"><a href="#7-org-apache-spark-deploy-yarn-ApplicationMaster" class="headerlink" title="7. org.apache.spark.deploy.yarn.ApplicationMaster"></a>7. org.apache.spark.deploy.yarn.ApplicationMaster</h4><p>Java 程序入口类</p>
<h4 id="8-–class"><a href="#8-–class" class="headerlink" title="8. –class"></a>8. –class</h4><p>指定spark任务需要执行任务主类 <code>org.apache.spark.ml.alogrithm.SmartRules</code></p>
<h4 id="9-–jar"><a href="#9-–jar" class="headerlink" title="9. –jar"></a>9. –jar</h4><p>指定spark需要的jar包路径  <code>hdfs://slave131:9000/user/mls_zl/lib2/cmpt/xxxxx-workflow-component-0.3.2-20180320-1101.jar</code></p>
<h4 id="10-–arg"><a href="#10-–arg" class="headerlink" title="10. –arg"></a>10. –arg</h4><p>指定spark任务（客户端编写的代码）所需的参数</p>
<p><code>&#39;hdfs://slave131:9000/user/mls_3.5/proc/1/11/92/submit_SmartRules_37Client.json&#39;</code> </p>
<h4 id="11-–properties-file"><a href="#11-–properties-file" class="headerlink" title="11. –properties-file"></a>11. –properties-file</h4><p> <code>/home/hadoop/tmp/nm-local-dir/usercache/xxxxx/appcache/application_1519271509270_0745/container_1519271509270_0745_01_000001/__spark_conf__/__spark_conf__.properties</code> </p>
<h3 id="2-3-进程三"><a href="#2-3-进程三" class="headerlink" title="2.3 进程三"></a>2.3 进程三</h3><p>使用 bash -c 启动 executor 守护进程 即进程四</p>
<p><a href="http://blog.sina.com.cn/s/blog_9ca9623b0102w7p9.html" target="_blank" rel="noopener">spark executor内幕</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop   111013 111010  0 13:40 ?        00:00:00 &#x2F;bin&#x2F;bash -c .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;tmp &#39;-Dspark.ui.port&#x3D;0&#39; &#39;-Dspark.driver.port&#x3D;37011&#39; -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002 -XX:OnOutOfMemoryError&#x3D;&#39;kill %p&#39; org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 1 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;__app__.jar 1&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;stdout 2&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;stderr</span><br></pre></td></tr></table></figure>

<h3 id="2-4-进程四：CoarseGrainedExecutorBackend"><a href="#2-4-进程四：CoarseGrainedExecutorBackend" class="headerlink" title="2.4 进程四：CoarseGrainedExecutorBackend"></a>2.4 进程四：CoarseGrainedExecutorBackend</h3><p>在spark中，executor是负责计算任务的，而CoarseGrainedExecutorBackend 则是负责Executor对象的创建和维护的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop   111567 111013 99 13:40 ?        00:00:32 .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;tmp -Dspark.ui.port&#x3D;0 -Dspark.driver.port&#x3D;37011 -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002 -XX:OnOutOfMemoryError&#x3D;kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 1 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;__app__.jar</span><br></pre></td></tr></table></figure>

<h4 id="1-Dspark-ui-port"><a href="#1-Dspark-ui-port" class="headerlink" title="1. -Dspark.ui.port"></a>1. -Dspark.ui.port</h4><p>0 代表随机选择一个可用的端口</p>
<h4 id="2-Dspark-driver-port"><a href="#2-Dspark-driver-port" class="headerlink" title="2. -Dspark.driver.port"></a>2. -Dspark.driver.port</h4><p>驱动器监听端口号 37011 </p>
<h4 id="3-Dspark-yarn-app-container-log-dir"><a href="#3-Dspark-yarn-app-container-log-dir" class="headerlink" title="3. -Dspark.yarn.app.container.log.dir"></a>3. -Dspark.yarn.app.container.log.dir</h4><p>指定app.container 的日志位置：<code>/home/hadoop/hadoop-2.7.3/logs/userlogs/application_1519271509270_0745/container_1519271509270_0745_01_000002</code></p>
<h4 id="4-XX-OnOutOfMemoryError-kill-p"><a href="#4-XX-OnOutOfMemoryError-kill-p" class="headerlink" title="4. -XX:OnOutOfMemoryError=kill %p"></a>4. -XX:OnOutOfMemoryError=kill %p</h4><p>出现OutOfMemoryError 时，启动 运行kill命令</p>
<h4 id="5-org-apache-spark-executor-CoarseGrainedExecutorBackend"><a href="#5-org-apache-spark-executor-CoarseGrainedExecutorBackend" class="headerlink" title="5. org.apache.spark.executor.CoarseGrainedExecutorBackend"></a>5. org.apache.spark.executor.CoarseGrainedExecutorBackend</h4><p>java 命令 主类</p>
<h4 id="6-–driver-url"><a href="#6-–driver-url" class="headerlink" title="6. –driver-url"></a>6. –driver-url</h4><p> <code>spark://CoarseGrainedScheduler@10.100.1.131:37011</code></p>
<p>指定CoarseGrainedScheduler对外暴露的url</p>
<h4 id="7-–executor-id"><a href="#7-–executor-id" class="headerlink" title="7. –executor-id"></a>7. –executor-id</h4><p>执行器的id，1</p>
<h4 id="8-–hostname"><a href="#8-–hostname" class="headerlink" title="8. –hostname"></a>8. –hostname</h4><p>主机名 slave131,谁的主机名，待查？</p>
<h4 id="9-–cores-8"><a href="#9-–cores-8" class="headerlink" title="9. –cores 8"></a>9. –cores 8</h4><p>执行核心数</p>
<h4 id="10-–app-id"><a href="#10-–app-id" class="headerlink" title="10. –app-id"></a>10. –app-id</h4><p>应用的id application_1519271509270_0745</p>
<p>由时间戳加id组成。</p>
<h4 id="–user-class-path"><a href="#–user-class-path" class="headerlink" title="–user-class-path"></a>–user-class-path</h4><p><code>file:/home/hadoop/tmp/nm-local-dir/usercache/xxxxx/appcache/application_1519271509270_0745/container_1519271509270_0745_01_000002/__app__.jar</code></p>
<h3 id="2-5-进程五"><a href="#2-5-进程五" class="headerlink" title="2.5 进程五"></a>2.5 进程五</h3><p>使用 bash -c 启动 进程五</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop   111619 111616  0 13:40 ?        00:00:00 &#x2F;bin&#x2F;bash -c .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;tmp &#39;-Dspark.ui.port&#x3D;0&#39; &#39;-Dspark.driver.port&#x3D;37011&#39; -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003 -XX:OnOutOfMemoryError&#x3D;&#39;kill %p&#39; org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 2 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;__app__.jar 1&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;stdout 2&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;stderr</span><br></pre></td></tr></table></figure>

<h3 id="2-6-进程六：CoarseGrainedExecutorBackend"><a href="#2-6-进程六：CoarseGrainedExecutorBackend" class="headerlink" title="2.6 进程六：CoarseGrainedExecutorBackend"></a>2.6 进程六：CoarseGrainedExecutorBackend</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop   112178 111619 99 13:40 ?        00:00:50 .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;tmp -Dspark.ui.port&#x3D;0 -Dspark.driver.port&#x3D;37011 -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003 -XX:OnOutOfMemoryError&#x3D;kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 2 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;xxxxx&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;__app__.jar</span><br></pre></td></tr></table></figure>

<h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>从上面的结果分析，提交某任务后，spark 启动的3个进程：一个 ApplicationMaster、两个CoarseGrainedExecutorBackend。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"># spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/06/04/spark/Spark%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91/8_Executor%20%E6%89%A7%E8%A1%8Ctask%E5%B9%B6%E8%BF%94%E5%9B%9E%E7%BB%93%E6%9E%9C/" rel="prev" title="spark8-executor执行task比返回结果">
      <i class="fa fa-chevron-left"></i> spark8-executor执行task比返回结果
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/06/04/spark/Spark%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91/SparkContext%E5%88%9D%E5%A7%8B%E5%8C%96%E8%BF%87%E7%A8%8B/" rel="next" title="SparkContext初始化过程">
      SparkContext初始化过程 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#spark-提交任务后，Linux中java-进程说明"><span class="nav-number">1.</span> <span class="nav-text">spark 提交任务后，Linux中java 进程说明</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、简述"><span class="nav-number">1.1.</span> <span class="nav-text">一、简述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、进程分析"><span class="nav-number">1.2.</span> <span class="nav-text">二、进程分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-进程一"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 进程一</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#信息重定向命令"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">信息重定向命令</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-进程二：-ApplicationMaster"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 进程二： ApplicationMaster</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-jdk-8u161-linux-x64-tar-gz-jdk1-8-0-161-bin-java"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">1. .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-server"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">2. -server</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-Xmx1024m"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">3. -Xmx1024m</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-Djava-io-tmpdir"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">4. -Djava.io.tmpdir</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-XX-MaxPermSize-2048m-XX-PermSize-512m"><span class="nav-number">1.2.2.5.</span> <span class="nav-text">5. -XX:MaxPermSize&#x3D;2048m -XX:PermSize&#x3D;512m</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-Dspark-yarn-app-container-log-dir"><span class="nav-number">1.2.2.6.</span> <span class="nav-text">6. -Dspark.yarn.app.container.log.dir</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-org-apache-spark-deploy-yarn-ApplicationMaster"><span class="nav-number">1.2.2.7.</span> <span class="nav-text">7. org.apache.spark.deploy.yarn.ApplicationMaster</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-–class"><span class="nav-number">1.2.2.8.</span> <span class="nav-text">8. –class</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-–jar"><span class="nav-number">1.2.2.9.</span> <span class="nav-text">9. –jar</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-–arg"><span class="nav-number">1.2.2.10.</span> <span class="nav-text">10. –arg</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-–properties-file"><span class="nav-number">1.2.2.11.</span> <span class="nav-text">11. –properties-file</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-进程三"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 进程三</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-进程四：CoarseGrainedExecutorBackend"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 进程四：CoarseGrainedExecutorBackend</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Dspark-ui-port"><span class="nav-number">1.2.4.1.</span> <span class="nav-text">1. -Dspark.ui.port</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Dspark-driver-port"><span class="nav-number">1.2.4.2.</span> <span class="nav-text">2. -Dspark.driver.port</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-Dspark-yarn-app-container-log-dir"><span class="nav-number">1.2.4.3.</span> <span class="nav-text">3. -Dspark.yarn.app.container.log.dir</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-XX-OnOutOfMemoryError-kill-p"><span class="nav-number">1.2.4.4.</span> <span class="nav-text">4. -XX:OnOutOfMemoryError&#x3D;kill %p</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-org-apache-spark-executor-CoarseGrainedExecutorBackend"><span class="nav-number">1.2.4.5.</span> <span class="nav-text">5. org.apache.spark.executor.CoarseGrainedExecutorBackend</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-–driver-url"><span class="nav-number">1.2.4.6.</span> <span class="nav-text">6. –driver-url</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-–executor-id"><span class="nav-number">1.2.4.7.</span> <span class="nav-text">7. –executor-id</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-–hostname"><span class="nav-number">1.2.4.8.</span> <span class="nav-text">8. –hostname</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-–cores-8"><span class="nav-number">1.2.4.9.</span> <span class="nav-text">9. –cores 8</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-–app-id"><span class="nav-number">1.2.4.10.</span> <span class="nav-text">10. –app-id</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#–user-class-path"><span class="nav-number">1.2.4.11.</span> <span class="nav-text">–user-class-path</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-进程五"><span class="nav-number">1.2.5.</span> <span class="nav-text">2.5 进程五</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-进程六：CoarseGrainedExecutorBackend"><span class="nav-number">1.2.6.</span> <span class="nav-text">2.6 进程六：CoarseGrainedExecutorBackend</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、总结"><span class="nav-number">1.3.</span> <span class="nav-text">三、总结</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhoul</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">351</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">92</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">190</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/longzl2015" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;longzl2015" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:289570126@qq.com" title="E-Mail → mailto:289570126@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/5276366/egg" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;5276366&#x2F;egg" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhoul</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://long12356-gitee-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: {page: {
            url: "https://longzl2015.github.io/2018/06/04/spark/Spark%E6%89%A7%E8%A1%8C%E9%80%BB%E8%BE%91/spark%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1%E5%90%8E%20linux%E4%B8%AD%E7%9A%84%E7%BA%BF%E7%A8%8B%E8%AF%B4%E6%98%8E/",
            identifier: "2018/06/04/spark/Spark执行逻辑/spark提交任务后 linux中的线程说明/",
            title: "spark 提交任务后，Linux中java 进程说明"
          }
        }
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://long12356-gitee-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
