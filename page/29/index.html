<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://longzl2015.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"我们没有找到任何搜索结果: ${query}","hits_stats":"找到约${hits}条结果（用时${time}ms）"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://longzl2015.github.io/page/29/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="zhoul">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://longzl2015.github.io/page/29/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">190</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">92</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">351</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/Spark%E7%A8%8B%E5%BA%8F%E6%8C%87%E5%AE%9AJAVA_HOME/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/Spark%E7%A8%8B%E5%BA%8F%E6%8C%87%E5%AE%9AJAVA_HOME/" class="post-title-link" itemprop="url">在Spark程序里面指定jdk版本</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" itemprop="url" rel="index">
                    <span itemprop="name">环境配置</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/Spark%E7%A8%8B%E5%BA%8F%E6%8C%87%E5%AE%9AJAVA_HOME/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/环境配置/Spark程序指定JAVA_HOME/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="在Spark程序里面指定jdk版本"><a href="#在Spark程序里面指定jdk版本" class="headerlink" title="在Spark程序里面指定jdk版本"></a>在Spark程序里面指定jdk版本</h1><p>[TOC]</p>
<p>在使用spark集群时经常会出现一种情况：集群的JDK版本为1.7，但是自己编写并编译的程序是用1.8版本的jdk。这种程序提交到spark中就会出现</p>
<blockquote>
<p>Unsupported major.minor version 52.0</p>
</blockquote>
<p>为了解决这种情况，可以通过设置相应参数让spark集群调用对应版本的jdk执行对应的程序。</p>
<h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>在每一个集群节点上安装好相应的jdk版本，这里的安装是指将jdk压缩包解压到某个目录。</p>
<p><strong>注</strong>：该方法的缺点是必须在每台节点上安装jdk。为了克服该方法可以查看另一篇文章《Spark无管理权限配置Python或JDK》</p>
<h2 id="在命令行显示调用"><a href="#在命令行显示调用" class="headerlink" title="在命令行显示调用"></a>在命令行显示调用</h2><p>使用<code>spark.executorEnv.JAVA_HOME</code> 和 <code>spark.yarn.appMasterEnv.JAVA_HOME</code>为spark的executor和driver执行jdk路径：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">SPARK_HOME/bin/spark-submit --master yarn-cluster  \</span></span><br><span class="line">        --executor-memory 8g \</span><br><span class="line">        --num-executors 80 \</span><br><span class="line">        --queue eggzl \</span><br><span class="line">        --conf "spark.yarn.appMasterEnv.JAVA_HOME=/home/user/java/jdk1.8.0_25" \</span><br><span class="line">        --conf "spark.executorEnv.JAVA_HOME=/home/user/java/jdk1.8.0_25" \</span><br><span class="line">        --executor-cores 1 \</span><br><span class="line">        --class com.xx.xx \</span><br><span class="line">        /home/user/spark/app.jar</span><br></pre></td></tr></table></figure>

<h2 id="使用spark-default-conf"><a href="#使用spark-default-conf" class="headerlink" title="使用spark_default.conf"></a>使用spark_default.conf</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.appMasterEnv.JAVA_HOME  &#x2F;home&#x2F;user&#x2F;java&#x2F;jdk1.8.0_25</span><br><span class="line">spark.executorEnv.JAVA_HOME        &#x2F;home&#x2F;user&#x2F;java&#x2F;jdk1.8.0_25</span><br></pre></td></tr></table></figure>

<p>其他的环境变量也可以通过该配置文件获取，比如 <code>spark.executorEnv.MY_BLOG=www.zzz.com</code> 这样我们就可以从程序里面获取 <code>MY_BLOG</code> 环境变量的值。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/spark_history%E6%9C%8D%E5%8A%A1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/spark_history%E6%9C%8D%E5%8A%A1/" class="post-title-link" itemprop="url">spark_history服务</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" itemprop="url" rel="index">
                    <span itemprop="name">环境配置</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/spark_history%E6%9C%8D%E5%8A%A1/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/环境配置/spark_history服务/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在运行Spark应用程序的时候，driver会提供一个webUI给出应用程序的运行信息，但是该webUI随着应用程序的完成而关闭端口，也就是说，Spark应用程序运行完后，将无法查看应用程序的历史记录。Spark history server就是为了应对这种情况而产生的，通过配置，Spark应用程序在运行完应用程序之后，将应用程序的运行信息写入指定目录，而Spark history server可以将这些运行信息装载并以web的方式供用户浏览。</p>
<p>要使用history server，对于提交应用程序的客户端需要配置以下参数（在conf/spark-defaults.conf中配置）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled  true </span><br><span class="line">spark.eventLog.dir      hdfs:&#x2F;&#x2F;hadoop1:8000&#x2F;sparklogs</span><br></pre></td></tr></table></figure>

<p>进入$SPARK_HOME/sbin路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;start-all.sh</span><br><span class="line">.&#x2F;start-history-server.sh</span><br></pre></td></tr></table></figure>

<p>注意：会启动失败，控制台显示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop@Node4:&#x2F;usr&#x2F;local&#x2F;SPARK&#x2F;spark-1.1.0-bin-hadoop2.4&#x2F;sbin$ .&#x2F;start-history-server.sh </span><br><span class="line">starting org.apache.spark.deploy.history.HistoryServer, logging to &#x2F;usr&#x2F;local&#x2F;SPARK&#x2F;spark-1.1.0-bin-hadoop2.4&#x2F;sbin&#x2F;..&#x2F;logs&#x2F;spark-hadoop-org.apache.spark.deploy.history.HistoryServer-1-Node4.out</span><br><span class="line">failed to launch org.apache.spark.deploy.history.HistoryServer:</span><br><span class="line">      at org.apache.spark.deploy.history.FsHistoryProvider.&lt;init&gt;(FsHistoryProvider.scala:41)</span><br><span class="line">      ... 6 more</span><br><span class="line">full log in &#x2F;usr&#x2F;local&#x2F;SPARK&#x2F;spark-1.1.0-bin-hadoop2.4&#x2F;sbin&#x2F;..&#x2F;logs&#x2F;spark-hadoop-org.apache.spark.deploy.history.HistoryServer-1-Node4.out</span><br></pre></td></tr></table></figure>

<p>找到日志文件，发现报错 Logging directory must be specified<br><strong>解决</strong>：在启动historyserver的时候需要加上参数，指明log的存放位置，例如，我们在spark-default.conf中配置的存放路径为hdfs://hadoop1:8000/sparklogs<br>有下面两种方法解决问题<br><strong>1. 将启动命令改成</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-history-server.sh hdfs:&#x2F;&#x2F;node4:9000&#x2F;directory</span><br></pre></td></tr></table></figure>

<p><strong>2. 启动命令不变，在conf/spark-env.sh中添加</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HISTORY_OPTS&#x3D;&quot;-Dspark.history.ui.port&#x3D;18080 -Dspark.history.retainedApplications&#x3D;3 -Dspark.history.fs.logDirectory&#x3D;hdfs:&#x2F;&#x2F;node4:9000&#x2F;directory&quot;</span><br></pre></td></tr></table></figure>

<p>这样，在启动HistoryServer之后，在浏览器中打开<code>http://node4:18080</code>就可以看到web页面了</p>
<p><strong>附：在conf/spark-defaults.conf中配置参数</strong></p>
<p><strong>history server相关的配置参数描述</strong></p>
<p><strong>1） spark.history.updateInterval</strong><br>　　默认值：10<br>　　以秒为单位，更新日志相关信息的时间间隔</p>
<p><strong>2）spark.history.retainedApplications</strong><br>　　默认值：50<br>　　在内存中保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，当再次访问已被删除的应用信息时需要重新构建页面。</p>
<p><strong>3）spark.history.ui.port</strong><br>　　默认值：18080<br>　　HistoryServer的web端口</p>
<p><strong>4）spark.history.kerberos.enabled</strong><br>　　默认值：false<br>　　是否使用kerberos方式登录访问HistoryServer，对于持久层位于安全集群的HDFS上是有用的，如果设置为true，就要配置下面的两个属性</p>
<p><strong>5）spark.history.kerberos.principal</strong><br>　　默认值：用于HistoryServer的kerberos主体名称</p>
<p><strong>6）spark.history.kerberos.keytab</strong><br>　　用于HistoryServer的kerberos keytab文件位置</p>
<p><strong>7）spark.history.ui.acls.enable</strong><br>　　默认值：false<br>　　授权用户查看应用程序信息的时候是否检查acl。如果启用，只有应用程序所有者和spark.ui.view.acls指定的用户可以查看应用程序信息;否则，不做任何检查</p>
<p><strong>8）spark.eventLog.enabled</strong><br>　　默认值：false<br>　　是否记录Spark事件，用于应用程序在完成后重构webUI</p>
<p><strong>9）spark.eventLog.dir</strong><br>　　默认值：file:///tmp/spark-events<br>　　保存日志相关信息的路径，可以是hdfs://开头的HDFS路径，也可以是file://开头的本地路径，都需要提前创建</p>
<p>10）*<em>spark.eventLog.compress *</em><br>　　默认值：false<br>　　是否压缩记录Spark事件，前提spark.eventLog.enabled为true，默认使用的是snappy</p>
<p><strong>以spark.history开头的需要配置在spark-env.sh中的SPARK_HISTORY_OPTS，以spark.eventLog开头的配置在spark-defaults.conf</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/spark%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/spark%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">spark参数介绍</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" itemprop="url" rel="index">
                    <span itemprop="name">环境配置</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/spark%E5%8F%82%E6%95%B0%E4%BB%8B%E7%BB%8D/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/环境配置/spark参数介绍/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="spark参数介绍"><a href="#spark参数介绍" class="headerlink" title="spark参数介绍"></a>spark参数介绍</h1><p>[TOC]</p>
<h3 id="1-spark-driver-memory"><a href="#1-spark-driver-memory" class="headerlink" title="1 spark.driver.memory"></a>1 spark.driver.memory</h3><p>设置分配给spark driver程序的内存，driver端会运行DAGScheduler、Backend等等进程。</p>
<h3 id="2-spark-executor-cores"><a href="#2-spark-executor-cores" class="headerlink" title="2 spark.executor.cores"></a>2 spark.executor.cores</h3><p>单个Executor使用的CPU核数。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</p>
<h3 id="3-spark-executor-memory"><a href="#3-spark-executor-memory" class="headerlink" title="3 spark.executor.memory"></a>3 spark.executor.memory</h3><p>设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</p>
<h3 id="4-spark-executor-instances"><a href="#4-spark-executor-instances" class="headerlink" title="4 spark.executor.instances"></a>4 spark.executor.instances</h3><p>该参数对于静态分配，表示Executor的数量。</p>
<p>如果启用了<code>spark.dynamicAllocation.enabled</code>，则表示Executors的初始大小。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/spark%E6%97%A0%E7%AE%A1%E7%90%86%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AEpython%E5%92%8Cjdk/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/spark%E6%97%A0%E7%AE%A1%E7%90%86%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AEpython%E5%92%8Cjdk/" class="post-title-link" itemprop="url">Spark无管理权限配置Python或JDK</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" itemprop="url" rel="index">
                    <span itemprop="name">环境配置</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/spark%E6%97%A0%E7%AE%A1%E7%90%86%E6%9D%83%E9%99%90%E9%85%8D%E7%BD%AEpython%E5%92%8Cjdk/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/环境配置/spark无管理权限配置python和jdk/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark无管理权限配置Python或JDK"><a href="#Spark无管理权限配置Python或JDK" class="headerlink" title="Spark无管理权限配置Python或JDK"></a>Spark无管理权限配置Python或JDK</h1><p>[TOC]</p>
<h2 id="部署java"><a href="#部署java" class="headerlink" title="部署java"></a>部署java</h2><p>将对应的jdk文件夹压缩成tar.gz</p>
<blockquote>
<p>tar -zcf jdk1.8.0_77.tar.gz jdk1.8.0_77</p>
</blockquote>
<p>然后在提交任务的时候添加某些参数：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">SPARK_HOME/bin/spark-submit --master yarn-cluster  \</span></span><br><span class="line"> --conf "spark.yarn.dist.archives=/home/zzz/jdk1.8.0_77.tar.gz"  \</span><br><span class="line"> --conf "spark.executorEnv.JAVA_HOME=./jdk1.8.0_77.tar.gz/jdk1.8.0_77"\</span><br><span class="line"> --conf "spark.yarn.appMasterEnv.JAVA_HOME=./jdk1.8.0_77.tar.gz/jdk1.8.0_77" \</span><br><span class="line"> xxxxx</span><br></pre></td></tr></table></figure>

<ul>
<li>spark.yarn.dist.archives：逗号分隔的压缩包。运行时，会将这些压缩包分发到所有的executor节点的工作目录，executor节点会自动解压这些压缩包。路径规则参考上面的命令。</li>
</ul>
<p>运行结果《在hadoop集群的任一台机器上执行 ps -ef | grep jdk1.8》</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop   110334 110332  0 13:40 ?        00:00:00 &#x2F;bin&#x2F;bash -c .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx1024m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;tmp &#39;-XX:MaxPermSize&#x3D;2048m&#39; &#39;-XX:PermSize&#x3D;512m&#39; -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class &#39;org.apache.spark.ml.alogrithm.SmartRules&#39; --jar hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_zl&#x2F;lib2&#x2F;cmpt&#x2F;zl-workflow-component-0.3.2-20180320-1101.jar --arg &#39;hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_3.5&#x2F;proc&#x2F;1&#x2F;11&#x2F;92&#x2F;submit_SmartRules_37Client.json&#39; --properties-file &#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;__spark_conf__&#x2F;__spark_conf__.properties 1&gt; &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;stdout 2&gt; &#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;stderr</span><br><span class="line">hadoop   110891 110334 99 13:40 ?        00:00:34 .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx1024m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;tmp -XX:MaxPermSize&#x3D;2048m -XX:PermSize&#x3D;512m -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001 org.apache.spark.deploy.yarn.ApplicationMaster --class org.apache.spark.ml.alogrithm.SmartRules --jar hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_zl&#x2F;lib2&#x2F;cmpt&#x2F;zl-workflow-component-0.3.2-20180320-1101.jar --arg hdfs:&#x2F;&#x2F;slave131:9000&#x2F;user&#x2F;mls_3.5&#x2F;proc&#x2F;1&#x2F;11&#x2F;92&#x2F;submit_SmartRules_37Client.json --properties-file &#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000001&#x2F;__spark_conf__&#x2F;__spark_conf__.properties</span><br><span class="line">hadoop   111013 111010  0 13:40 ?        00:00:00 &#x2F;bin&#x2F;bash -c .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;tmp &#39;-Dspark.ui.port&#x3D;0&#39; &#39;-Dspark.driver.port&#x3D;37011&#39; -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002 -XX:OnOutOfMemoryError&#x3D;&#39;kill %p&#39; org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 1 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;__app__.jar 1&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;stdout 2&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;stderr</span><br><span class="line">hadoop   111567 111013 99 13:40 ?        00:00:32 .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;tmp -Dspark.ui.port&#x3D;0 -Dspark.driver.port&#x3D;37011 -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002 -XX:OnOutOfMemoryError&#x3D;kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 1 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000002&#x2F;__app__.jar</span><br><span class="line">hadoop   111619 111616  0 13:40 ?        00:00:00 &#x2F;bin&#x2F;bash -c .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;tmp &#39;-Dspark.ui.port&#x3D;0&#39; &#39;-Dspark.driver.port&#x3D;37011&#39; -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003 -XX:OnOutOfMemoryError&#x3D;&#39;kill %p&#39; org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 2 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;__app__.jar 1&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;stdout 2&gt;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;stderr</span><br><span class="line">hadoop   112178 111619 99 13:40 ?        00:00:50 .&#x2F;jdk-8u161-linux-x64.tar.gz&#x2F;jdk1.8.0_161&#x2F;bin&#x2F;java -server -Xmx4096m -Djava.io.tmpdir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;tmp -Dspark.ui.port&#x3D;0 -Dspark.driver.port&#x3D;37011 -Dspark.yarn.app.container.log.dir&#x3D;&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.7.3&#x2F;logs&#x2F;userlogs&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003 -XX:OnOutOfMemoryError&#x3D;kill %p org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark:&#x2F;&#x2F;CoarseGrainedScheduler@10.100.1.131:37011 --executor-id 2 --hostname slave131 --cores 8 --app-id application_1519271509270_0745 --user-class-path file:&#x2F;home&#x2F;hadoop&#x2F;tmp&#x2F;nm-local-dir&#x2F;usercache&#x2F;zhoulong&#x2F;appcache&#x2F;application_1519271509270_0745&#x2F;container_1519271509270_0745_01_000003&#x2F;__app__.jar</span><br></pre></td></tr></table></figure>



<h2 id="部署python"><a href="#部署python" class="headerlink" title="部署python"></a>部署python</h2><p>将对应的python压缩成tar.gz</p>
<blockquote>
<p>$ tar -zcf anaconda2.tar.gz anaconda2</p>
</blockquote>
<p>然后操作如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export PYSPARK_PYTHON=./anaconda2.tar.gz/anaconda2/bin/python</span><br><span class="line"><span class="meta">$</span><span class="bash">SPARK_HOME/bin/pyspark </span></span><br><span class="line"> --conf "spark.yarn.dist.archives=/home/iteblog/anaconda2.tar.gz" \    </span><br><span class="line"> --conf "spark.executorEnv.PYSPARK_PYTHON=./anaconda2.tar.gz/anaconda2/bin/python"\    </span><br><span class="line"> --master yarn-client</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/cache%E5%92%8Cpersist%20%E5%8C%BA%E5%88%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/cache%E5%92%8Cpersist%20%E5%8C%BA%E5%88%AB/" class="post-title-link" itemprop="url">缓存方法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/" itemprop="url" rel="index">
                    <span itemprop="name">语法解释</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/cache%E5%92%8Cpersist%20%E5%8C%BA%E5%88%AB/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/语法解释/cache和persist 区别/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="缓存方法"><a href="#缓存方法" class="headerlink" title="缓存方法"></a>缓存方法</h1><p>[TOC]</p>
<p>来源: <a href="http://blog.csdn.net/houmou/article/details/52491419" target="_blank" rel="noopener">spark中cache和persist的区别</a></p>
<h2 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h2><p>在spark中，cache和persist都是用于将一个RDD进行缓存的，这样在之后的使用过程中就不需要重新进行计算了，可以大大节省程序运行的时间。</p>
<h2 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h2><p>两者的区别在于：cache 其实是调用了 persist 方法，缓存策略为 MEMORY_ONLY。而 persist 可以通过设置参数有多种缓存策略。</p>
<h2 id="persist12种策略"><a href="#persist12种策略" class="headerlink" title="persist12种策略"></a>persist12种策略</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>)</span><br></pre></td></tr></table></figure>

<p>这12种策略都是由5个参数决定的。StorageLevel 构造函数源码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useDisk: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useMemory: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useOffHeap: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _deserialized: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _replication: <span class="type">Int</span> = 1</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Externalizable</span> </span>&#123;</span><br><span class="line">  ......</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useDisk</span></span>: <span class="type">Boolean</span> = _useDisk</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useMemory</span></span>: <span class="type">Boolean</span> = _useMemory</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">useOffHeap</span></span>: <span class="type">Boolean</span> = _useOffHeap</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deserialized</span></span>: <span class="type">Boolean</span> = _deserialized</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">replication</span></span>: <span class="type">Int</span> = _replication</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>可以看到StorageLevel类的主构造器包含了5个参数：</p>
<ul>
<li>useDisk：使用硬盘（外存）</li>
<li>useMemory：使用内存</li>
<li>useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</li>
<li>deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象</li>
<li>replication：备份数（在多个节点上备份）</li>
</ul>
<p>另外还注意到<code>OFF_HEAP</code>这种策略</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val OFF_HEAP &#x3D; new StorageLevel(false, false, true, false)</span><br></pre></td></tr></table></figure>

<p>使用了堆外内存，它不能和其它几个参数共存。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (useOffHeap) &#123;</span><br><span class="line">  require(!useDisk, <span class="string">"Off-heap storage level does not support using disk"</span>)</span><br><span class="line">  require(!useMemory, <span class="string">"Off-heap storage level does not support using heap memory"</span>)</span><br><span class="line">  require(!deserialized, <span class="string">"Off-heap storage level does not support deserialized storage"</span>)</span><br><span class="line">  require(replication == <span class="number">1</span>, <span class="string">"Off-heap storage level does not support multiple replication"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/" class="post-title-link" itemprop="url">共享变量 -广播变量与累加器（转）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/" itemprop="url" rel="index">
                    <span itemprop="name">语法解释</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/语法解释/共享变量/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="共享变量-广播变量与累加器（转）"><a href="#共享变量-广播变量与累加器（转）" class="headerlink" title="共享变量 -广播变量与累加器（转）"></a>共享变量 -广播变量与累加器（转）</h1><p>[TOC]</p>
<p>来源: <a href="https://andone1cc.github.io/2017/03/02/Spark/%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%E4%B8%8E%E7%B4%AF%E5%8A%A0%E5%99%A8/" target="_blank" rel="noopener">Spark学习笔记——广播变量与累加器</a></p>
<h3 id="一、Spark-广播变量"><a href="#一、Spark-广播变量" class="headerlink" title="一、Spark 广播变量"></a>一、Spark 广播变量</h3><h4 id="1-引入"><a href="#1-引入" class="headerlink" title="1.引入"></a><strong>1.引入</strong></h4><p>我们声明定义的变量是在Driver中产生,算子中的匿名函数是在Executor中执行的。也就是如果在Driver中定义的变量最终是要发送到task中去,task需要引用executor中线程池执行,而Executor是一个jvm进程，变量副本过多会占用jvm过多的堆内存，会引起频繁的GC、OOM。如果不使用广播变量，那么有多少个task就会在集群中有多少个变量副本。所以为了解决变量占用内存的问题，我们直接在executor层面保存一份变量即可。不用给每一个task都保存一份变量，只需要保存executor的个数那么多个。</p>
<h4 id="2-广播变量的原理"><a href="#2-广播变量的原理" class="headerlink" title="2.广播变量的原理"></a><strong>2.广播变量的原理</strong></h4><p>task在执行的时候如果使用到了广播变量，它会找本地管理广播变量的组件(<code>BlockManager</code>)去要，如果本地的BlockManager中没有广播变量，BlockManager会去Driver端(有一个BlockManagerMaster组件)去拉取广播变量。</p>
<p>广播变量不是Driver主动发给executor的，而是等到哪个task执行使用到了广播变量，根据需要去取，免得浪费资源。</p>
<h4 id="3-使用流程"><a href="#3-使用流程" class="headerlink" title="3.使用流程"></a><strong>3.使用流程</strong></h4><p>Driver端：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val broadcast&#x3D;sc.broatcast(变量) 广播的变量可以是基本类型和集合</span><br></pre></td></tr></table></figure>

<p>Executor端：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">broadcast.value</span><br></pre></td></tr></table></figure>

<p>【注意事项】</p>
<p>1.广播变量只能在Driver端定义，不能在Executor端定义。</p>
<p>2.广播变量在Executor端不能修改。</p>
<h4 id="4-广播变量-map实现join算子"><a href="#4-广播变量-map实现join算子" class="headerlink" title="4.广播变量+map实现join算子"></a><strong>4.广播变量+map实现join算子</strong></h4><p>因为join算子会产生shuffle,shuffle过程会有数据的移动,数据的读写I/O,占用过多的资源。所以我们在编写程序时尽量避免使用shuffle类的算子。</p>
<p>使用广播变量+map 实现join</p>
<p>适用场景：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">一个RDD的数据量比较大，一个RDD的数据量比较小，适合用这种方式来取代join.</span><br><span class="line">如果两个RDD的数据量都特别的大，那么会造成executor进程的OOM.</span><br></pre></td></tr></table></figure>

<p><strong>[代码演示]</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestBroadCast</span> </span>&#123;</span><br><span class="line"> 	<span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">     	SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setAppName(<span class="string">"BroadCast"</span>)</span><br><span class="line">                .setMaster(<span class="string">"local"</span>)</span><br><span class="line">                .set(<span class="string">"spark.testing.memory"</span>, <span class="string">"2147480000"</span>);</span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        List&lt;Tuple2&lt;String, String&gt;&gt; nameList = Arrays.asList(</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="string">"1"</span>, <span class="string">"zhangsan"</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="string">"2"</span>, <span class="string">"lisi"</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="string">"3"</span>, <span class="string">"wangwu"</span>)</span><br><span class="line">        );</span><br><span class="line">        List&lt;Tuple2&lt;String, String&gt;&gt; scoreList = Arrays.asList(</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="string">"1"</span>, <span class="string">"90"</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="string">"2"</span>, <span class="string">"80"</span>),</span><br><span class="line">                <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="string">"3"</span>, <span class="string">"89"</span>)</span><br><span class="line">        );</span><br><span class="line">       JavaPairRDD&lt;String, String&gt; nameRDD = sc.parallelizePairs(nameList);</span><br><span class="line">       JavaPairRDD&lt;String, String&gt; scoreRDD = sc.parallelizePair(scoreList);</span><br><span class="line">       List&lt;Tuple2&lt;String, String&gt;&gt; collect = nameRDD.collect();</span><br><span class="line">        Map&lt;String, String&gt; nameMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;String, String&gt; tuple2 : collect) &#123;</span><br><span class="line">            nameMap.put(tuple2._1, tuple2._2);</span><br><span class="line">        &#125;	</span><br><span class="line">        <span class="keyword">final</span> Broadcast&lt;Map&lt;String, String&gt;&gt; nMB =sc.broadcast(nameMap);</span><br><span class="line">        scoreRDD.map(<span class="keyword">new</span> Function&lt;Tuple2&lt;String,String&gt;,String&gt;()&#123;</span><br><span class="line">           <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;</span><br><span class="line">           <span class="meta">@Override</span></span><br><span class="line">           <span class="function"><span class="keyword">public</span> String <span class="title">call</span><span class="params">(Tuple2&lt;String, String&gt; tuple)</span> </span>&#123;	</span><br><span class="line">                Map&lt;String, String&gt; nameMap = nMB.value();</span><br><span class="line">                String id=tuple._1;</span><br><span class="line">                String score=tuple._2;</span><br><span class="line">                String name=nameMap.get(id);</span><br><span class="line">                <span class="keyword">if</span>(name != <span class="keyword">null</span>)&#123;</span><br><span class="line">                    System.out.println(id +<span class="string">" name:"</span>+name +<span class="string">" score:"</span>+score);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).collect();</span><br><span class="line">        sc.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>[执行结果]</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#96;id:1 name:zhangsan score:90</span><br><span class="line"> id:2 name:lisi score:80</span><br><span class="line"> id:3 name:wangwu score:89&#96;</span><br></pre></td></tr></table></figure>

<h3 id="二、累加器"><a href="#二、累加器" class="headerlink" title="二、累加器"></a>二、累加器</h3><h4 id="1-什么是累加器？"><a href="#1-什么是累加器？" class="headerlink" title="1.什么是累加器？"></a><strong>1.什么是累加器？</strong></h4><p>累加器可以看成是一个集群规模级别的一个大变量。</p>
<h4 id="2-累加器与广播变量比较"><a href="#2-累加器与广播变量比较" class="headerlink" title="2.累加器与广播变量比较"></a><strong>2.累加器与广播变量比较</strong></h4><p><code>累加器</code>是在Driver端创建，在Driver端读取，在Executor端操作(累加操作)，在Executor端是不能读取的。</p>
<p><code>广播变量</code>是在Driver端创建，在Executor端读取，在Executor端不能修改。</p>
<h4 id="3-利用累加器算文件的行数"><a href="#3-利用累加器算文件的行数" class="headerlink" title="3.利用累加器算文件的行数"></a><strong>3.利用累加器算文件的行数</strong></h4><p><strong>[代码演示]</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TestAccumulator</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">                .setAppName(<span class="string">"BroadCast"</span>)</span><br><span class="line">                .setMaster(<span class="string">"local"</span>)</span><br><span class="line">                .set(<span class="string">"spark.testing.memory"</span>, <span class="string">"2147480000"</span>);</span><br><span class="line">       JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);	</span><br><span class="line">        <span class="keyword">final</span> Accumulator&lt;Integer&gt; accumulator = sc.accumulator(<span class="number">0</span>);	</span><br><span class="line">        JavaRDD&lt;String&gt; userLogRDD = sc.textFile(<span class="string">"cs"</span>);	</span><br><span class="line">        userLogRDD.foreach(<span class="keyword">new</span> VoidFunction&lt;String&gt;() &#123;	</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> serialVersionUID = <span class="number">1L</span>;	</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">call</span><span class="params">(String s)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                accumulator.add(<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        System.out.println(<span class="string">"line count:"</span> + accumulator.value());</span><br><span class="line">        sc.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-累加器的错误用法"><a href="#4-累加器的错误用法" class="headerlink" title="4.累加器的错误用法"></a><strong>4.累加器的错误用法</strong></h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> accum= sc.accumulator(<span class="number">0</span>, <span class="string">"Error Accumulator"</span>)</span><br><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)	</span><br><span class="line"><span class="comment">//用accumulator统计偶数出现的次数，同时偶数返回0，奇数返回1	</span></span><br><span class="line"><span class="keyword">val</span> newData = data.map&#123;x =&gt; &#123;	</span><br><span class="line"><span class="keyword">if</span>(x%<span class="number">2</span> == <span class="number">0</span>)&#123;	</span><br><span class="line">accum += <span class="number">1</span>	</span><br><span class="line"><span class="number">0</span>	</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="number">1</span>	</span><br><span class="line">&#125;&#125;	</span><br><span class="line"><span class="comment">//使用action操作触发执行	</span></span><br><span class="line">newData.count	</span><br><span class="line"><span class="comment">//此时accum的值为5，是我们要的结果	</span></span><br><span class="line">accum.value	</span><br><span class="line"><span class="comment">//继续操作，查看刚才变动的数据,foreach也是action操作	</span></span><br><span class="line">newData.foreach(println)	</span><br><span class="line"><span class="comment">//上个步骤没有进行累计器操作，可是累加器此时的结果已经是10了	</span></span><br><span class="line"><span class="comment">//这并不是我们想要的结果</span></span><br><span class="line">accum.value</span><br></pre></td></tr></table></figure>



<p><strong>原因分析</strong></p>
<p>官方对这个问题的解释如下描述:</p>
<blockquote>
<p>For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.</p>
</blockquote>
<p>我们都知道，spark中的一系列transform操作会构成一串长的任务链，此时需要通过一个action操作来触发，accumulator也是一样。因此在一个action操作之前，你调用value方法查看其数值，肯定是没有任何变化的。</p>
<p>所以在第一次count(action操作)之后，我们发现累加器的数值变成了5，是我们要的答案。</p>
<p>之后又对新产生的的newData进行了一次foreach(action操作)，其实这个时候又执行了一次map(transform)操作，所以累加器又增加了5。最终获得的结果变成了10。</p>
<p><img src="/images/%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/spark%E7%B4%AF%E5%8A%A0%E5%99%A81.png" alt="img"></p>
<p><strong>解决办法</strong></p>
<p>看了上面的分析，大家都有这种印象了，那就是使用累加器的过程中只能使用一次action的操作才能保证结果的准确性。</p>
<p>事实上，还是有解决方案的，只要将任务之间的依赖关系切断就可以了。什么方法有这种功能呢？你们肯定都想到了，cache，persist。调用这个方法的时候会将之前的依赖切除，后续的累加器就不会再被之前的transfrom操作影响到了。</p>
<p><img src="/images/%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F/spark%E7%B4%AF%E5%8A%A0%E5%99%A82.png" alt="img"></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> accum= sc.accumulator(<span class="number">0</span>, <span class="string">"Error Accumulator"</span>)</span><br><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="comment">//代码和上方相同</span></span><br><span class="line"><span class="keyword">val</span> newData = data.map&#123;x =&gt; &#123;...&#125;&#125;</span><br><span class="line"><span class="comment">//使用cache缓存数据，切断依赖。</span></span><br><span class="line">newData.cache.count</span><br><span class="line"><span class="comment">//此时accum的值为5</span></span><br><span class="line">accum.value</span><br><span class="line">newData.foreach(println)</span><br><span class="line"><span class="comment">//此时的accum依旧是5</span></span><br><span class="line">accum.value</span><br></pre></td></tr></table></figure>



<p>注：使用Accumulator时，为了保证准确性，只使用一次action操作。如果需要使用多次则使用cache或persist操作切断依赖。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E9%AB%98%E7%BA%A7%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E9%AB%98%E7%BA%A7%E7%AF%87/" class="post-title-link" itemprop="url">Spark性能优化指南——高级篇（转）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/" itemprop="url" rel="index">
                    <span itemprop="name">数据处理实践</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/Spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8C%87%E5%8D%97-%E9%AB%98%E7%BA%A7%E7%AF%87/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/数据处理实践/Spark性能优化指南-高级篇/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="一、数据倾斜调优"><a href="#一、数据倾斜调优" class="headerlink" title="一、数据倾斜调优"></a>一、数据倾斜调优</h1><h2 id="1-1-调优概述"><a href="#1-1-调优概述" class="headerlink" title="1.1 调优概述"></a>1.1 调优概述</h2><p>有的时候，我们可能会遇到大数据计算中一个最棘手的问题——数据倾斜，此时Spark作业的性能会比期望差很多。数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p>
<h2 id="1-2-数据倾斜发生时的现象"><a href="#1-2-数据倾斜发生时的现象" class="headerlink" title="1.2 数据倾斜发生时的现象"></a>1.2 数据倾斜发生时的现象</h2><ul>
<li>绝大多数task执行得都非常快，但个别task执行极慢。比如，总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时。这种情况很常见。</li>
<li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li>
</ul>
<h2 id="1-3-数据倾斜发生的原理"><a href="#1-3-数据倾斜发生的原理" class="headerlink" title="1.3 数据倾斜发生的原理"></a>1.3 数据倾斜发生的原理</h2><p>数据倾斜的原理很简单：在进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理，比如按照key进行聚合或join等操作。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别task可能分配到了100万数据，要运行一两个小时。因此，整个Spark作业的运行进度是由运行时间最长的那个task决定的。</p>
<p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p>
<p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p>
<p><img src="https://tech.meituan.com/img/spark-tuning/skwed-mech.png" alt="数据倾斜原理"></p>
<h2 id="1-4-如何定位导致数据倾斜的代码"><a href="#1-4-如何定位导致数据倾斜的代码" class="headerlink" title="1.4 如何定位导致数据倾斜的代码"></a>1.4 如何定位导致数据倾斜的代码</h2><p>数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p>
<h3 id="1-4-1-某个task执行特别慢的情况"><a href="#1-4-1-某个task执行特别慢的情况" class="headerlink" title="1.4.1 某个task执行特别慢的情况"></a>1.4.1 某个task执行特别慢的情况</h3><p>首先要看的，就是数据倾斜发生在第几个stage中。</p>
<p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以在Spark Web UI上深入看一下当前这个stage各个task分配的数据量，从而进一步确定是不是task分配的数据不均匀导致了数据倾斜。</p>
<p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而运行时间特别长的task需要处理几千KB的数据，处理的数据量差了10倍。此时更加能够确定是发生了数据倾斜。</p>
<p><img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-web-ui-demo.png" alt="img"></p>
<p>知道数据倾斜发生在哪一个stage之后，接着我们就需要根据stage划分原理，推算出来发生倾斜的那个stage对应代码中的哪一部分，这部分代码中肯定会有一个shuffle类算子。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p>
<p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。</p>
<ul>
<li>stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。</li>
<li>stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val conf &#x3D; new SparkConf()</span><br><span class="line">val sc &#x3D; new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">val lines &#x3D; sc.textFile(&quot;hdfs:&#x2F;&#x2F;...&quot;)</span><br><span class="line">val words &#x3D; lines.flatMap(_.split(&quot; &quot;))</span><br><span class="line">val pairs &#x3D; words.map((_, 1))</span><br><span class="line">val wordCounts &#x3D; pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure>

<p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p>
<h3 id="1-4-2-某个task莫名其妙内存溢出的情况"><a href="#1-4-2-某个task莫名其妙内存溢出的情况" class="headerlink" title="1.4.2 某个task莫名其妙内存溢出的情况"></a>1.4.2 某个task莫名其妙内存溢出的情况</h3><p>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p>
<p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p>
<h2 id="1-5-查看导致数据倾斜的key的数据分布情况"><a href="#1-5-查看导致数据倾斜的key的数据分布情况" class="headerlink" title="1.5 查看导致数据倾斜的key的数据分布情况"></a>1.5 查看导致数据倾斜的key的数据分布情况</h2><p>知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD/Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p>
<p>此时根据你执行操作的情况不同，可以有很多种查看key分布的方式：</p>
<ol>
<li>如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。</li>
<li>如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</li>
</ol>
<p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val sampledPairs &#x3D; pairs.sample(false, 0.1)</span><br><span class="line">val sampledWordCounts &#x3D; sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure>

<h2 id="1-6-数据倾斜的解决方案"><a href="#1-6-数据倾斜的解决方案" class="headerlink" title="1.6 数据倾斜的解决方案"></a>1.6 数据倾斜的解决方案</h2><h3 id="1-6-1-解决方案一：使用Hive-ETL预处理数据"><a href="#1-6-1-解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="1.6.1 解决方案一：使用Hive ETL预处理数据"></a>1.6.1 解决方案一：使用Hive ETL预处理数据</h3><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p>
<p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p>
<p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p>
<p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p>
<p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p>
<p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p>
<p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p>
<h3 id="1-6-2-解决方案二：过滤少数导致倾斜的key"><a href="#1-6-2-解决方案二：过滤少数导致倾斜的key" class="headerlink" title="1.6.2 解决方案二：过滤少数导致倾斜的key"></a>1.6.2 解决方案二：过滤少数导致倾斜的key</h3><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p>
<p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p>
<p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p>
<p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p>
<p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p>
<p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p>
<h3 id="1-6-3-解决方案三：提高shuffle操作的并行度"><a href="#1-6-3-解决方案三：提高shuffle操作的并行度" class="headerlink" title="1.6.3 解决方案三：提高shuffle操作的并行度"></a>1.6.3 解决方案三：提高shuffle操作的并行度</h3><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p>
<p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p>
<p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p>
<p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p>
<p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p>
<p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p>
<p><img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-add-partition.png" alt="img"></p>
<h3 id="1-6-4-解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#1-6-4-解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="1.6.4 解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>1.6.4 解决方案四：两阶段聚合（局部聚合+全局聚合）</h3><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p>
<p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p>
<p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p>
<p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p>
<p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p>
<p><img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-two-phase-aggr.png" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 第一步，给RDD中的每个key都打上一个随机前缀。</span><br><span class="line">JavaPairRDD&lt;String, Long&gt; randomPrefixRdd &#x3D; rdd.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;String, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                Random random &#x3D; new Random();</span><br><span class="line">                int prefix &#x3D; random.nextInt(10);</span><br><span class="line">                return new Tuple2&lt;String, Long&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 第二步，对打上随机前缀的key进行局部聚合。</span><br><span class="line">JavaPairRDD&lt;String, Long&gt; localAggrRdd &#x3D; randomPrefixRdd.reduceByKey(</span><br><span class="line">        new Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Long call(Long v1, Long v2) throws Exception &#123;</span><br><span class="line">                return v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 第三步，去除RDD中每个key的随机前缀。</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd &#x3D; localAggrRdd.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, Long&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                long originalKey &#x3D; Long.valueOf(tuple._1.split(&quot;_&quot;)[1]);</span><br><span class="line">                return new Tuple2&lt;Long, Long&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 第四步，对去除了随机前缀的RDD进行全局聚合。</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; globalAggrRdd &#x3D; removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        new Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Long call(Long v1, Long v2) throws Exception &#123;</span><br><span class="line">                return v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure>

<h3 id="1-6-5-解决方案五：将reduce-join转为map-join"><a href="#1-6-5-解决方案五：将reduce-join转为map-join" class="headerlink" title="1.6.5 解决方案五：将reduce join转为map join"></a>1.6.5 解决方案五：将reduce join转为map join</h3><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p>
<p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p>
<p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p>
<p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p>
<p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p>
<p><img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-map-join.png" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 首先将数据量比较小的RDD的数据，collect到Driver中来。</span><br><span class="line">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data &#x3D; rdd1.collect()</span><br><span class="line">&#x2F;&#x2F; 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span><br><span class="line">&#x2F;&#x2F; 可以尽可能节省内存空间，并且减少网络传输性能开销。</span><br><span class="line">final Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast &#x3D; sc.broadcast(rdd1Data);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 对另外一个RDD执行map类操作，而不再是join类操作。</span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd &#x3D; rdd2.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                &#x2F;&#x2F; 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span><br><span class="line">                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data &#x3D; rdd1DataBroadcast.value();</span><br><span class="line">                &#x2F;&#x2F; 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span><br><span class="line">                Map&lt;Long, Row&gt; rdd1DataMap &#x3D; new HashMap&lt;Long, Row&gt;();</span><br><span class="line">                for(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                &#x2F;&#x2F; 获取当前RDD数据的key以及value。</span><br><span class="line">                String key &#x3D; tuple._1;</span><br><span class="line">                String value &#x3D; tuple._2;</span><br><span class="line">                &#x2F;&#x2F; 从rdd1数据Map中，根据key获取到可以join到的数据。</span><br><span class="line">                Row rdd1Value &#x3D; rdd1DataMap.get(key);</span><br><span class="line">                return new Tuple2&lt;String, String&gt;(key, new Tuple2&lt;String, Row&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 这里得提示一下。</span><br><span class="line">&#x2F;&#x2F; 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span><br><span class="line">&#x2F;&#x2F; 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span><br><span class="line">&#x2F;&#x2F; rdd2中每条数据都可能会返回多条join后的数据。</span><br></pre></td></tr></table></figure>

<h3 id="1-6-6-解决方案六：采样倾斜key并分拆join操作"><a href="#1-6-6-解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="1.6.6 解决方案六：采样倾斜key并分拆join操作"></a>1.6.6 解决方案六：采样倾斜key并分拆join操作</h3><p><strong>方案适用场景：</strong>两个RDD/Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD/Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD/Hive表中的少数几个key的数据量过大，而另一个RDD/Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p>
<p><strong>方案实现思路：</strong></p>
<ul>
<li>对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。</li>
<li>然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。</li>
<li>接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。</li>
<li>再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。</li>
<li>而另外两个普通的RDD就照常join即可。</li>
<li>最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</li>
</ul>
<p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p>
<p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p>
<p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p>
<p><img src="https://tech.meituan.com/img/spark-tuning/shuffle-skwed-sample-expand.png" alt="img"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; sampledRDD &#x3D; rdd1.sample(false, 0.1);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span><br><span class="line">&#x2F;&#x2F; 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span><br><span class="line">&#x2F;&#x2F; 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD &#x3D; sampledRDD.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                return new Tuple2&lt;Long, Long&gt;(tuple._1, 1L);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; countedSampledRDD &#x3D; mappedSampledRDD.reduceByKey(</span><br><span class="line">        new Function2&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Long call(Long v1, Long v2) throws Exception &#123;</span><br><span class="line">                return v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD &#x3D; countedSampledRDD.mapToPair( </span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;Long, Long&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                return new Tuple2&lt;Long, Long&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">final Long skewedUserid &#x3D; reversedSampledRDD.sortByKey(false).take(1).get(0)._2;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; skewedRDD &#x3D; rdd1.filter(</span><br><span class="line">        new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123;</span><br><span class="line">                return tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">&#x2F;&#x2F; 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; commonRDD &#x3D; rdd1.filter(</span><br><span class="line">        new Function&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Boolean call(Tuple2&lt;Long, String&gt; tuple) throws Exception &#123;</span><br><span class="line">                return !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; rdd2，就是那个所有key的分布相对较为均匀的rdd。</span><br><span class="line">&#x2F;&#x2F; 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span><br><span class="line">&#x2F;&#x2F; 对扩容的每条数据，都打上0～100的前缀。</span><br><span class="line">JavaPairRDD&lt;String, Row&gt; skewedRdd2 &#x3D; rdd2.filter(</span><br><span class="line">         new Function&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Boolean call(Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123;</span><br><span class="line">                return tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(</span><br><span class="line">                    Tuple2&lt;Long, Row&gt; tuple) throws Exception &#123;</span><br><span class="line">                Random random &#x3D; new Random();</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list &#x3D; new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                for(int i &#x3D; 0; i &lt; 100; i++) &#123;</span><br><span class="line">                    list.add(new Tuple2&lt;String, Row&gt;(i + &quot;_&quot; + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                return list;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span><br><span class="line">&#x2F;&#x2F; 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 &#x3D; skewedRDD.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                Random random &#x3D; new Random();</span><br><span class="line">                int prefix &#x3D; random.nextInt(100);</span><br><span class="line">                return new Tuple2&lt;String, String&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedUserid2infoRDD)</span><br><span class="line">        .mapToPair(new PairFunction&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">                        private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">                        @Override</span><br><span class="line">                        public Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; call(</span><br><span class="line">                            Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple)</span><br><span class="line">                            throws Exception &#123;</span><br><span class="line">                            long key &#x3D; Long.valueOf(tuple._1.split(&quot;_&quot;)[1]);</span><br><span class="line">                            return new Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 &#x3D; commonRDD.join(rdd2);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span><br><span class="line">&#x2F;&#x2F; 就是最终的join结果。</span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD &#x3D; joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure>

<h3 id="1-6-7-解决方案七：使用随机前缀和扩容RDD进行join"><a href="#1-6-7-解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="1.6.7 解决方案七：使用随机前缀和扩容RDD进行join"></a>1.6.7 解决方案七：使用随机前缀和扩容RDD进行join</h3><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p>
<p><strong>方案实现思路：</strong></p>
<ul>
<li>该方案的实现思路基本和“解决方案六”类似，首先查看RDD/Hive表中的数据分布情况，找到那个造成数据倾斜的RDD/Hive表，比如有多个key都对应了超过1万条数据。</li>
<li>然后将该RDD的每条数据都打上一个n以内的随机前缀。</li>
<li>同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。</li>
<li>最后将两个处理后的RDD进行join即可。</li>
</ul>
<p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p>
<p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p>
<p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p>
<p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span><br><span class="line">JavaPairRDD&lt;String, Row&gt; expandedRDD &#x3D; rdd1.flatMapToPair(</span><br><span class="line">        new PairFlatMapFunction&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Iterable&lt;Tuple2&lt;String, Row&gt;&gt; call(Tuple2&lt;Long, Row&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list &#x3D; new ArrayList&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                for(int i &#x3D; 0; i &lt; 100; i++) &#123;</span><br><span class="line">                    list.add(new Tuple2&lt;String, Row&gt;(0 + &quot;_&quot; + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                return list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span><br><span class="line">JavaPairRDD&lt;String, String&gt; mappedRDD &#x3D; rdd2.mapToPair(</span><br><span class="line">        new PairFunction&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            private static final long serialVersionUID &#x3D; 1L;</span><br><span class="line">            @Override</span><br><span class="line">            public Tuple2&lt;String, String&gt; call(Tuple2&lt;Long, String&gt; tuple)</span><br><span class="line">                    throws Exception &#123;</span><br><span class="line">                Random random &#x3D; new Random();</span><br><span class="line">                int prefix &#x3D; random.nextInt(100);</span><br><span class="line">                return new Tuple2&lt;String, String&gt;(prefix + &quot;_&quot; + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 将两个处理后的RDD进行join即可。</span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD &#x3D; mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure>

<h3 id="1-6-8-解决方案八：多种方案组合使用"><a href="#1-6-8-解决方案八：多种方案组合使用" class="headerlink" title="1.6.8 解决方案八：多种方案组合使用"></a>1.6.8 解决方案八：多种方案组合使用</h3><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p>
<h1 id="二、shuffle调优"><a href="#二、shuffle调优" class="headerlink" title="二、shuffle调优"></a>二、shuffle调优</h1><h2 id="2-1-调优概述"><a href="#2-1-调优概述" class="headerlink" title="2.1 调优概述"></a>2.1 调优概述</h2><p>大多数Spark作业的性能主要就是消耗在了shuffle环节，因为该环节包含了大量的磁盘IO、序列化、网络数据传输等操作。因此，如果要让作业的性能更上一层楼，就有必要对shuffle过程进行调优。但是也必须提醒大家的是，影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。因此大家务必把握住调优的基本原则，千万不要舍本逐末。下面我们就给大家详细讲解shuffle的原理，以及相关参数的说明，同时给出各个参数的调优建议。</p>
<h2 id="2-2-ShuffleManager发展概述"><a href="#2-2-ShuffleManager发展概述" class="headerlink" title="2.2 ShuffleManager发展概述"></a>2.2 ShuffleManager发展概述</h2><p>在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p>
<p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</p>
<p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p>
<p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p>
<h2 id="2-3-HashShuffleManager运行原理"><a href="#2-3-HashShuffleManager运行原理" class="headerlink" title="2.3 HashShuffleManager运行原理"></a>2.3 HashShuffleManager运行原理</h2><h3 id="2-3-1-未经优化的HashShuffleManager"><a href="#2-3-1-未经优化的HashShuffleManager" class="headerlink" title="2.3.1 未经优化的HashShuffleManager"></a>2.3.1 未经优化的HashShuffleManager</h3><p>下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p>
<p>我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p>
<p>那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p>
<p>接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p>
<p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p>
<p><img src="https://tech.meituan.com/img/spark-tuning/hash-shuffle-common.png" alt="img"></p>
<h3 id="2-3-2-优化后的HashShuffleManager"><a href="#2-3-2-优化后的HashShuffleManager" class="headerlink" title="2.3.2 优化后的HashShuffleManager"></a>2.3.2 优化后的HashShuffleManager</h3><p>下图说明了优化后的HashShuffleManager的原理。这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p>
<p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p>
<p>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p>
<p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p>
<p><img src="https://tech.meituan.com/img/spark-tuning/hash-shuffle-consolidate.png" alt="img"></p>
<h2 id="2-4-SortShuffleManager运行原理"><a href="#2-4-SortShuffleManager运行原理" class="headerlink" title="2.4 SortShuffleManager运行原理"></a>2.4 SortShuffleManager运行原理</h2><p>SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。</p>
<h3 id="2-4-1普通运行机制"><a href="#2-4-1普通运行机制" class="headerlink" title="2.4. 1普通运行机制"></a>2.4. 1普通运行机制</h3><p>下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</p>
<p>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p>
<p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p>
<p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。<br><img src="https://tech.meituan.com/img/spark-tuning/sort-shuffle-common.png" alt="img"></p>
<h3 id="2-4-2-bypass运行机制"><a href="#2-4-2-bypass运行机制" class="headerlink" title="2.4.2 bypass运行机制"></a>2.4.2 bypass运行机制</h3><p>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：</p>
<ul>
<li>shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。</li>
<li>不是聚合类的shuffle算子（比如reduceByKey）。</li>
</ul>
<p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p>
<p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p>
<p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。<br><img src="https://tech.meituan.com/img/spark-tuning/sort-shuffle-bypass.png" alt="img"></p>
<h2 id="2-5-shuffle相关参数调优"><a href="#2-5-shuffle相关参数调优" class="headerlink" title="2.5 shuffle相关参数调优"></a>2.5 shuffle相关参数调优</h2><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p>
<h3 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h3><ul>
<li>默认值：32k</li>
<li>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。</li>
<li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li>
</ul>
<h3 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h3><ul>
<li>默认值：48m</li>
<li>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。</li>
<li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li>
</ul>
<h3 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h3><ul>
<li>默认值：3</li>
<li>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li>
<li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li>
</ul>
<h3 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h3><ul>
<li>默认值：5s</li>
<li>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li>
<li>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</li>
</ul>
<h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul>
<li>默认值：0.2</li>
<li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li>
<li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</li>
</ul>
<h3 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h3><ul>
<li>默认值：sort</li>
<li>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li>
<li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li>
</ul>
<h3 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h3><ul>
<li>默认值：200</li>
<li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li>
<li>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li>
</ul>
<h3 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h3><ul>
<li>默认值：false</li>
<li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li>
<li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/sparkSql%E5%87%BD%E6%95%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/sparkSql%E5%87%BD%E6%95%B0/" class="post-title-link" itemprop="url">sparkSql函数</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 00:00:05" itemprop="dateCreated datePublished" datetime="2017-06-04T00:00:05+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/" itemprop="url" rel="index">
                    <span itemprop="name">语法解释</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/sparkSql%E5%87%BD%E6%95%B0/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/03/spark/语法解释/sparkSql函数/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>spark-sql 函数介绍</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/sparkSql%E5%87%BD%E6%95%B0/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/%E5%87%BD%E6%95%B0RDD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/%E5%87%BD%E6%95%B0RDD/" class="post-title-link" itemprop="url">RDD函数</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 00:00:02" itemprop="dateCreated datePublished" datetime="2017-06-04T00:00:02+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/" itemprop="url" rel="index">
                    <span itemprop="name">语法解释</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/%E5%87%BD%E6%95%B0RDD/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/03/spark/语法解释/函数RDD/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="RDD函数"><a href="#RDD函数" class="headerlink" title="RDD函数"></a>RDD函数</h1><p>[TOC]</p>
<p>RDD提供了两种类型的操作：transformation 和 action</p>
<p>所有的 transformation 都是采用的懒策略，如果只是将 transformation 提交是不会执行计算的，计算只有在action被提交的时候才被触发。</p>
<h2 id="一、基本RDD"><a href="#一、基本RDD" class="headerlink" title="一、基本RDD"></a>一、基本RDD</h2><p>抽象类<code>RDD</code>包含了各种数据类型的RDD都适用的通用操作。</p>
<h3 id="1-1-transformation操作"><a href="#1-1-transformation操作" class="headerlink" title="1.1 transformation操作"></a>1.1 transformation操作</h3><p>针对各个元素的转化操作</p>
<h4 id="1-1-1-map-func"><a href="#1-1-1-map-func" class="headerlink" title="1.1.1 map(func)"></a>1.1.1 map(func)</h4><p>接收一个函数，把这个函数用于 RDD 中的每个元素，将函数的返回结果作为结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val words &#x3D; List(&quot;zks&quot;,&quot;zhaikaishun&quot;,&quot;kaishun&quot;,&quot;kai&quot;,&quot;xiaozhai&quot;)</span><br><span class="line">words: List[String] &#x3D; List(zks, zhaikaishun, kaishun, kai, xiaozhai)</span><br><span class="line"></span><br><span class="line">scala&gt; words.map(_.length)</span><br><span class="line">res2: List[Int] &#x3D; List(3, 11, 7, 3, 8)</span><br></pre></td></tr></table></figure>

<h4 id="1-1-2-flatMap-func"><a href="#1-1-2-flatMap-func" class="headerlink" title="1.1.2 flatMap(func)"></a>1.1.2 flatMap(func)</h4><p>接收一个函数(这个函数的返回结果通常为集合)，把这个函数用于 RDD 中的每个元素，将函数的返回结果展平，将展平后的数据作为结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val words &#x3D; List(&quot;zks&quot;,&quot;zhaikaishun&quot;,&quot;kaishun&quot;,&quot;kai&quot;,&quot;xiaozhai&quot;)</span><br><span class="line">words: List[String] &#x3D; List(zks, zhaikaishun, kaishun, kai, xiaozhai)</span><br><span class="line"></span><br><span class="line">scala&gt; words.flatMap(_.toList)</span><br><span class="line">res4: List[Char] &#x3D; List(z, k, s, z, h, a, i, k, a, i, s, h, u, n, k, a, i, s, h, u, n, k, a, i, x, i, a, o, z, h, a, i)</span><br></pre></td></tr></table></figure>

<h4 id="1-1-3-filter-func"><a href="#1-1-3-filter-func" class="headerlink" title="1.1.3 filter(func)"></a>1.1.3 filter(func)</h4><p>过滤不满足条件的元素。filter操作可能会引起数据倾斜，甚至可能导致空分区，新形成的RDD将会包含这些可能生成的空分区。所有这些都可能会导致问题，要想解决它们，最好在filter之后重新分区。</p>
<h3 id="1-2-伪集合操作"><a href="#1-2-伪集合操作" class="headerlink" title="1.2 伪集合操作"></a>1.2 伪集合操作</h3><p>尽管RDD不是严格意义上的集合，但它支持许多数学上的集合操作。注意：这些操作都要求操作的RDD是相同的数据类型的。</p>
<h4 id="1-2-1-distinct"><a href="#1-2-1-distinct" class="headerlink" title="1.2.1 distinct()"></a>1.2.1 distinct()</h4><p>对RDD中的元素进行去重处理。需要注意的是，distinct操作开销很大，因为它需要shuffle所有数据，以确保每一个元素都只有一份。</p>
<h4 id="1-2-2-union-otherDataset"><a href="#1-2-2-union-otherDataset" class="headerlink" title="1.2.2 union(otherDataset)"></a>1.2.2 union(otherDataset)</h4><p>合并两个RDD中所有元素的RDD。spark的union并不会去重，这点与数学上的不同。</p>
<h4 id="1-2-3-intersection"><a href="#1-2-3-intersection" class="headerlink" title="1.2.3 intersection"></a>1.2.3 intersection</h4><p>返回两个RDD中都有的元素(即取交集)。intersection会在运行时除去所有重复的元素，因此它也需要shuffle，性能要差一些。</p>
<h4 id="1-2-4-subtract"><a href="#1-2-4-subtract" class="headerlink" title="1.2.4 subtract"></a>1.2.4 subtract</h4><p>返回一个由只存在于第一个RDD中而不存在于第二个RDD中的所有元素组成的RDD。它也需要shuffle</p>
<h4 id="1-2-5-cartesian-otherDataset"><a href="#1-2-5-cartesian-otherDataset" class="headerlink" title="1.2.5 cartesian(otherDataset)"></a>1.2.5 cartesian(otherDataset)</h4><p>笛卡尔积。在数据集T和U上调用时，返回一个(T，U）对的数据集，所有元素交互进行笛卡尔积。开销非常大。</p>
<h4 id="1-2-6-sample-withReplacement-frac-seed"><a href="#1-2-6-sample-withReplacement-frac-seed" class="headerlink" title="1.2.6 sample(withReplacement, frac, seed)"></a>1.2.6 sample(withReplacement, frac, seed)</h4><p>根据给定的 seed，从RDD中随机地按 指定比例frac 选一部分记录，创建新的RDD</p>
<p>withReplacement 表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样。</p>
<h3 id="1-3-基于分区的转化操作"><a href="#1-3-基于分区的转化操作" class="headerlink" title="1.3 基于分区的转化操作"></a>1.3 基于分区的转化操作</h3><h4 id="1-3-1-glom"><a href="#1-3-1-glom" class="headerlink" title="1.3.1 glom"></a>1.3.1 glom</h4><p>将每个分区中的所有元素都形成一个数组。如果在处理当前元素时需要使用前后的元素，该操作将会非常有用，不过有时我们可能还需要将分区边界的数据收集起来并广播到各节点以备使用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> glomRDD = rdd.glom() <span class="comment">//RDD[Array[T]]</span></span><br><span class="line">glomRDD.foreach(rdd =&gt; println(rdd.getClass.getSimpleName))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出</span></span><br><span class="line">int[] <span class="comment">//说明RDD中的元素被转换成数组Array[Int]</span></span><br></pre></td></tr></table></figure>

<h4 id="1-3-2-mapPartitions"><a href="#1-3-2-mapPartitions" class="headerlink" title="1.3.2 mapPartitions"></a>1.3.2 mapPartitions</h4><p>基于分区的map，spark对每个分区的迭代器进行操作。</p>
<p>普通的map算子对RDD中的每一个元素进行操作，而 mapPartitions 算子对RDD中每一个分区进行操作。</p>
<ul>
<li>如果是普通的map算子，假设一个partition有1万条数据，那么map算子中的function要执行1万次，也就是对每个元素进行操作。</li>
<li>如果是mapPartition算子，由于一个task处理一个RDD的partition，那么一个task只会执行一次function，function一次接收所有的partition数据，效率比较高。</li>
</ul>
<h4 id="1-3-3-mapPartitionsWithIndex"><a href="#1-3-3-mapPartitionsWithIndex" class="headerlink" title="1.3.3 mapPartitionsWithIndex"></a>1.3.3 mapPartitionsWithIndex</h4><p>与mapPartitions不同之处在于带有分区的序号。</p>
<h3 id="1-4-管道-pipe-操作"><a href="#1-4-管道-pipe-操作" class="headerlink" title="1.4 管道(pipe)操作"></a>1.4 管道(pipe)操作</h3><p>spark在RDD上提供了<code>pipe()</code>方法。通过pipe()，你可以使用任意语言将RDD中的各元素从标准输入流中以字符串形式读出，并将这些元素执行任何你需要的操作，然后把结果以字符串形式写入标准输出，这个过程就是RDD的转化操作过程。</p>
<p>使用pipe()的方法很简单，假如我们有一个用其他语言写成的从标准输入接收数据并将处理结果写入标准输出的可执行脚本，我们只需要将该脚本分发到各个节点相同路径下，并将其路径作为pipe()的参数传入即可。</p>
<p>创建外部shell脚本:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">echo &quot;Running shell script&quot;</span><br><span class="line">while read LINE; do</span><br><span class="line">   echo $&#123;LINE&#125;!</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p>Spark rdd 调用:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val data &#x3D; sc.parallelize(List(&quot;hi&quot;,&quot;hello&quot;,&quot;how&quot;,&quot;are&quot;,&quot;you&quot;))</span><br><span class="line">val scriptPath &#x3D; &quot;&#x2F;root&#x2F;echo.sh&quot;</span><br><span class="line">val pipeRDD &#x3D; dataRDD.pipe(scriptPath)</span><br><span class="line">pipeRDD.collect()</span><br></pre></td></tr></table></figure>

<p>得到结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Array[String] &#x3D; Array(Running shell script, hi!, Running shell script, hello!, Running shell script, how!, Running shell script, are!, you!)</span><br></pre></td></tr></table></figure>

<h3 id="1-5-action操作"><a href="#1-5-action操作" class="headerlink" title="1.5 action操作"></a>1.5 action操作</h3><h4 id="1-5-1-foreach-func"><a href="#1-5-1-foreach-func" class="headerlink" title="1.5.1 foreach(func)"></a>1.5.1 foreach(func)</h4><p>对每个元素进行操作，并不会返回结果。这通常用于更新一个累加器变量，或者和外部存储系统做交互</p>
<h4 id="1-5-2-foreachPartition"><a href="#1-5-2-foreachPartition" class="headerlink" title="1.5.2 foreachPartition"></a>1.5.2 foreachPartition</h4><p>基于分区的foreach操作，操作分区元素的迭代器，并不会返回结果。</p>
<h4 id="1-5-3-reduce-func"><a href="#1-5-3-reduce-func" class="headerlink" title="1.5.3 reduce(func)"></a>1.5.3 reduce(func)</h4><p>reduce方法将RDD中元素前两个传给输入函数，产生一个新的return值，新产生的return值与RDD中下一个元素（第三个元素）组成两个元素，再被传给输入函数，直到最后只有一个值为止。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> c = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">c.reduce((x, y) =&gt; x + y)<span class="comment">//结果55</span></span><br></pre></td></tr></table></figure>

<h4 id="1-5-4-fold-zeroValue-func"><a href="#1-5-4-fold-zeroValue-func" class="headerlink" title="1.5.4 fold(zeroValue)(func)"></a>1.5.4 fold(zeroValue)(func)</h4><p>与reduce类似，不同的是，它接受一个“初始值”来作为<strong>每个分区第一次调用</strong>时的结果。fold要求函数返回值类型与RDD元素类型相同。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> l = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">l.fold(<span class="number">0</span>)((x, y) =&gt; x + y)<span class="comment">//结果55</span></span><br></pre></td></tr></table></figure>

<h4 id="1-5-5-aggregate-zeroValue-seqOp-combOp"><a href="#1-5-5-aggregate-zeroValue-seqOp-combOp" class="headerlink" title="1.5.5 aggregate(zeroValue)(seqOp, combOp)"></a>1.5.5 aggregate(zeroValue)(seqOp, combOp)</h4><p>与reduce和fold类似，但它把我们从返回值类型必须与所操作的RDD元素类型相同的限制中解放出来。</p>
<p>该函数签名如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">T</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">U</span> = withScope &#123;...&#125;</span><br></pre></td></tr></table></figure>

<p>zeroValue</p>
<blockquote>
<p>需要注意的是，<strong>zeroValue 会被每一个分区计算一次</strong>.<br>计算过程中的初始值，同时也确定了最终结果的类型</p>
</blockquote>
<p>seqOp 函数 : 对分区中的元素进行迭代计算，将一个分区中的所有元素聚合为一个 U 类型的结果。</p>
<blockquote>
<p>参数U 的来源 : 1 zeroValue ; 2 seqOp 的输出结果。即当前的聚合结果。<br>参数T 的来源 : 原始数据分区中的元素</p>
</blockquote>
<p>combOp 函数 : 对 seqOp 函数 产生的结果进行聚合。</p>
<blockquote>
<p>参数U 的来源 : seqOp 进行聚合后，产生的结果。</p>
</blockquote>
<p><strong>aggregate()使用举例：计算平均数</strong></p>
<p>要算平均值，需要两个值: 一个是rdd的各元素的累加和，另一个是元素计数，初始化为<code>(0, 0)</code>。</p>
<p>初始如下:</p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val l = <span class="built_in">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">l.aggregate(<span class="number">0</span>, <span class="number">0</span>)(seqOp, combOp)</span><br></pre></td></tr></table></figure>

<p>seqOp如下：</p>
<figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(x, y) =&gt; (x._1 + y, x._2 + <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>x</code> 代表的是zeroValue初始值或者seqOp的返回值，即是<code>(Int, Int)</code>啊，可以用<code>x._1</code>和<code>x._2</code>来代指这两个元素的，</li>
<li><code>y</code>代表rdd遍历过程中的元素</li>
</ul>
<p>因此<code>x._1 + y</code>就是各个元素的累加和，<code>x._2 + 1</code>就是元素计数。遍历完成后返回的<code>(Int, Int)</code>就是<code>累加和</code>和<code>元素计数</code>。</p>
<p>combOp如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(x, y) &#x3D;&gt; (x._1 + y._1, x._2 + y._2)</span><br></pre></td></tr></table></figure>

<p>因为我们的计算是分布式计算，<code>combOp</code>是将前面<code>seqOp</code>的结果进行合并的。</p>
<p>例如第一个节点遍历1和2, 返回的是<code>(3, 2)</code>，第二个节点遍历3和4, 返回的是<code>(7, 2)</code>，那么将它们合并的话就是<code>3 + 7, 2 + 2</code>.</p>
<p>最后程序是这样的：</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val l = List(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">r = l.aggregate(<span class="number">0</span>, <span class="number">0</span>)(<span class="function">(<span class="params">x, y</span>) =&gt;</span> <span class="function">(<span class="params">x._1 + y, x._2 + <span class="number">1</span></span>), (<span class="params">x, y</span>) =&gt;</span> (x._1 + y._1, x._2 + y._2))</span><br><span class="line">m = r._1 / r._2.toFload</span><br></pre></td></tr></table></figure>

<p><code>m</code>就是所要求的均值。</p>
<h4 id="1-5-6-count"><a href="#1-5-6-count" class="headerlink" title="1.5.6 count()"></a>1.5.6 count()</h4><p>返回数据集的元素个数</p>
<h4 id="1-5-7-collect"><a href="#1-5-7-collect" class="headerlink" title="1.5.7 collect()"></a>1.5.7 collect()</h4><p>以数组的形式，返回数据集的所有元素到Driver节点。collect()函数通常在使用filter或者其它操作减少数据量的函数后再使用。因为如果返回的数据量很大，很可能会让Driver程序OOM</p>
<h4 id="1-5-8-take-n"><a href="#1-5-8-take-n" class="headerlink" title="1.5.8 take(n)"></a>1.5.8 take(n)</h4><p>take用于获取RDD中从0到n-1下标的元素，不排序。</p>
<h4 id="1-5-9-top"><a href="#1-5-9-top" class="headerlink" title="1.5.9  top"></a>1.5.9  top</h4><p>如果为元素定义了顺序，就可以使用top返回前几个元素。</p>
<h4 id="1-5-10-takeSample-withReplacement-num-seed"><a href="#1-5-10-takeSample-withReplacement-num-seed" class="headerlink" title="1.5.10  takeSample(withReplacement,num,seed)"></a>1.5.10  takeSample(withReplacement,num,seed)</h4><p>根据指定的种子seed,采样返回指定个数num的元素，并以 数组 的形式返回。</p>
<h2 id="二、键值对RDD"><a href="#二、键值对RDD" class="headerlink" title="二、键值对RDD"></a>二、键值对RDD</h2><p><code>PairRDDFunctions</code>封装了用于操作键值对RDD的一些功能函数。一些文件读取操作(<code>sc.sequenceFile()</code>等)会直接返回RDD[(K, V)]类型。在RDD上使用map操作也可以将一个RDD转换为RDD[(K, V)]类型。在用Scala书写的Spark程序中，RDD[(K, V)]类型到PairRDDFunctions类型的转换一般由隐式转换函数完成。</p>
<h3 id="2-1-transformation操作"><a href="#2-1-transformation操作" class="headerlink" title="2.1  transformation操作"></a>2.1  transformation操作</h3><h4 id="2-1-1-mapValues"><a href="#2-1-1-mapValues" class="headerlink" title="2.1.1 mapValues"></a>2.1.1 mapValues</h4><p>对各个键值对的值进行映射。该操作会保留RDD的分区信息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"hive"</span>,<span class="string">"spark"</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(list)</span><br><span class="line"><span class="keyword">val</span> pairRdd = rdd.map(x =&gt; (x,<span class="number">1</span>))</span><br><span class="line">pairRdd.mapValues(_+<span class="number">1</span>).collect.foreach(println)<span class="comment">//对每个value进行+1</span></span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(hadoop,2)</span><br><span class="line">(spark,2)</span><br><span class="line">(hive,2)</span><br><span class="line">(spark,2)</span><br></pre></td></tr></table></figure>

<h4 id="2-1-2-flatMapValues"><a href="#2-1-2-flatMapValues" class="headerlink" title="2.1.2 flatMapValues"></a>2.1.2 flatMapValues</h4><p>对各个键值对的值进行映射，并将最后结果展平。该操作会保留RDD的分区信息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = sc.parallelize([(<span class="string">"a"</span>, [<span class="string">"x"</span>, <span class="string">"y"</span>, <span class="string">"z"</span>]), (<span class="string">"b"</span>, [<span class="string">"p"</span>, <span class="string">"r"</span>])])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span></span>(x): <span class="keyword">return</span> x</span><br><span class="line">x.flatMapValues(f).collect()</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#39;a&#39;, &#39;x&#39;), (&#39;a&#39;, &#39;y&#39;), (&#39;a&#39;, &#39;z&#39;), (&#39;b&#39;, &#39;p&#39;), (&#39;b&#39;, &#39;r&#39;)]</span><br></pre></td></tr></table></figure>

<h3 id="2-2-聚合操作"><a href="#2-2-聚合操作" class="headerlink" title="2.2 聚合操作"></a>2.2 聚合操作</h3><h4 id="2-2-1-reduceByKey-func"><a href="#2-2-1-reduceByKey-func" class="headerlink" title="2.2.1 reduceByKey(func)"></a>2.2.1 reduceByKey(func)</h4><p>使用 func 函数合并具有相同键的值。reduceByKey只是对键相同的值进行规约，并最终形成RDD[(K, V)]，而不像reduce那样返回单独一个“值”。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"hive"</span>,<span class="string">"spark"</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(list)</span><br><span class="line"><span class="keyword">val</span> pairRdd = rdd.map((_,<span class="number">1</span>))</span><br><span class="line">pairRdd.reduceByKey(_+_).collect.foreach(println)</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(hive,1)</span><br><span class="line">(spark,2)</span><br><span class="line">(hadoop,1)</span><br></pre></td></tr></table></figure>

<h4 id="2-2-2-foldByKey"><a href="#2-2-2-foldByKey" class="headerlink" title="2.2.2 foldByKey"></a>2.2.2 foldByKey</h4><p>与fold类似，就像reduceByKey之于reduce那样。</p>
<p>熟悉MapReduce中的合并器(combiner)概念的你可能已经注意到，<code>reduceByKey</code>和<code>foldByKey</code>会在为每个键计算全局的总结果之前先自动在每台机器上进行本地合并。用户不需要指定合并器。</p>
<h4 id="2-2-3-combineByKey"><a href="#2-2-3-combineByKey" class="headerlink" title="2.2.3 combineByKey"></a>2.2.3 combineByKey</h4><p>是最常用的基于键进行聚合的函数，大多数基于键聚合的函数都是用它实现的。与aggregate一样，combineByKey可以让用户返回与输入数据的类型不同的返回值。combineByKey的内部实现分为三步来完成：</p>
<ul>
<li>首先根据是否需要在map端进行combine操作决定是否对RDD先进行一次mapPartitions操作(利用createCombiner、mergeValue、mergeCombiners三个函数)来达到减少shuffle数据量的作用。</li>
<li>第二步根据partitioner对MapPartitionsRDD进行shuffle操作。</li>
<li>最后在reduce端对shuffle的结果再进行一次combine操作。</li>
</ul>
<h3 id="2-3-分组操作"><a href="#2-3-分组操作" class="headerlink" title="2.3 分组操作"></a>2.3 分组操作</h3><h4 id="2-3-1-groupBy"><a href="#2-3-1-groupBy" class="headerlink" title="2.3.1 groupBy"></a>2.3.1 groupBy</h4><p>根据自定义的东东进行分组。groupBy是基本RDD就有的操作。</p>
<h4 id="2-3-2-groupByKey"><a href="#2-3-2-groupByKey" class="headerlink" title="2.3.2 groupByKey"></a>2.3.2 groupByKey</h4><p>根据键对数据进行分组。虽然groupByKey+reduce也可以实现reduceByKey一样的效果.</p>
<p>但是请你记住：groupByKey是低效的，而reduceByKey会在本地先进行聚合，然后再通过网络传输求得最终结果。在执行聚合或分组操作时，可以指定分区数以对并行度进行调优。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>(<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"hive"</span>,<span class="string">"spark"</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(list)</span><br><span class="line"><span class="keyword">val</span> pairRdd = rdd.map(x =&gt; (x,<span class="number">1</span>))</span><br><span class="line">pairRdd.groupByKey().collect.foreach(println)</span><br></pre></td></tr></table></figure>

<p>结果:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(hive,CompactBuffer(1))</span><br><span class="line">(spark,CompactBuffer(1, 1))</span><br><span class="line">(hadoop,CompactBuffer(1))</span><br></pre></td></tr></table></figure>

<h3 id="2-4-连接操作"><a href="#2-4-连接操作" class="headerlink" title="2.4 连接操作"></a>2.4 连接操作</h3><h4 id="2-4-1-cogroup"><a href="#2-4-1-cogroup" class="headerlink" title="2.4.1 cogroup"></a>2.4.1 cogroup</h4><p>可以对多个RDD进行连接、分组、甚至求键的交集。其他的连接操作都是基于cogroup实现的。</p>
<h4 id="2-4-2-join"><a href="#2-4-2-join" class="headerlink" title="2.4.2 join"></a>2.4.2 join</h4><p>对数据进行内连接，也即当两个键值对RDD中都存在对应键时才输出。当一个输入对应的某个键有多个值时，生成的键值对RDD会包含来自两个输入RDD的每一组相对应的记录，也即笛卡尔积。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="string">"a1"</span>),(<span class="string">"B"</span>,<span class="string">"b1"</span>),(<span class="string">"C"</span>,<span class="string">"c1"</span>),(<span class="string">"D"</span>,<span class="string">"d1"</span>),(<span class="string">"E"</span>,<span class="string">"e1"</span>),(<span class="string">"F"</span>,<span class="string">"f1"</span>)))</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> b = sc.parallelize(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="string">"a2"</span>),(<span class="string">"B"</span>,<span class="string">"b2"</span>),(<span class="string">"C"</span>,<span class="string">"c1"</span>),(<span class="string">"C"</span>,<span class="string">"c2"</span>),(<span class="string">"C"</span>,<span class="string">"c3"</span>),(<span class="string">"E"</span>,<span class="string">"e2"</span>)))</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"> </span><br><span class="line">scala&gt; a.join(b).collect    <span class="comment">// 这里的join是inner join，只返回左右都匹配上的内容</span></span><br><span class="line"> </span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">Array</span>((<span class="type">B</span>,(b1,b2)), (<span class="type">A</span>,(a1,a2)), (<span class="type">C</span>,(c1,c1)), (<span class="type">C</span>,(c1,c2)), (<span class="type">C</span>,(c1,c3)), (<span class="type">E</span>,(e1,e2)))</span><br><span class="line"> </span><br><span class="line">scala&gt; b.join(a).collect    </span><br><span class="line">res2: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">Array</span>((<span class="type">B</span>,(b2,b1)), (<span class="type">A</span>,(a2,a1)), (<span class="type">C</span>,(c1,c1)), (<span class="type">C</span>,(c2,c1)), (<span class="type">C</span>,(c3,c1)), (<span class="type">E</span>,(e2,e1)))</span><br></pre></td></tr></table></figure>

<h4 id="2-4-3-leftOuterJoin"><a href="#2-4-3-leftOuterJoin" class="headerlink" title="2.4.3 leftOuterJoin"></a>2.4.3 leftOuterJoin</h4><p>即左外连接，源RDD的每一个键都有对应的记录，第二个RDD的值可能缺失，因此用Option表示。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; a.leftOuterJoin(b).collect</span><br><span class="line">res3: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">Option</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="type">B</span>,(b1,<span class="type">Some</span>(b2))), (<span class="type">F</span>,(f1,<span class="type">None</span>)), (<span class="type">D</span>,(d1,<span class="type">None</span>)), (<span class="type">A</span>,(a1,<span class="type">Some</span>(a2))), (<span class="type">C</span>,(c1,<span class="type">Some</span>(c1))), (<span class="type">C</span>,(c1,<span class="type">Some</span>(c2))), (<span class="type">C</span>,(c1,<span class="type">Some</span>(c3))), (<span class="type">E</span>,(e1,<span class="type">Some</span>(e2))))</span><br><span class="line"> </span><br><span class="line">scala&gt; b.leftOuterJoin(a).collect</span><br><span class="line">res5: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">Option</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="type">B</span>,(b2,<span class="type">Some</span>(b1))), (<span class="type">A</span>,(a2,<span class="type">Some</span>(a1))), (<span class="type">C</span>,(c1,<span class="type">Some</span>(c1))), (<span class="type">C</span>,(c2,<span class="type">Some</span>(c1))), (<span class="type">C</span>,(c3,<span class="type">Some</span>(c1))), (<span class="type">E</span>,(e2,<span class="type">Some</span>(e1))))</span><br></pre></td></tr></table></figure>

<h4 id="2-4-4-rightOuterJoin"><a href="#2-4-4-rightOuterJoin" class="headerlink" title="2.4.4 rightOuterJoin"></a>2.4.4 rightOuterJoin</h4><p>即右外连接，与左外连接相反。</p>
<h4 id="2-4-5-fullOuterJoin"><a href="#2-4-5-fullOuterJoin" class="headerlink" title="2.4.5 fullOuterJoin"></a>2.4.5 fullOuterJoin</h4><p>即全外连接，它是是左右外连接的并集。</p>
<p>如果一个RDD需要在多次连接操作中使用，对该RDD分区并持久化分区后的RDD是有益的，它可以避免不必要的shuffle。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a = sc.parallelize(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="string">"a1"</span>),(<span class="string">"B"</span>,<span class="string">"b1"</span>),(<span class="string">"C"</span>,<span class="string">"c1"</span>),(<span class="string">"D"</span>,<span class="string">"d1"</span>),(<span class="string">"E"</span>,<span class="string">"e1"</span>),(<span class="string">"F"</span>,<span class="string">"f1"</span>)))</span><br><span class="line">a: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">49</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"> </span><br><span class="line">scala&gt; <span class="keyword">val</span> b = sc.parallelize(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="string">"a2"</span>),(<span class="string">"B"</span>,<span class="string">"b2"</span>),(<span class="string">"C"</span>,<span class="string">"c1"</span>),(<span class="string">"C"</span>,<span class="string">"c2"</span>),(<span class="string">"C"</span>,<span class="string">"c3"</span>),(<span class="string">"E"</span>,<span class="string">"e2"</span>)))</span><br><span class="line">b: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">50</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"> </span><br><span class="line">scala&gt; a.fullOuterJoin(b).collect</span><br><span class="line">res15: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Option</span>[<span class="type">String</span>], <span class="type">Option</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="type">B</span>,(<span class="type">Some</span>(b1),<span class="type">Some</span>(b2))), (<span class="type">F</span>,(<span class="type">Some</span>(f1),<span class="type">None</span>)), (<span class="type">D</span>,(<span class="type">Some</span>(d1),<span class="type">None</span>)), (<span class="type">A</span>,(<span class="type">Some</span>(a1),<span class="type">Some</span>(a2))), (<span class="type">C</span>,(<span class="type">Some</span>(c1),<span class="type">Some</span>(c1))), (<span class="type">C</span>,(<span class="type">Some</span>(c1),<span class="type">Some</span>(c2))), (<span class="type">C</span>,(<span class="type">Some</span>(c1),<span class="type">Some</span>(c3))), (<span class="type">E</span>,(<span class="type">Some</span>(e1),<span class="type">Some</span>(e2))))</span><br><span class="line"> </span><br><span class="line">scala&gt; b.fullOuterJoin(a).collect</span><br><span class="line">res16: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Option</span>[<span class="type">String</span>], <span class="type">Option</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="type">B</span>,(<span class="type">Some</span>(b2),<span class="type">Some</span>(b1))), (<span class="type">F</span>,(<span class="type">None</span>,<span class="type">Some</span>(f1))), (<span class="type">D</span>,(<span class="type">None</span>,<span class="type">Some</span>(d1))), (<span class="type">A</span>,(<span class="type">Some</span>(a2),<span class="type">Some</span>(a1))), (<span class="type">C</span>,(<span class="type">Some</span>(c1),<span class="type">Some</span>(c1))), (<span class="type">C</span>,(<span class="type">Some</span>(c2),<span class="type">Some</span>(c1))), (<span class="type">C</span>,(<span class="type">Some</span>(c3),<span class="type">Some</span>(c1))), (<span class="type">E</span>,(<span class="type">Some</span>(e2),<span class="type">Some</span>(e1))))</span><br></pre></td></tr></table></figure>

<h3 id="2-5-数据排序："><a href="#2-5-数据排序：" class="headerlink" title="2.5 数据排序："></a>2.5 数据排序：</h3><p>在基本类型RDD中，<code>sortBy()</code>可以用来排序，<code>max()</code>和<code>min()</code>则可以用来方便地获取最大值和最小值。另外，在OrderedRDDFunctions中，存在一个sortByKey()可以方便地对键值对RDD进行排序，通过spark提供的隐式转换函数可以将RDD自动地转换为OrderedRDDFunctions，并随意地使用它的排序功能。</p>
<h3 id="2-6行动操作："><a href="#2-6行动操作：" class="headerlink" title="2.6行动操作："></a>2.6行动操作：</h3><p>键值对RDD提供了一些额外的行动操作供我们随意使用。如下：</p>
<h4 id="2-6-1-countByKey"><a href="#2-6-1-countByKey" class="headerlink" title="2.6.1 countByKey"></a>2.6.1 countByKey</h4><p>对每个键对应的元素分别计数。</p>
<h4 id="2-6-2-collectAsMap"><a href="#2-6-2-collectAsMap" class="headerlink" title="2.6.2 collectAsMap"></a>2.6.2 collectAsMap</h4><p>将结果以Map的形式返回，以便查询。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data &#x3D; sc.parallelize(List((1, &quot;www&quot;), (1, &quot;iteblog&quot;), (1, &quot;com&quot;), </span><br><span class="line">　　　　(2, &quot;bbs&quot;), (2, &quot;iteblog&quot;), (2, &quot;com&quot;), (3, &quot;good&quot;)))</span><br><span class="line">data: org.apache.spark.rdd.RDD[(Int, String)] &#x3D;</span><br><span class="line">　　　　ParallelCollectionRDD[26] at parallelize at &lt;console&gt;:12</span><br><span class="line"> </span><br><span class="line">scala&gt; data.collectAsMap</span><br><span class="line">res28: scala.collection.Map[Int,String] &#x3D; Map(2 -&gt; com, 1 -&gt; com, 3 -&gt; good)</span><br></pre></td></tr></table></figure>

<p>从结果我们可以看出，如果RDD中同一个Key中存在多个Value，那么后面的Value将会把前面的Value覆盖，最终得到的结果就是Key唯一，而且对应一个Value。</p>
<h4 id="2-6-3-lookup"><a href="#2-6-3-lookup" class="headerlink" title="2.6.3 lookup:"></a>2.6.3 lookup:</h4><p>lookup用于(K,V)类型的RDD,指定K值，返回RDD中该K对应的所有V值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="number">0</span>),(<span class="string">"A"</span>,<span class="number">2</span>),(<span class="string">"B"</span>,<span class="number">1</span>),(<span class="string">"B"</span>,<span class="number">2</span>),(<span class="string">"C"</span>,<span class="number">1</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at :<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.lookup(<span class="string">"A"</span>)</span><br><span class="line">res0: <span class="type">Seq</span>[<span class="type">Int</span>] = <span class="type">WrappedArray</span>(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.lookup(<span class="string">"B"</span>)</span><br><span class="line">res1: <span class="type">Seq</span>[<span class="type">Int</span>] = <span class="type">WrappedArray</span>(<span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h2 id="三、数值Rdd"><a href="#三、数值Rdd" class="headerlink" title="三、数值Rdd"></a>三、数值Rdd</h2><p>DoubleRDDFunctions为包含数值数据的RDD提供了一些描述性的统计操作，RDD可以通过隐式转换方便地使用这些方便的功能。</p>
<p>这些数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用stats()时通过一次遍历数据计算出来，并以StatCounter对象返回。如果你只想计算这些统计数据中的一个，也可以直接对RDD调用对应的方法。更多信息参见Spark API。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://smallx.me/2016/06/07/spark%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/" target="_blank" rel="noopener">spark使用总结</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/%E5%87%BD%E6%95%B0DataFrame/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/%E5%87%BD%E6%95%B0DataFrame/" class="post-title-link" itemprop="url">DataFrame函数</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 00:00:01" itemprop="dateCreated datePublished" datetime="2017-06-04T00:00:01+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/" itemprop="url" rel="index">
                    <span itemprop="name">语法解释</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/03/spark/%E8%AF%AD%E6%B3%95%E8%A7%A3%E9%87%8A/%E5%87%BD%E6%95%B0DataFrame/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/03/spark/语法解释/函数DataFrame/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="DataFrame函数"><a href="#DataFrame函数" class="headerlink" title="DataFrame函数"></a>DataFrame函数</h1><p>[TOC]</p>
<h2 id="一、Action-操作"><a href="#一、Action-操作" class="headerlink" title="一、Action 操作"></a>一、Action 操作</h2><h3 id="1-1-collect"><a href="#1-1-collect" class="headerlink" title="1.1 collect()"></a>1.1 collect()</h3><p>返回值是一个数组，返回dataframe集合所有的行</p>
<h3 id="1-2-collectAsList"><a href="#1-2-collectAsList" class="headerlink" title="1.2 collectAsList()"></a>1.2 collectAsList()</h3><p>返回值是一个java类型的数组，返回dataframe集合所有的行</p>
<h3 id="1-3-count"><a href="#1-3-count" class="headerlink" title="1.3  count()"></a>1.3  count()</h3><p>返回一个number类型的，返回dataframe集合的行数</p>
<h3 id="1-4-describe-cols-String"><a href="#1-4-describe-cols-String" class="headerlink" title="1.4 describe(cols: String*)"></a>1.4 describe(cols: String*)</h3><p>描述某些列的count, mean, stddev, min, max。这个可以传多个参数，中间用逗号分隔，如果有字段为空，那么不参与运算。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.describe(<span class="string">"age"</span>, <span class="string">"height"</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="1-5-first"><a href="#1-5-first" class="headerlink" title="1.5 first()"></a>1.5 first()</h3><p>返回第一行 ，类型是row类型</p>
<h3 id="1-6-head"><a href="#1-6-head" class="headerlink" title="1.6 head()"></a>1.6 head()</h3><p>返回第一行 ，类型是row类型</p>
<h3 id="1-7-head-n-Int"><a href="#1-7-head-n-Int" class="headerlink" title="1.7 head(n:Int)"></a>1.7 head(n:Int)</h3><p>返回n行  ，类型是row 类型</p>
<h3 id="1-8-show"><a href="#1-8-show" class="headerlink" title="1.8 show()"></a>1.8 show()</h3><p>返回dataframe集合的值 默认是20行，返回类型是unit</p>
<h3 id="1-9-show-n-Int"><a href="#1-9-show-n-Int" class="headerlink" title="1.9  show(n:Int)"></a>1.9  show(n:Int)</h3><p>返回n行，返回值类型是unit</p>
<h3 id="1-10-table-n-Int"><a href="#1-10-table-n-Int" class="headerlink" title="1.10 table(n:Int)"></a>1.10 table(n:Int)</h3><p>返回n行  ，类型是row 类型</p>
<h2 id="二、-dataframe的基本操作"><a href="#二、-dataframe的基本操作" class="headerlink" title="二、 dataframe的基本操作"></a>二、 dataframe的基本操作</h2><h3 id="2-1-cache"><a href="#2-1-cache" class="headerlink" title="2.1 cache()"></a>2.1 cache()</h3><p>将DataFrame缓存到内存中，以便之后加速运算</p>
<h3 id="2-2-columns"><a href="#2-2-columns" class="headerlink" title="2.2 columns"></a>2.2 columns</h3><p>返回一个string类型的数组，返回值是所有列的名字</p>
<h3 id="2-3-dtypes"><a href="#2-3-dtypes" class="headerlink" title="2.3 dtypes"></a>2.3 dtypes</h3><p>返回一个string类型的二维数组，返回值是所有列的名字以及类型</p>
<h3 id="2-4-explan"><a href="#2-4-explan" class="headerlink" title="2.4 explan()"></a>2.4 explan()</h3><p>打印物理执行计划  </p>
<h3 id="2-5-explain-n-Boolean"><a href="#2-5-explain-n-Boolean" class="headerlink" title="2.5 explain(n:Boolean)"></a>2.5 explain(n:Boolean)</h3><p>输入值为 false 或者true ，返回值是unit  默认是false ，如果输入true 将会打印 逻辑的和物理的</p>
<h3 id="2-6-isLocal"><a href="#2-6-isLocal" class="headerlink" title="2.6 isLocal"></a>2.6 isLocal</h3><p>返回值是Boolean类型，如果允许模式是local返回true 否则返回false</p>
<h3 id="2-7-persist-newlevel-StorageLevel"><a href="#2-7-persist-newlevel-StorageLevel" class="headerlink" title="2.7 persist(newlevel:StorageLevel)"></a>2.7 persist(newlevel:StorageLevel)</h3><p>与cache类似。不同的是：cache只有一个默认的缓存级别MEMORY_ONLY ，而persist可以根据情况设置其它的缓存级别。</p>
<h3 id="2-8-printSchema"><a href="#2-8-printSchema" class="headerlink" title="2.8 printSchema()"></a>2.8 printSchema()</h3><p>打印出字段名称和类型 按照树状结构来打印</p>
<h3 id="2-9-registerTempTable-tablename-String"><a href="#2-9-registerTempTable-tablename-String" class="headerlink" title="2.9 registerTempTable(tablename:String)"></a>2.9 registerTempTable(tablename:String)</h3><p>返回Unit ，将df的对象只放在一张表里面，这个表随着对象的删除而删除了</p>
<h3 id="2-10-schema"><a href="#2-10-schema" class="headerlink" title="2.10 schema"></a>2.10 schema</h3><p>返回structType 类型，将字段名称和类型按照结构体类型返回</p>
<h3 id="2-11-toDF"><a href="#2-11-toDF" class="headerlink" title="2.11 toDF()"></a>2.11 toDF()</h3><p>返回一个新的dataframe类型的</p>
<h3 id="2-12-toDF-colnames：String"><a href="#2-12-toDF-colnames：String" class="headerlink" title="2.12 toDF(colnames：String*)"></a>2.12 toDF(colnames：String*)</h3><p>将参数中的几个字段返回一个新的dataframe类型的，</p>
<h3 id="2-13-unpersist"><a href="#2-13-unpersist" class="headerlink" title="2.13 unpersist()"></a>2.13 unpersist()</h3><p>返回dataframe.this.type 类型，去除模式中的数据</p>
<h3 id="2-14-unpersist-blocking-Boolean"><a href="#2-14-unpersist-blocking-Boolean" class="headerlink" title="2.14 unpersist(blocking:Boolean)"></a>2.14 unpersist(blocking:Boolean)</h3><p>返回dataframe.this.type类型 true 和unpersist是一样的作用false 是去除RDD</p>
<p>##三、集成查询：</p>
<h3 id="3-1-数据说明"><a href="#3-1-数据说明" class="headerlink" title="3.1 数据说明"></a>3.1 数据说明</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().appName(<span class="string">"Spark SQL basic example"</span>).config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>).getOrCreate()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> df = spark.read.format(<span class="string">"csv"</span>).option(<span class="string">"header"</span>, <span class="literal">true</span>).load(<span class="string">"/user/hadoop/csvdata/csvdata"</span>)</span><br><span class="line">df.show()</span><br><span class="line">scala &gt;</span><br><span class="line">scala&gt; df.show()</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">| id|name|subject|score|</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|   <span class="number">50</span>|</span><br><span class="line">|  <span class="number">6</span>|  n6|     s1|   <span class="number">60</span>|</span><br><span class="line">|  <span class="number">7</span>|  n6|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|   <span class="number">90</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|   <span class="number">70</span>|</span><br><span class="line">+---+----+-------+-----+</span><br></pre></td></tr></table></figure>

<h3 id="3-2-agg"><a href="#3-2-agg" class="headerlink" title="3.2 agg"></a>3.2 agg</h3><p>在整个数据集范围类进行聚合操作。该函数相当于：<code>ds.groupBy().agg(…)</code></p>
<ul>
<li>函数原型</li>
</ul>
<p>1、 agg(expers:column*) </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.agg(max(<span class="string">"age"</span>), avg(<span class="string">"salary"</span>))</span><br><span class="line">df.groupBy().agg(max(<span class="string">"age"</span>), avg(<span class="string">"salary"</span>))</span><br></pre></td></tr></table></figure>

<p>2、 agg(exprs: Map[String, String])  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.agg(<span class="type">Map</span>(<span class="string">"age"</span> -&gt; <span class="string">"max"</span>, <span class="string">"salary"</span> -&gt; <span class="string">"avg"</span>))</span><br><span class="line">df.groupBy().agg(<span class="type">Map</span>(<span class="string">"age"</span> -&gt; <span class="string">"max"</span>, <span class="string">"salary"</span> -&gt; <span class="string">"avg"</span>))</span><br></pre></td></tr></table></figure>

<p>3、 agg(aggExpr: (String, String), aggExprs: (String, String)*)  </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.agg(<span class="type">Map</span>(<span class="string">"age"</span> -&gt; <span class="string">"max"</span>, <span class="string">"salary"</span> -&gt; <span class="string">"avg"</span>))</span><br><span class="line">df.groupBy().agg(<span class="type">Map</span>(<span class="string">"age"</span> -&gt; <span class="string">"max"</span>, <span class="string">"salary"</span> -&gt; <span class="string">"avg"</span>))</span><br></pre></td></tr></table></figure>

<ul>
<li>例子：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.agg(<span class="string">"score"</span>-&gt;<span class="string">"avg"</span>, <span class="string">"score"</span>-&gt;<span class="string">"max"</span>, <span class="string">"score"</span>-&gt;<span class="string">"min"</span>, <span class="string">"score"</span>-&gt;<span class="string">"count"</span>).show()</span><br><span class="line">+----------+----------+----------+------------+</span><br><span class="line">|avg(score)|max(score)|min(score)|count(score)|</span><br><span class="line">+----------+----------+----------+------------+</span><br><span class="line">|      <span class="number">40.0</span>|        <span class="number">90</span>|        <span class="number">10</span>|          <span class="number">12</span>|</span><br><span class="line">+----------+----------+----------+------------+</span><br></pre></td></tr></table></figure>

<h3 id="3-3-groupBy"><a href="#3-3-groupBy" class="headerlink" title="3.3  groupBy"></a>3.3  groupBy</h3><p>使用指定的列对数据集进行分组，以便我们可以对其进行聚合。请参阅RelationalGroupedDataset获取所有可用的聚合函数。<br>这是groupBy的一个变体，它只能使用列名称对现有列进行分组（即不能构造表达式）。</p>
<ul>
<li>函数原型</li>
</ul>
<p>1、def groupBy(col1: String, cols: String<em>): RelationalGroupedDataset<br>2、def groupBy(cols: Column</em>): RelationalGroupedDataset </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(<span class="string">"age"</span>).agg(<span class="type">Map</span>(<span class="string">"age"</span> -&gt;<span class="string">"count"</span>)).show();</span><br><span class="line">df.groupBy(<span class="string">"age"</span>).avg().show();</span><br></pre></td></tr></table></figure>

<ul>
<li>例子1 </li>
</ul>
<p>在使用groupBy函数时，一般都是先分组，在使用agg等聚合函数对数据进行聚合。<br>按name字段进行聚合，然后再使用agg聚合函数进行聚合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">"name"</span>).agg(<span class="string">"score"</span>-&gt;<span class="string">"avg"</span>).sort(<span class="string">"name"</span>).show()</span><br><span class="line">+----+----------+</span><br><span class="line">|name|avg(score)|</span><br><span class="line">+----+----------+</span><br><span class="line">|  n1|      <span class="number">10.0</span>|</span><br><span class="line">|  n2|      <span class="number">20.0</span>|</span><br><span class="line">|  n3|      <span class="number">25.0</span>|</span><br><span class="line">|  n4|      <span class="number">40.0</span>|</span><br><span class="line">|  n5|      <span class="number">50.0</span>|</span><br><span class="line">|  n6|      <span class="number">50.0</span>|</span><br><span class="line">|  n8|      <span class="number">90.0</span>|</span><br><span class="line">|  n9|      <span class="number">40.0</span>|</span><br><span class="line">+----+----------+</span><br></pre></td></tr></table></figure>

<ul>
<li>例子2 </li>
</ul>
<p>按id和name两个字段对数据集进行分组，然后求score列的平均值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">"id"</span>,<span class="string">"name"</span>).agg(<span class="string">"score"</span>-&gt;<span class="string">"avg"</span>).sort(<span class="string">"name"</span>).show()</span><br><span class="line">+---+----+----------+</span><br><span class="line">| id|name|avg(score)|</span><br><span class="line">+---+----+----------+</span><br><span class="line">|  <span class="number">1</span>|  n1|      <span class="number">10.0</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|      <span class="number">20.0</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|      <span class="number">25.0</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|      <span class="number">40.0</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|      <span class="number">50.0</span>|</span><br><span class="line">|  <span class="number">7</span>|  n6|      <span class="number">40.0</span>|</span><br><span class="line">|  <span class="number">6</span>|  n6|      <span class="number">60.0</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|      <span class="number">90.0</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|      <span class="number">45.0</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|      <span class="number">30.0</span>|</span><br><span class="line">+---+----+----------+</span><br></pre></td></tr></table></figure>

<ul>
<li>例子3 </li>
</ul>
<p>计算每个subject的平均分数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">"subject"</span>).agg(<span class="string">"score"</span>-&gt;<span class="string">"avg"</span>).sort(<span class="string">"subject"</span>).show()</span><br><span class="line">+-------+------------------+</span><br><span class="line">|subject|        avg(score)|</span><br><span class="line">+-------+------------------+</span><br><span class="line">|     s1|              <span class="number">28.0</span>|</span><br><span class="line">|     s2|              <span class="number">42.5</span>|</span><br><span class="line">|     s3|<span class="number">56.666666666666664</span>|</span><br><span class="line">+-------+------------------+</span><br></pre></td></tr></table></figure>

<ul>
<li>例子4 </li>
</ul>
<p>同时计算多个列值的平均值，最小值，最大值。<br>（注：我这里用的是同一列，完全可以是不同列）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.groupBy(<span class="string">"subject"</span>).agg(<span class="string">"score"</span>-&gt;<span class="string">"avg"</span>, <span class="string">"score"</span>-&gt;<span class="string">"max"</span>, <span class="string">"score"</span>-&gt;<span class="string">"min"</span>, <span class="string">"score"</span>-&gt;<span class="string">"count"</span>).sort(<span class="string">"subject"</span>).show()</span><br><span class="line">+-------+------------------+----------+----------+------------+</span><br><span class="line">|subject|        avg(score)|max(score)|min(score)|count(score)|</span><br><span class="line">+-------+------------------+----------+----------+------------+</span><br><span class="line">|     s1|              <span class="number">28.0</span>|        <span class="number">60</span>|        <span class="number">10</span>|           <span class="number">5</span>|</span><br><span class="line">|     s2|              <span class="number">42.5</span>|        <span class="number">70</span>|        <span class="number">20</span>|           <span class="number">4</span>|</span><br><span class="line">|     s3|<span class="number">56.666666666666664</span>|        <span class="number">90</span>|        <span class="number">30</span>|           <span class="number">3</span>|</span><br><span class="line">+-------+------------------+----------+----------+------------+</span><br></pre></td></tr></table></figure>

<h3 id="3-4-apply-和-col"><a href="#3-4-apply-和-col" class="headerlink" title="3.4 apply 和 col"></a>3.4 apply 和 col</h3><p>根据列名选择列并将其作为列返回。</p>
<ul>
<li>函数原型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(colName: <span class="type">String</span>): <span class="type">Column</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">col</span></span>(colName: <span class="type">String</span>): <span class="type">Column</span></span><br></pre></td></tr></table></figure>

<ul>
<li>例子1</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.apply(<span class="string">"name"</span>)</span><br><span class="line">res11: org.apache.spark.sql.<span class="type">Column</span> = name</span><br><span class="line"></span><br><span class="line">scala&gt; df.col(<span class="string">"name"</span>)</span><br><span class="line">res16: org.apache.spark.sql.<span class="type">Column</span> = name</span><br></pre></td></tr></table></figure>

<h3 id="3-7-cube"><a href="#3-7-cube" class="headerlink" title="3.7 cube"></a>3.7 cube</h3><p>使用指定的列为当前数据集创建一个多维数据集，因此我们可以对它们运行聚合。请参阅RelationalGroupedDataset获取所有可用的聚合函数。<br>这是立方体的变体，只能使用列名称对现有列进行分组（即不能构造表达式）。</p>
<ul>
<li>原型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cube</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cube</span></span>(cols: <span class="type">Column</span>*): <span class="type">RelationalGroupedDataset</span></span><br></pre></td></tr></table></figure>

<ul>
<li>例子1</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.cube(<span class="string">"name"</span>, <span class="string">"score"</span>)</span><br><span class="line">res18: org.apache.spark.sql.<span class="type">RelationalGroupedDataset</span> = org.apache.spark.sql.<span class="type">RelationalGroupedDataset</span>@<span class="number">3</span>f88db17</span><br></pre></td></tr></table></figure>

<h3 id="3-8-drop"><a href="#3-8-drop" class="headerlink" title="3.8 drop"></a>3.8 drop</h3><p>删除数据集中的某个列。</p>
<ul>
<li>函数原型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(col: <span class="type">Column</span>): <span class="type">DataFrame</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(colNames: <span class="type">String</span>*): <span class="type">DataFrame</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop</span></span>(colName: <span class="type">String</span>): <span class="type">DataFrame</span></span><br></pre></td></tr></table></figure>

<ul>
<li>例子1</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.drop(<span class="string">"id"</span>).show()</span><br><span class="line">+----+-------+-----+</span><br><span class="line">|name|subject|score|</span><br><span class="line">+----+-------+-----+</span><br><span class="line">|  n1|     s1|   <span class="number">10</span>|</span><br><span class="line">|  n2|     s2|   <span class="number">20</span>|</span><br><span class="line">|  n3|     s3|   <span class="number">30</span>|</span><br><span class="line">|  n3|     s1|   <span class="number">20</span>|</span><br><span class="line">|  n4|     s2|   <span class="number">40</span>|</span><br><span class="line">|  n5|     s3|   <span class="number">50</span>|</span><br><span class="line">|  n6|     s1|   <span class="number">60</span>|</span><br><span class="line">|  n6|     s2|   <span class="number">40</span>|</span><br><span class="line">|  n8|     s3|   <span class="number">90</span>|</span><br><span class="line">|  n9|     s1|   <span class="number">30</span>|</span><br><span class="line">|  n9|     s1|   <span class="number">20</span>|</span><br><span class="line">|  n9|     s2|   <span class="number">70</span>|</span><br><span class="line">+----+-------+-----+</span><br></pre></td></tr></table></figure>

<h3 id="3-9-join"><a href="#3-9-join" class="headerlink" title="3.9 join"></a>3.9 join</h3><ul>
<li>join类型的说明<br>内连接 : 只连接匹配的行<br>左外连接 : 包含左边表的全部行（不管右边的表中是否存在与它们匹配的行），以及右边表中全部匹配的行<br>右外连接 : 包含右边表的全部行（不管左边的表中是否存在与它们匹配的行），以及左边表中全部匹配的行<br>全外连接 : 包含左、右两个表的全部行，不管另外一边的表中是否存在与它们匹配的行。</li>
<li>功能说明<br>使用给定的连接表达式连接另一个DataFrame。以下执行df1和df2之间的完整外部联接。<br>使用给定的连接表达式与另一个DataFrame进行内部连接。<br>使用给定的列与另一个DataFrame进行设置连接。<br>加入另一个DataFrame。</li>
<li>函数原型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], joinExprs: <span class="type">Column</span>, joinType: <span class="type">String</span>): <span class="type">DataFrame</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], joinExprs: <span class="type">Column</span>): <span class="type">DataFrame</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumns: <span class="type">Seq</span>[<span class="type">String</span>], joinType: <span class="type">String</span>): <span class="type">DataFrame</span> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumns: <span class="type">Seq</span>[<span class="type">String</span>]): <span class="type">DataFrame</span> </span><br><span class="line"><span class="comment">// 内部使用给定的列与另一个DataFrame进行同等连接。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumn: <span class="type">String</span>): <span class="type">DataFrame</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_]): <span class="type">DataFrame</span></span><br></pre></td></tr></table></figure>

<p>注意：这里的joinType必须是这几个中的一个：<code>inner</code>, <code>outer</code>, <code>left_outer</code>, <code>right_outer</code>, <code>leftsemi</code>.</p>
<ul>
<li>例子1<br>该例子演示inner join。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.show()</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">| id|name|subject|score|</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|   <span class="number">50</span>|</span><br><span class="line">|  <span class="number">6</span>|  n6|     s1|   <span class="number">60</span>|</span><br><span class="line">|  <span class="number">7</span>|  n6|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|   <span class="number">90</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|   <span class="number">70</span>|</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df2 = df.select(<span class="string">"id"</span>, <span class="string">"subject"</span>,<span class="string">"score"</span>)</span><br><span class="line">df2: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: string, subject: string ... <span class="number">1</span> more field]</span><br><span class="line"></span><br><span class="line">scala&gt; df2.show()</span><br><span class="line">+---+-------+-----+</span><br><span class="line">| id|subject|score|</span><br><span class="line">+---+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">4</span>|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|     s3|   <span class="number">50</span>|</span><br><span class="line">|  <span class="number">6</span>|     s1|   <span class="number">60</span>|</span><br><span class="line">|  <span class="number">7</span>|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">8</span>|     s3|   <span class="number">90</span>|</span><br><span class="line">|  <span class="number">8</span>|     s1|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">9</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">9</span>|     s2|   <span class="number">70</span>|</span><br><span class="line">+---+-------+-----+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df3 = df.join(df2, df(<span class="string">"id"</span>)===df2(<span class="string">"id"</span>))</span><br><span class="line"><span class="number">17</span>/<span class="number">12</span>/<span class="number">03</span> <span class="number">21</span>:<span class="number">40</span>:<span class="number">59</span> <span class="type">WARN</span> <span class="type">Column</span>: <span class="type">Constructing</span> trivially <span class="literal">true</span> equals predicate, <span class="symbol">'id</span>#<span class="number">0</span> = id#<span class="number">0</span>'. <span class="type">Perhaps</span> you need to use aliases.</span><br><span class="line">df3: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: string, name: string ... <span class="number">5</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; df3.show()</span><br><span class="line">+---+----+-------+-----+---+-------+-----+</span><br><span class="line">| id|name|subject|score| id|subject|score|</span><br><span class="line">+---+----+-------+-----+---+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|   <span class="number">10</span>|  <span class="number">1</span>|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|   <span class="number">20</span>|  <span class="number">2</span>|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|  <span class="number">3</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|  <span class="number">3</span>|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|  <span class="number">3</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|  <span class="number">3</span>|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|   <span class="number">40</span>|  <span class="number">4</span>|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|   <span class="number">50</span>|  <span class="number">5</span>|     s3|   <span class="number">50</span>|</span><br><span class="line">|  <span class="number">6</span>|  n6|     s1|   <span class="number">60</span>|  <span class="number">6</span>|     s1|   <span class="number">60</span>|</span><br><span class="line">|  <span class="number">7</span>|  n6|     s2|   <span class="number">40</span>|  <span class="number">7</span>|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|   <span class="number">90</span>|  <span class="number">8</span>|     s1|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|   <span class="number">90</span>|  <span class="number">8</span>|     s3|   <span class="number">90</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|   <span class="number">30</span>|  <span class="number">8</span>|     s1|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|   <span class="number">30</span>|  <span class="number">8</span>|     s3|   <span class="number">90</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|   <span class="number">20</span>|  <span class="number">9</span>|     s2|   <span class="number">70</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|   <span class="number">20</span>|  <span class="number">9</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|   <span class="number">70</span>|  <span class="number">9</span>|     s2|   <span class="number">70</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|   <span class="number">70</span>|  <span class="number">9</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">+---+----+-------+-----+---+-------+-----+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df4 = df2.limit(<span class="number">6</span>)</span><br><span class="line">df4: org.apache.spark.sql.<span class="type">Dataset</span>[org.apache.spark.sql.<span class="type">Row</span>] = [id: string, subject: string ... <span class="number">1</span> more field]</span><br><span class="line"></span><br><span class="line">scala&gt; df4.show()</span><br><span class="line">+---+-------+-----+</span><br><span class="line">| id|subject|score|</span><br><span class="line">+---+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">4</span>|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|     s3|   <span class="number">50</span>|</span><br><span class="line">+---+-------+-----+</span><br><span class="line"></span><br><span class="line">scala&gt; df.show()</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">| id|name|subject|score|</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|   <span class="number">50</span>|</span><br><span class="line">|  <span class="number">6</span>|  n6|     s1|   <span class="number">60</span>|</span><br><span class="line">|  <span class="number">7</span>|  n6|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|   <span class="number">90</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|   <span class="number">70</span>|</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df6 = df.join(df4, <span class="string">"id"</span>)</span><br><span class="line">df6: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: string, name: string ... <span class="number">4</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; df6.show()</span><br><span class="line">+---+----+-------+-----+-------+-----+</span><br><span class="line">| id|name|subject|score|subject|score|</span><br><span class="line">+---+----+-------+-----+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|   <span class="number">10</span>|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|   <span class="number">20</span>|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|   <span class="number">40</span>|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|   <span class="number">50</span>|     s3|   <span class="number">50</span>|</span><br><span class="line">+---+----+-------+-----+-------+-----+</span><br></pre></td></tr></table></figure>

<ul>
<li>例子2<br>本例说明left_outer的使用和结果。<br>注意：数据集df4和df与上例的相同。<br>小结：通过例子可以看到，left_outer的效果是，保留左边表格的所有id，即使右边的表没有这些id（关联字段的值）</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df4.show()</span><br><span class="line">+---+-------+-----+</span><br><span class="line">| id|subject|score|</span><br><span class="line">+---+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">4</span>|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|     s3|   <span class="number">50</span>|</span><br><span class="line">+---+-------+-----+</span><br><span class="line"></span><br><span class="line">scala&gt; df.show()</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">| id|name|subject|score|</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|   <span class="number">50</span>|</span><br><span class="line">|  <span class="number">6</span>|  n6|     s1|   <span class="number">60</span>|</span><br><span class="line">|  <span class="number">7</span>|  n6|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|   <span class="number">90</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|   <span class="number">70</span>|</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> df7 = df.join(df4, df(<span class="string">"id"</span>)===df4(<span class="string">"id"</span>), <span class="string">"left_outer"</span>)</span><br><span class="line"><span class="number">17</span>/<span class="number">12</span>/<span class="number">03</span> <span class="number">21</span>:<span class="number">53</span>:<span class="number">40</span> <span class="type">WARN</span> <span class="type">Column</span>: <span class="type">Constructing</span> trivially <span class="literal">true</span> equals predicate, <span class="symbol">'id</span>#<span class="number">0</span> = id#<span class="number">0</span>'. <span class="type">Perhaps</span> you need to use aliases.</span><br><span class="line">df7: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: string, name: string ... <span class="number">5</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; df7.show()</span><br><span class="line">+---+----+-------+-----+----+-------+-----+</span><br><span class="line">| id|name|subject|score|  id|subject|score|</span><br><span class="line">+---+----+-------+-----+----+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|   <span class="number">10</span>|   <span class="number">1</span>|     s1|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|   <span class="number">20</span>|   <span class="number">2</span>|     s2|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|   <span class="number">3</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|   <span class="number">3</span>|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|   <span class="number">3</span>|     s1|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|   <span class="number">3</span>|     s3|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|   <span class="number">40</span>|   <span class="number">4</span>|     s2|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|   <span class="number">50</span>|   <span class="number">5</span>|     s3|   <span class="number">50</span>|</span><br><span class="line">|  <span class="number">6</span>|  n6|     s1|   <span class="number">60</span>|<span class="literal">null</span>|   <span class="literal">null</span>| <span class="literal">null</span>|</span><br><span class="line">|  <span class="number">7</span>|  n6|     s2|   <span class="number">40</span>|<span class="literal">null</span>|   <span class="literal">null</span>| <span class="literal">null</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|   <span class="number">90</span>|<span class="literal">null</span>|   <span class="literal">null</span>| <span class="literal">null</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|   <span class="number">30</span>|<span class="literal">null</span>|   <span class="literal">null</span>| <span class="literal">null</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|   <span class="number">20</span>|<span class="literal">null</span>|   <span class="literal">null</span>| <span class="literal">null</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|   <span class="number">70</span>|<span class="literal">null</span>|   <span class="literal">null</span>| <span class="literal">null</span>|</span><br><span class="line">+---+----+-------+-----+----+-------+-----+</span><br></pre></td></tr></table></figure>

<h3 id="3-10-na"><a href="#3-10-na" class="headerlink" title="3.10 na"></a>3.10 na</h3><p>返回一个DataFrameNaFunctions以处理丢失的数据。</p>
<ul>
<li>函数原型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">na</span></span>: <span class="type">DataFrameNaFunctions</span></span><br></pre></td></tr></table></figure>

<p>注意：该函数会返回一个类型的类，该类包含了各种操作空列的函数。<br>这些函数包括：drop(),fill(),replace(),fillCol(),replaceCol()</p>
<ul>
<li>例子1</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 删除包含任何空值的行</span></span><br><span class="line">scala&gt; df.na.drop()</span><br></pre></td></tr></table></figure>

<ul>
<li>例子2</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用常量值填充空值</span></span><br><span class="line">scala&gt; df.na.fill(<span class="string">"null"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3-11-select"><a href="#3-11-select" class="headerlink" title="3.11 select"></a>3.11 select</h3><p>选择一组列。注意：该函数返回的是一个DataFrame类。</p>
<ul>
<li>函数原型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这是select的一个变体，只能使用列名选择现有的列（即不能构造表达式）。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(col: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">DataFrame</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">// 选择一组基于列的表达式。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(cols: <span class="type">Column</span>*): <span class="type">DataFrame</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">// 选择一组SQL表达式。这是接受SQL表达式的select的一个变体。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectExpr</span></span>(exprs: <span class="type">String</span>*): <span class="type">DataFrame</span></span><br></pre></td></tr></table></figure>

<ul>
<li>例子1</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select(<span class="string">"id"</span>, <span class="string">"score"</span>).show()</span><br><span class="line">+---+-----+</span><br><span class="line">| id|score|</span><br><span class="line">+---+-----+</span><br><span class="line">|  <span class="number">1</span>|   <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">4</span>|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|   <span class="number">50</span>|</span><br><span class="line">|  <span class="number">6</span>|   <span class="number">60</span>|</span><br><span class="line">|  <span class="number">7</span>|   <span class="number">40</span>|</span><br><span class="line">|  <span class="number">8</span>|   <span class="number">90</span>|</span><br><span class="line">|  <span class="number">8</span>|   <span class="number">30</span>|</span><br><span class="line">|  <span class="number">9</span>|   <span class="number">20</span>|</span><br><span class="line">|  <span class="number">9</span>|   <span class="number">70</span>|</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure>

<ul>
<li>例子2：对select的列值进行操作</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; df.select($<span class="string">"id"</span>, $<span class="string">"score"</span>*<span class="number">10</span>).show()</span><br><span class="line">+---+------------+</span><br><span class="line">| id|(score * <span class="number">10</span>)|</span><br><span class="line">+---+------------+</span><br><span class="line">|  <span class="number">1</span>|       <span class="number">100.0</span>|</span><br><span class="line">|  <span class="number">2</span>|       <span class="number">200.0</span>|</span><br><span class="line">|  <span class="number">3</span>|       <span class="number">300.0</span>|</span><br><span class="line">|  <span class="number">3</span>|       <span class="number">200.0</span>|</span><br><span class="line">|  <span class="number">4</span>|       <span class="number">400.0</span>|</span><br><span class="line">|  <span class="number">5</span>|       <span class="number">500.0</span>|</span><br><span class="line">|  <span class="number">6</span>|       <span class="number">600.0</span>|</span><br><span class="line">|  <span class="number">7</span>|       <span class="number">400.0</span>|</span><br><span class="line">|  <span class="number">8</span>|       <span class="number">900.0</span>|</span><br><span class="line">|  <span class="number">8</span>|       <span class="number">300.0</span>|</span><br><span class="line">|  <span class="number">9</span>|       <span class="number">200.0</span>|</span><br><span class="line">|  <span class="number">9</span>|       <span class="number">700.0</span>|</span><br><span class="line">+---+------------+</span><br></pre></td></tr></table></figure>

<ul>
<li>例子3：selectExpr的使用(select表达式)</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">ds.selectExpr(<span class="string">"colA"</span>, <span class="string">"colB as newName"</span>, <span class="string">"abs(colC)"</span>)</span><br><span class="line">或</span><br><span class="line">ds.select(expr(<span class="string">"colA"</span>), expr(<span class="string">"colB as newName"</span>), expr(<span class="string">"abs(colC)"</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; df.selectExpr(<span class="string">"id"</span>, <span class="string">"score * 10"</span>).show()</span><br><span class="line">+---+------------+</span><br><span class="line">| id|(score * <span class="number">10</span>)|</span><br><span class="line">+---+------------+</span><br><span class="line">|  <span class="number">1</span>|       <span class="number">100.0</span>|</span><br><span class="line">|  <span class="number">2</span>|       <span class="number">200.0</span>|</span><br><span class="line">|  <span class="number">3</span>|       <span class="number">300.0</span>|</span><br><span class="line">|  <span class="number">3</span>|       <span class="number">200.0</span>|</span><br><span class="line">|  <span class="number">4</span>|       <span class="number">400.0</span>|</span><br><span class="line">|  <span class="number">5</span>|       <span class="number">500.0</span>|</span><br><span class="line">|  <span class="number">6</span>|       <span class="number">600.0</span>|</span><br><span class="line">|  <span class="number">7</span>|       <span class="number">400.0</span>|</span><br><span class="line">|  <span class="number">8</span>|       <span class="number">900.0</span>|</span><br><span class="line">|  <span class="number">8</span>|       <span class="number">300.0</span>|</span><br><span class="line">|  <span class="number">9</span>|       <span class="number">200.0</span>|</span><br><span class="line">|  <span class="number">9</span>|       <span class="number">700.0</span>|</span><br><span class="line">+---+------------+</span><br><span class="line"></span><br><span class="line">或</span><br><span class="line">scala&gt; df.selectExpr(<span class="string">"id"</span>, <span class="string">"score as points"</span>).show()</span><br><span class="line">+---+------+</span><br><span class="line">| id|points|</span><br><span class="line">+---+------+</span><br><span class="line">|  <span class="number">1</span>|    <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|    <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|    <span class="number">30</span>|</span><br><span class="line">... ...</span><br></pre></td></tr></table></figure>

<h3 id="3-12-withColumn-和-withColumnRenamed"><a href="#3-12-withColumn-和-withColumnRenamed" class="headerlink" title="3.12 withColumn 和 withColumnRenamed"></a>3.12 withColumn 和 withColumnRenamed</h3><p>通过添加一列或替换具有相同名称的现有列来返回新的数据集。<br>withColumnRenamed只是重命名列。 </p>
<ul>
<li>函数原型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withColumn</span></span>(colName: <span class="type">String</span>, col: <span class="type">Column</span>): <span class="type">DataFrame</span> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withColumnRenamed</span></span>(existingName: <span class="type">String</span>, newName: <span class="type">String</span>): <span class="type">DataFrame</span> <span class="number">12</span></span><br></pre></td></tr></table></figure>

<ul>
<li>例子1：通过重命名现有列来添加新列</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df8 = df.withColumn(<span class="string">"subs"</span>, df(<span class="string">"subject"</span>))</span><br><span class="line">df8: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: string, name: string ... <span class="number">3</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; df8.show()</span><br><span class="line">+---+----+-------+-----+----+</span><br><span class="line">| id|name|subject|score|subs|</span><br><span class="line">+---+----+-------+-----+----+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|   <span class="number">10</span>|  s1|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|   <span class="number">20</span>|  s2|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|   <span class="number">30</span>|  s3|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|   <span class="number">20</span>|  s1|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|   <span class="number">40</span>|  s2|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|   <span class="number">50</span>|  s3|</span><br><span class="line">|  <span class="number">6</span>|  n6|     s1|   <span class="number">60</span>|  s1|</span><br><span class="line">|  <span class="number">7</span>|  n6|     s2|   <span class="number">40</span>|  s2|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|   <span class="number">90</span>|  s3|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|   <span class="number">30</span>|  s1|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|   <span class="number">20</span>|  s1|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|   <span class="number">70</span>|  s2|</span><br><span class="line">+---+----+-------+-----+----+</span><br></pre></td></tr></table></figure>

<ul>
<li>例子2：重命名现有列，但不添加新列</li>
</ul>
<p>从下面的例子中可以看出，把score列的值替换了，但并没有添加新的列。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> df9 = df.withColumn(<span class="string">"score"</span>, df(<span class="string">"score"</span>)/<span class="number">100</span>)</span><br><span class="line">df9: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: string, name: string ... <span class="number">2</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; df9.show()</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">| id|name|subject|score|</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|  <span class="number">0.1</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|  <span class="number">0.2</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|  <span class="number">0.3</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|  <span class="number">0.2</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|  <span class="number">0.4</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|  <span class="number">0.5</span>|</span><br><span class="line">|  <span class="number">6</span>|  n6|     s1|  <span class="number">0.6</span>|</span><br><span class="line">|  <span class="number">7</span>|  n6|     s2|  <span class="number">0.4</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|  <span class="number">0.9</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|  <span class="number">0.3</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|  <span class="number">0.2</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|  <span class="number">0.7</span>|</span><br><span class="line">+---+----+-------+-----+</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 也可以直接通过withColumnRenamed进行重命名</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> df9 = df.withColumnRenamed(<span class="string">"score"</span>,<span class="string">"score2"</span>) </span><br><span class="line">df9: org.apache.spark.sql.<span class="type">DataFrame</span> = [id: string, name: string ... <span class="number">2</span> more fields]</span><br><span class="line"></span><br><span class="line">scala&gt; df9.show()</span><br><span class="line">+---+----+-------+------+</span><br><span class="line">| id|name|subject|score2|</span><br><span class="line">+---+----+-------+------+</span><br><span class="line">|  <span class="number">1</span>|  n1|     s1|    <span class="number">10</span>|</span><br><span class="line">|  <span class="number">2</span>|  n2|     s2|    <span class="number">20</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s3|    <span class="number">30</span>|</span><br><span class="line">|  <span class="number">3</span>|  n3|     s1|    <span class="number">20</span>|</span><br><span class="line">|  <span class="number">4</span>|  n4|     s2|    <span class="number">40</span>|</span><br><span class="line">|  <span class="number">5</span>|  n5|     s3|    <span class="number">50</span>|</span><br><span class="line">|  <span class="number">6</span>|  n6|     s1|    <span class="number">60</span>|</span><br><span class="line">|  <span class="number">7</span>|  n6|     s2|    <span class="number">40</span>|</span><br><span class="line">|  <span class="number">8</span>|  n8|     s3|    <span class="number">90</span>|</span><br><span class="line">|  <span class="number">8</span>|  n9|     s1|    <span class="number">30</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s1|    <span class="number">20</span>|</span><br><span class="line">|  <span class="number">9</span>|  n9|     s2|    <span class="number">70</span>|</span><br><span class="line">+---+----+-------+-----</span><br></pre></td></tr></table></figure>

<h3 id="3-13-stat"><a href="#3-13-stat" class="headerlink" title="3.13 stat"></a>3.13 stat</h3><p>为工作统计功能支持返回一个DataFrameStatFunctions。<br>该类的函数包括：approxQuantile,corr,cov,freqItems,sampleBy,countMinSketch,bloomFilter,buildBloomFilter等</p>
<ul>
<li>函数原型</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stat</span></span>: <span class="type">DataFrameStatFunctions</span></span><br></pre></td></tr></table></figure>

<ul>
<li>例子1</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> cols = <span class="type">Array</span>(<span class="string">"score"</span>)</span><br><span class="line">cols: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(score)</span><br><span class="line"></span><br><span class="line">scala&gt; df.stat.freqItems(cols)</span><br><span class="line">res56: org.apache.spark.sql.<span class="type">DataFrame</span> = [score_freqItems: array&lt;string&gt;]</span><br><span class="line"></span><br><span class="line">scala&gt; df.stat.freqItems(cols).show()</span><br><span class="line">+--------------------+</span><br><span class="line">|     score_freqItems|</span><br><span class="line">+--------------------+</span><br><span class="line">|[<span class="number">90</span>, <span class="number">30</span>, <span class="number">60</span>, <span class="number">50</span>, ...|</span><br><span class="line">+--------------------+</span><br></pre></td></tr></table></figure>

<h3 id="3-14-其他"><a href="#3-14-其他" class="headerlink" title="3.14 其他"></a>3.14 其他</h3><ul>
<li><p>as(alias: String) 返回一个新的dataframe类型，就是原来的一个别名</p>
</li>
<li><p>distinct 去重 返回一个dataframe类型</p>
</li>
<li><p>dropDuplicates(colNames: Array[String]) 删除相同的列 返回一个dataframe</p>
</li>
<li><p>except(other: DataFrame) 返回一个dataframe，返回在当前集合存在的在其他集合不存在的</p>
</li>
<li><p>explode[A, B](inputColumn: String, outputColumn: String)(f: (A) ⇒ TraversableOnce[B])(implicit arg0: scala.reflect.api.JavaUniverse.TypeTag[B]) 返回值是dataframe类型，这个 将一个字段进行更多行的拆分</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.explode(<span class="string">"name"</span>,<span class="string">"names"</span>) &#123;name :<span class="type">String</span>=&gt; name.split(<span class="string">" "</span>)&#125;.show();</span><br><span class="line">将name字段根据空格来拆分，拆分的字段放在names里面</span><br></pre></td></tr></table></figure>
</li>
<li><p>filter(conditionExpr: String): 刷选部分数据，返回dataframe类型 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.filter(<span class="string">"age&gt;10"</span>).show();   </span><br><span class="line">df.filter(df(<span class="string">"age"</span>)&gt;<span class="number">10</span>).show();   </span><br><span class="line">df.where(df(<span class="string">"age"</span>)&gt;<span class="number">10</span>).show();</span><br></pre></td></tr></table></figure>

</li>
</ul>
<ul>
<li>intersect(other: DataFrame) 返回一个dataframe，在2个dataframe都存在的元素</li>
<li>limit(n: Int) 返回dataframe类型  去n 条数据出来</li>
<li>orderBy(sortExprs: Column*)  做alise排序</li>
<li>sort(sortExprs: Column*)    排序 df.sort(df(“age”).desc).show(); 默认是asc</li>
<li>unionAll(other:Dataframe) 合并 df.unionAll(ds).show();</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/28/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/28/">28</a><span class="page-number current">29</span><a class="page-number" href="/page/30/">30</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="extend next" rel="next" href="/page/30/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhoul</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">351</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">92</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">190</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/longzl2015" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;longzl2015" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:289570126@qq.com" title="E-Mail → mailto:289570126@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/5276366/egg" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;5276366&#x2F;egg" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhoul</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://long12356-gitee-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>

</body>
</html>
