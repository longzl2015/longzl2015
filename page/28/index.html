<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('https://longzl2015.github.io').hostname,
    root: '/',
    scheme: 'Gemini',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"我们没有找到任何搜索结果: ${query}","hits_stats":"找到约${hits}条结果（用时${time}ms）"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://longzl2015.github.io/page/28/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="zhoul">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://longzl2015.github.io/page/28/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">190</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">92</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">351</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/quartz/%E6%97%B6%E9%92%9F%E4%B8%8D%E5%90%8C%E6%AD%A5%E9%80%A0%E6%88%90%E7%9A%84%E5%BC%82%E5%B8%B8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/quartz/%E6%97%B6%E9%92%9F%E4%B8%8D%E5%90%8C%E6%AD%A5%E9%80%A0%E6%88%90%E7%9A%84%E5%BC%82%E5%B8%B8/" class="post-title-link" itemprop="url">时钟不同步造成的异常</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/quartz/" itemprop="url" rel="index">
                    <span itemprop="name">quartz</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/quartz/%E6%97%B6%E9%92%9F%E4%B8%8D%E5%90%8C%E6%AD%A5%E9%80%A0%E6%88%90%E7%9A%84%E5%BC%82%E5%B8%B8/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/quartz/时钟不同步造成的异常/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="时钟不同步造成的异常"><a href="#时钟不同步造成的异常" class="headerlink" title="时钟不同步造成的异常"></a>时钟不同步造成的异常</h1><p>异常信息：</p>
<blockquote>
<p>This scheduler instance (<nodename>) is still active but was recovered by another instance in the cluster</p>
</blockquote>
<p>对应源码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">doCheckin</span><span class="params">()</span> <span class="keyword">throws</span> JobPersistenceException </span>&#123;</span><br><span class="line">     <span class="keyword">boolean</span> transOwner = <span class="keyword">false</span>;</span><br><span class="line">     <span class="keyword">boolean</span> transStateOwner = <span class="keyword">false</span>;</span><br><span class="line">     <span class="keyword">boolean</span> recovered = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">     Connection conn = getNonManagedTXConnection();</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">         <span class="comment">// Other than the first time, always checkin first to make sure there is </span></span><br><span class="line">         <span class="comment">// work to be done before we acquire the lock (since that is expensive, </span></span><br><span class="line">         <span class="comment">// and is almost never necessary).  This must be done in a separate</span></span><br><span class="line">         <span class="comment">// transaction to prevent a deadlock under recovery conditions.</span></span><br><span class="line">         List&lt;SchedulerStateRecord&gt; failedRecords = <span class="keyword">null</span>;</span><br><span class="line">         <span class="keyword">if</span> (!firstCheckIn) &#123;</span><br><span class="line">             failedRecords = clusterCheckIn(conn);</span><br><span class="line">             commitConnection(conn);</span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">         <span class="keyword">if</span> (firstCheckIn || (failedRecords.size() &gt; <span class="number">0</span>)) &#123;</span><br><span class="line">             getLockHandler().obtainLock(conn, LOCK_STATE_ACCESS);</span><br><span class="line">             transStateOwner = <span class="keyword">true</span>;</span><br><span class="line"> </span><br><span class="line">             <span class="comment">// Now that we own the lock, make sure we still have work to do. </span></span><br><span class="line">             <span class="comment">// The first time through, we also need to make sure we update/create our state record</span></span><br><span class="line">             failedRecords = (firstCheckIn) ? clusterCheckIn(conn) : findFailedInstances(conn);</span><br><span class="line"> </span><br><span class="line">             <span class="keyword">if</span> (failedRecords.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                 getLockHandler().obtainLock(conn, LOCK_TRIGGER_ACCESS);</span><br><span class="line">                 <span class="comment">//getLockHandler().obtainLock(conn, LOCK_JOB_ACCESS);</span></span><br><span class="line">                 transOwner = <span class="keyword">true</span>;</span><br><span class="line"> </span><br><span class="line">                 clusterRecover(conn, failedRecords);</span><br><span class="line">                 recovered = <span class="keyword">true</span>;</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line">         </span><br><span class="line">         commitConnection(conn);</span><br><span class="line">     &#125; <span class="keyword">catch</span> (JobPersistenceException e) &#123;</span><br><span class="line">         rollbackConnection(conn);</span><br><span class="line">         <span class="keyword">throw</span> e;</span><br><span class="line">     &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">         <span class="keyword">try</span> &#123;</span><br><span class="line">             releaseLock(LOCK_TRIGGER_ACCESS, transOwner);</span><br><span class="line">         &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">             <span class="keyword">try</span> &#123;</span><br><span class="line">                 releaseLock(LOCK_STATE_ACCESS, transStateOwner);</span><br><span class="line">             &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                 cleanupConnection(conn);</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     firstCheckIn = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> recovered;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> List&lt;SchedulerStateRecord&gt; <span class="title">findFailedInstances</span><span class="params">(Connection conn)</span></span></span><br><span class="line"><span class="function">        <span class="keyword">throws</span> JobPersistenceException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            List&lt;SchedulerStateRecord&gt; failedInstances = <span class="keyword">new</span> LinkedList&lt;SchedulerStateRecord&gt;();</span><br><span class="line">            <span class="keyword">boolean</span> foundThisScheduler = <span class="keyword">false</span>;</span><br><span class="line">            <span class="keyword">long</span> timeNow = System.currentTimeMillis();</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// 获取 qrzt_scheduler_state 表中，记录。对应sql是：SELECT * FROM QRTZ_SCHEDULER_STATE WHERE SCHED_NAME = 'zl'，其中SCHED_NAME是配置文件中的org.quartz.scheduler.instanceName值</span></span><br><span class="line">            List&lt;SchedulerStateRecord&gt; states = getDelegate().selectSchedulerStateRecords(conn, <span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(SchedulerStateRecord rec: states) &#123;</span><br><span class="line">        </span><br><span class="line">                <span class="comment">// find own record...</span></span><br><span class="line">                <span class="keyword">if</span> (rec.getSchedulerInstanceId().equals(getInstanceId())) &#123;</span><br><span class="line">                    foundThisScheduler = <span class="keyword">true</span>;</span><br><span class="line">                    <span class="keyword">if</span> (firstCheckIn) &#123;</span><br><span class="line">                        failedInstances.add(rec);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// find failed instances...</span></span><br><span class="line">                    <span class="keyword">if</span> (calcFailedIfAfter(rec) &lt; timeNow) &#123;</span><br><span class="line">                        failedInstances.add(rec);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// The first time through, also check for orphaned fired triggers.</span></span><br><span class="line">            <span class="keyword">if</span> (firstCheckIn) &#123;</span><br><span class="line">                failedInstances.addAll(findOrphanedFailedInstances(conn, states));</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// If not the first time but we didn't find our own instance, then</span></span><br><span class="line">            <span class="comment">// Someone must have done recovery for us.</span></span><br><span class="line">            <span class="comment">// !foundThisScheduler 表示 应用程序没有找到 自己的 instance</span></span><br><span class="line">            <span class="comment">// !firstCheckIn       表示 应该表示 应用程序是否为第一次checkIn</span></span><br><span class="line">            <span class="keyword">if</span> ((!foundThisScheduler) &amp;&amp; (!firstCheckIn)) &#123;</span><br><span class="line">                <span class="comment">// FUTURE_<span class="doctag">TODO:</span> revisit when handle self-failed-out impl'ed (see FUTURE_TODO in clusterCheckIn() below)</span></span><br><span class="line">                getLog().warn(</span><br><span class="line">                    <span class="string">"This scheduler instance ("</span> + getInstanceId() + <span class="string">") is still "</span> + </span><br><span class="line">                    <span class="string">"active but was recovered by another instance in the cluster.  "</span> +</span><br><span class="line">                    <span class="string">"This may cause inconsistent behavior."</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">return</span> failedInstances;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            lastCheckin = System.currentTimeMillis();</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> JobPersistenceException(<span class="string">"Failure identifying failed instances when checking-in: "</span></span><br><span class="line">                    + e.getMessage(), e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/spark%E6%95%99%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/spark%E6%95%99%E7%A8%8B/" class="post-title-link" itemprop="url">spark使用教程(转)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/spark%E6%95%99%E7%A8%8B/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/spark教程/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="spark使用教程"><a href="#spark使用教程" class="headerlink" title="spark使用教程"></a>spark使用教程</h1><p>文章来源：<a href="http://smallx.me/2016/06/07/spark%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/" target="_blank" rel="noopener">spark使用教程</a></p>
<blockquote>
<p>本文是spark的使用教程，文中主要用scala来讲解spark，并且会尽量覆盖较新版本的spark的内容。这篇文章主要记录了一些我平时学到的spark知识，虽然较长，但它并没有包含spark的方方面面，更多更全的spark教程和信息请在<a href="http://spark.apache.org/" target="_blank" rel="noopener">spark官网</a>观看。</p>
</blockquote>
<h1 id="第一个Spark程序"><a href="#第一个Spark程序" class="headerlink" title="第一个Spark程序"></a>第一个Spark程序</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 功能：用spark实现的单词计数程序</span></span><br><span class="line"><span class="comment"> * 环境：spark 1.6.1, scala 2.10.4</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 导入相关类库</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">// 建立spark运行上下文</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local[3]"</span>, <span class="string">"WordCount"</span>, <span class="keyword">new</span> <span class="type">SparkConf</span>())</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 加载数据，创建RDD</span></span><br><span class="line">    <span class="keyword">val</span> inRDD = sc.textFile(<span class="string">"words.txt"</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对RDD进行转换，得到最终结果</span></span><br><span class="line">    <span class="keyword">val</span> res = inRDD.flatMap(_.split(' ')).map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将计算结果collect到driver节点，并打印</span></span><br><span class="line">    res.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 停止spark运行上下文</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="关于RDD"><a href="#关于RDD" class="headerlink" title="关于RDD"></a>关于RDD</h1><p>弹性分布式数据集(RDD)是分布式处理的一个数据集的抽象，<strong>RDD是只读的，在RDD之上的操作都是并行的</strong>。实际上，RDD只是一个逻辑实体，其中存储了分布式数据集的一些信息，并没有包含所谓的“物理数据”，“物理数据”只有在RDD被计算并持久化之后才存在于内存或磁盘中。RDD的重要内部属性有：</p>
<ul>
<li>计算RDD分区的函数。</li>
<li>所依赖的直接父RDD列表。</li>
<li>RDD分区及其地址列表。</li>
<li>RDD分区器。</li>
<li>RDD分区优先位置。</li>
</ul>
<p>RDD操作起来与Scala集合类型没有太大差别，这就是Spark追求的目标：像编写单机程序一样编写分布式程序，但它们的数据和运行模型有很大的不同，用户需要具备更强的系统把控能力和分布式系统知识。</p>
<h2 id="Transformation与Action"><a href="#Transformation与Action" class="headerlink" title="Transformation与Action"></a>Transformation与Action</h2><p>RDD提供了两种类型的操作：<strong>transformation操作</strong>(转化操作)和<strong>action操作</strong>(行动操作)。transformation操作是得到一个新的<strong>RDD</strong>，方式很多，比如从数据源生成一个新的RDD，从RDD生成一个新的RDD。action操作则是得到<strong>其他数据类型</strong>的结果。</p>
<p>所有的transformation都是采用的懒策略，就是如果只是将transformation提交是不会执行计算的，spark在内部只是用新的RDD记录这些transformation操作并形成RDD对象的有向无环图(DAG)，计算只有在action被提交的时候才被触发。实际上，我们不应该把RDD看作存放着特定数据的数据集，而最好把每个RDD当作我们通过transformation操作构建出来的、记录如何计算数据的指令列表。</p>
<p>RDD的action算子会触发一个新的job，spark会在DAG中寻找是否有cached或者persisted的中间结果，如果没有找到，那么就会重新执行这些中间过程以重新计算该RDD。因此，如果想在多个action操作中重用同一个RDD，那么最好使用 <code>cache()</code>/<code>persist()</code>将RDD缓存在内存中，但如果RDD过大，那么最好使用 <code>persist(StorageLevel.MEMORY_AND_DISK)</code> 代替。注意cache/persist仅仅是设置RDD的存储等级，因此你应该在第一次调用action之前调用cache/persist。cache/persist使得中间计算结果存在内存中，这个才是说为啥Spark是内存计算引擎的地方。在MR里，你是要放到HDFS里的，但Spark允许你把中间结果放内存里。</p>
<p><strong>在spark程序中打印日志时，尤其需要注意打印日志的代码很有可能使用到了action算子，如果没有缓存中间RDD就可能导致程序的效率大大降低。另外，如果一个RDD的计算过程中有抽样、随机值或者其他形式的变化，那么一定要缓存中间结果，否则程序执行结果可能都是不准确的！</strong></p>
<blockquote>
<p>参考链接及进一步阅读：</p>
<ul>
<li><a href="http://www.jianshu.com/p/b70fe63a77a8" target="_blank" rel="noopener">Spark会把数据都载入到内存么？</a></li>
</ul>
<ul>
<li><a href="http://www.spark.tc/using-sparks-cache-for-correctness-not-just-performance/" target="_blank" rel="noopener">Using Spark’s cache for correctness, not just performance</a></li>
</ul>
</blockquote>
<h2 id="RDD持久化-缓存"><a href="#RDD持久化-缓存" class="headerlink" title="RDD持久化(缓存)"></a>RDD持久化(缓存)</h2><p>正如在转化和行动操作部分所说的一样，为了避免在一个RDD上多次调用action操作从而可能导致的重新计算，我们应该将该RDD在第一次调用action之前进行持久化。对RDD进行持久化对于迭代式和交互式应用非常有好处，好处大大滴有。</p>
<p>持久化可以使用<code>cache()</code>或者<code>persist()</code>。默认情况下的缓存级别为<code>MEMORY_ONLY</code>，spark会将对象直接缓存在JVM的堆空间中，而不经过序列化处理。我们可以给persist()传递持久化级别参数以指定的方式持久化RDD。<code>MEMORY_AND_DISK</code>持久化级别尽量将RDD缓存在内存中，如果内存缓存不下了，就将剩余分区缓存在磁盘中。<code>MEMORY_ONLY_SER</code>将RDD进行序列化处理(每个分区序列化为一个字节数组)然后缓存在内存中。还有<code>MEMORY_AND_DISK_SER</code>等等很多选项。选择持久化级别的原则是：尽量选择缓存在内存中，如果内存不够，则首选序列化内存方式，除非RDD分区重算开销比缓存到磁盘来的更大(很多时候，重算RDD分区会比从磁盘中读取要快)或者序列化之后内存还是不够用，否则不推荐缓存到磁盘上。</p>
<p>如果要缓存的数据太多，内存中放不下，spark会自动利用最近最少使用(LRU)策略把最老的分区从内存中移除。对于仅放在内存中的缓存级别，下次要用到已被移除的分区时，这些分区就需要重新计算。对于使用内存与磁盘的缓存级别，被移除的分区都会被写入磁盘。</p>
<p>另外，RDD还有一个<code>unpersist()</code>方法，用于手动把持久化的RDD从缓存中移除。</p>
<p>环境变量<code>SPARK_LOCAL_DIRS</code>用来设置RDD持久化到磁盘的目录，它同时也是shuffle的缓存目录。</p>
<h2 id="各种RDD与RDD操作"><a href="#各种RDD与RDD操作" class="headerlink" title="各种RDD与RDD操作"></a>各种RDD与RDD操作</h2><h3 id="基本RDD"><a href="#基本RDD" class="headerlink" title="基本RDD"></a>基本RDD</h3><p>抽象类<code>RDD</code>包含了各种数据类型的RDD都适用的通用操作。下面对基本类型RDD的操作进行分门别类地介绍。</p>
<p><strong>针对各个元素的转化操作：</strong></p>
<ul>
<li><strong>map</strong>: 对各个元素进行映射操作。</li>
<li><strong>flatMap</strong>: 对各个元素进行映射操作，并将最后结果展平。</li>
</ul>
<ul>
<li><strong>filter</strong>: 过滤不满足条件的元素。filter操作可能会引起数据倾斜，甚至可能导致空分区，新形成的RDD将会包含这些可能生成的空分区。所有这些都可能会导致问题，要想解决它们，最好在filter之后重新分区。</li>
</ul>
<p><strong>伪集合操作：</strong></p>
<p>尽管RDD不是严格意义上的集合，但它支持许多数学上的集合操作。注意：这些操作都要求操作的RDD是相同的数据类型的。</p>
<ul>
<li><strong>distinct</strong>: 对RDD中的元素进行去重处理。需要注意的是，distinct操作开销很大，因为它需要shuffle所有数据，以确保每一个元素都只有一份。</li>
<li><strong>union</strong>: 返回一个包含两个或多个RDD中所有元素的RDD。spark的union并不会去重，这点与数学上的不同。</li>
<li><strong>intersection</strong>: 返回两个RDD中都有的元素。intersection会在运行时除去所有重复的元素，因此它也需要shuffle，性能要差一些。</li>
<li><strong>subtract</strong>: 返回一个由只存在于第一个RDD中而不存在于第二个RDD中的所有元素组成的RDD。它也需要shuffle。</li>
<li><strong>cartesian</strong>: 计算两个RDD的笛卡尔积。需要注意的是，求大规模RDD的笛卡尔积开销巨大。</li>
<li><strong>sample</strong>: 对RDD进行采样，返回一个采样RDD。</li>
</ul>
<p><strong>基于分区的转化操作：</strong></p>
<ul>
<li><strong>glom</strong>: 将每个分区中的所有元素都形成一个数组。如果在处理当前元素时需要使用前后的元素，该操作将会非常有用，不过有时我们可能还需要将分区边界的数据收集起来并广播到各节点以备使用。</li>
</ul>
<ul>
<li><strong>mapPartitions</strong>: 基于分区的map，spark会为操作分区的函数该分区的元素的迭代器。</li>
<li><strong>mapPartitionsWithIndex</strong>: 与mapPartitions不同之处在于带有分区的序号。</li>
</ul>
<p><strong>管道(pipe)操作：</strong></p>
<p>spark在RDD上提供了<code>pipe()</code>方法。通过pipe()，你可以使用任意语言将RDD中的各元素从标准输入流中以字符串形式读出，并将这些元素执行任何你需要的操作，然后把结果以字符串形式写入标准输出，这个过程就是RDD的转化操作过程。</p>
<p>使用pipe()的方法很简单，假如我们有一个用其他语言写成的从标准输入接收数据并将处理结果写入标准输出的可执行脚本，我们只需要将该脚本分发到各个节点相同路径下，并将其路径作为pipe()的参数传入即可。</p>
<p><strong>行动操作：</strong></p>
<ul>
<li><strong>foreach</strong>: 对每个元素进行操作，并不会返回结果。</li>
<li><strong>foreachPartition</strong>: 基于分区的foreach操作，操作分区元素的迭代器，并不会返回结果。</li>
<li><strong>reduce</strong>: 对RDD中所有元素进行规约，最终得到一个规约结果。reduce接收的规约函数要求其返回值类型与RDD中元素类型相同。</li>
<li><strong>fold</strong>: 与reduce类似，不同的是，它接受一个“初始值”来作为每个分区第一次调用时的结果。fold同样要求规约函数返回值类型与RDD元素类型相同。</li>
<li><strong>aggregate</strong>: 与reduce和fold类似，但它把我们从返回值类型必须与所操作的RDD元素类型相同的限制中解放出来。</li>
<li><strong>count</strong>: 返回RDD元素个数。</li>
<li><strong>collect</strong>: 收集RDD的元素到driver节点，如果数据有序，那么collect得到的数据也会是有序的。大数据量最好不要使用RDD的collect，因为它会在本机上生成一个新的Array，以存储来自各个节点的所有数据，此时更好的办法是将数据存储在HDFS等分布式持久化层上。</li>
<li><strong>take</strong>: 返回指定数量的元素到driver节点。它会尝试只访问尽量少的分区，因此该操作会得到一个不均衡的集合。需要注意的是，该操作返回元素的顺序与你预期的可能不一样。</li>
<li><strong>top</strong>: 如果为元素定义了顺序，就可以使用top返回前几个元素。</li>
<li><strong>takeSample</strong>: 返回采样数据。</li>
</ul>
<h3 id="键值对RDD"><a href="#键值对RDD" class="headerlink" title="键值对RDD"></a>键值对RDD</h3><p><code>PairRDDFunctions</code>封装了用于操作键值对RDD的一些功能函数。一些文件读取操作(<code>sc.sequenceFile()</code>等)会直接返回RDD[(K, V)]类型。在RDD上使用map操作也可以将一个RDD转换为RDD[(K, V)]类型。在用Scala书写的Spark程序中，RDD[(K, V)]类型到PairRDDFunctions类型的转换一般由隐式转换函数完成。</p>
<p>基本类型RDD的操作同样适用于键值对RDD。下面对键值对类型RDD特有的操作进行分门别类地介绍。</p>
<p><strong>针对各个元素的转化操作：</strong></p>
<ul>
<li><strong>mapValues</strong>: 对各个键值对的值进行映射。该操作会保留RDD的分区信息。</li>
<li><strong>flatMapValues</strong>: 对各个键值对的值进行映射，并将最后结果展平。该操作会保留RDD的分区信息。</li>
</ul>
<p><strong>聚合操作：</strong></p>
<ul>
<li><strong>reduceByKey</strong>: 与reduce相当类似，它们都接收一个函数，并使用该函数对值进行合并。不同的是，reduceByKey是transformation操作，reduceByKey只是对键相同的值进行规约，并最终形成RDD[(K, V)]，而不像reduce那样返回单独一个“值”。</li>
<li><strong>foldByKey</strong>: 与fold类似，就像reduceByKey之于reduce那样。熟悉MapReduce中的合并器(combiner)概念的你可能已经注意到，reduceByKey和foldByKey会在为每个键计算全局的总结果之前先自动在每台机器上进行本地合并。用户不需要指定合并器。更泛化的combineByKey可以让你自定义合并的行为。</li>
<li><strong>combineByKey</strong>: 是最常用的基于键进行聚合的函数，大多数基于键聚合的函数都是用它实现的。与aggregate一样，combineByKey可以让用户返回与输入数据的类型不同的返回值。combineByKey的内部实现分为三步来完成：首先根据是否需要在map端进行combine操作决定是否对RDD先进行一次mapPartitions操作(利用createCombiner、mergeValue、mergeCombiners三个函数)来达到减少shuffle数据量的作用。第二步根据partitioner对MapPartitionsRDD进行shuffle操作。最后在reduce端对shuffle的结果再进行一次combine操作。</li>
</ul>
<p><strong>数据分组：</strong></p>
<ul>
<li><strong>groupBy</strong>: 根据自定义的东东进行分组。groupBy是基本RDD就有的操作。</li>
<li><strong>groupByKey</strong>: 根据键对数据进行分组。虽然<code>groupByKey</code>+<code>reduce</code>也可以实现<code>reduceByKey</code>一样的效果，但是请你记住：groupByKey是低效的，而reduceByKey会在本地先进行聚合，然后再通过网络传输求得最终结果。</li>
</ul>
<blockquote>
<p>在执行聚合或分组操作时，可以指定分区数以对并行度进行调优。</p>
</blockquote>
<p><strong>连接：</strong></p>
<ul>
<li><strong>cogroup</strong>: 可以对多个RDD进行连接、分组、甚至求键的交集。其他的连接操作都是基于cogroup实现的。</li>
<li><strong>join</strong>: 对数据进行内连接，也即当两个键值对RDD中都存在对应键时才输出。当一个输入对应的某个键有多个值时，生成的键值对RDD会包含来自两个输入RDD的每一组相对应的记录，也即笛卡尔积。</li>
<li><strong>leftOuterJoin</strong>: 即左外连接，源RDD的每一个键都有对应的记录，第二个RDD的值可能缺失，因此用Option表示。</li>
<li><strong>rightOuterJoin</strong>: 即右外连接，与左外连接相反。</li>
<li><strong>fullOuterJoin</strong>: 即全外连接，它是是左右外连接的并集。</li>
</ul>
<blockquote>
<p>如果一个RDD需要在多次连接操作中使用，对该RDD分区并持久化分区后的RDD是有益的，它可以避免不必要的shuffle。</p>
</blockquote>
<p><strong>数据排序：</strong></p>
<p>在基本类型RDD中，<code>sortBy()</code>可以用来排序，<code>max()</code>和<code>min()</code>则可以用来方便地获取最大值和最小值。另外，在OrderedRDDFunctions中，存在一个<code>sortByKey()</code>可以方便地对键值对RDD进行排序，通过spark提供的隐式转换函数可以将RDD自动地转换为OrderedRDDFunctions，并随意地使用它的排序功能。</p>
<p><strong>行动操作：</strong></p>
<p>键值对RDD提供了一些额外的行动操作供我们随意使用。如下：</p>
<ul>
<li><strong>countByKey</strong>: 对每个键对应的元素分别计数。</li>
<li><strong>collectAsMap</strong>: 将结果以Map的形式返回，以便查询。</li>
<li><strong>lookup</strong>: 返回给定键对应的所有值。</li>
</ul>
<h3 id="数值RDD"><a href="#数值RDD" class="headerlink" title="数值RDD"></a>数值RDD</h3><p><code>DoubleRDDFunctions</code>为包含数值数据的RDD提供了一些描述性的统计操作，RDD可以通过隐式转换方便地使用这些方便的功能。</p>
<p>这些数值操作是通过流式算法实现的，允许以每次一个元素的方式构建出模型。这些统计数据都会在调用<code>stats()</code>时通过一次遍历数据计算出来，并以<code>StatCounter</code>对象返回。如果你只想计算这些统计数据中的一个，也可以直接对RDD调用对应的方法。更多信息参见Spark API。</p>
<h2 id="RDD依赖、窄宽依赖"><a href="#RDD依赖、窄宽依赖" class="headerlink" title="RDD依赖、窄宽依赖"></a>RDD依赖、窄宽依赖</h2><h3 id="RDD依赖与DAG"><a href="#RDD依赖与DAG" class="headerlink" title="RDD依赖与DAG"></a>RDD依赖与DAG</h3><p>一系列转化操作形成RDD的有向无环图(DAG)，行动操作触发作业的提交与执行。每个RDD维护了其对<strong>直接父RDD</strong>(一个或多个)的依赖，其中包含了父RDD的引用和依赖类型信息，通过<code>dependencies()</code>我们可以获取对应RDD的依赖，其返回一个依赖列表。</p>
<p>通过RDD的父RDD引用就可以从DAG上向前回溯找到其所有的祖先RDD。spark提供了<code>toDebugString</code>方法来查看RDD的谱系。对于如下一段简单的代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val input &#x3D; sc.parallelize(1 to 10)</span><br><span class="line">val repartitioned &#x3D; input.repartition(2)</span><br><span class="line">val sum &#x3D; repartitioned.sum</span><br></pre></td></tr></table></figure>

<p>我们就可以通过在RDD上调用toDebugString来查看其依赖以及转化关系，结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; input.toDebugString</span><br><span class="line">res0: String &#x3D; (4) ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:21 []</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; repartitioned.toDebugString</span><br><span class="line">res1: String &#x3D;</span><br><span class="line">(2) MapPartitionsRDD[4] at repartition at &lt;console&gt;:23 []</span><br><span class="line"> |  CoalescedRDD[3] at repartition at &lt;console&gt;:23 []</span><br><span class="line"> |  ShuffledRDD[2] at repartition at &lt;console&gt;:23 []</span><br><span class="line"> +-(4) MapPartitionsRDD[1] at repartition at &lt;console&gt;:23 []</span><br><span class="line">    |  ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:21 []</span><br></pre></td></tr></table></figure>

<p>上述<code>repartitioned</code>的依赖链存在两个缩进等级。同一缩进等级的转化操作构成一个Stage(阶段)，它们不需要混洗(shuffle)数据，并可以流水线执行(pipelining)。</p>
<h3 id="窄依赖和宽依赖"><a href="#窄依赖和宽依赖" class="headerlink" title="窄依赖和宽依赖"></a>窄依赖和宽依赖</h3><p>spark中RDD之间的依赖分为<strong>窄(Narrow)依赖</strong>和<strong>宽(Wide)依赖</strong>两种。我们先放出一张示意图：</p>
<p><a href="http://smallx.me/2016/06/07/spark%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/%E7%AA%84%E4%BE%9D%E8%B5%96%E5%92%8C%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" target="_blank" rel="noopener"><img src="http://smallx.me/2016/06/07/spark%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/%E7%AA%84%E4%BE%9D%E8%B5%96%E5%92%8C%E5%AE%BD%E4%BE%9D%E8%B5%96.jpg" alt="窄依赖和宽依赖"></a></p>
<p><strong>窄依赖</strong>指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区。图中，map/filter和union属于第一类，对输入进行协同划分(co-partitioned)的join属于第二类。</p>
<p><strong>宽依赖</strong>指子RDD的分区依赖于父RDD的多个或所有分区，这是因为<strong>shuffle</strong>类操作，如图中的groupByKey和未经协同划分的join。</p>
<p>窄依赖对优化很有利。逻辑上，每个RDD的算子都是一个fork/join(此join非上文的join算子，而是指同步多个并行任务的barrier(路障))： 把计算fork到每个分区，算完后join，然后fork/join下一个RDD的算子。如果直接翻译到物理实现，是很不经济的：一是每一个RDD(即使 是中间结果)都需要物化到内存或存储中，费时费空间；二是join作为全局的barrier，是很昂贵的，会被最慢的那个节点拖死。如果子RDD的分区到父RDD的分区是窄依赖，就可以实施经典的fusion优化，把两个fork/join合为一个；如果连续的变换算子序列都是窄依赖，就可以把很多个fork/join并为一个，不但减少了大量的全局barrier，而且无需物化很多中间结果RDD，这将极大地提升性能。Spark把这个叫做流水线(pipeline)优化。关于流水线优化，从MapPartitionsRDD中compute()的实现就可以看出端倪，该compute方法只是对迭代器进行复合，复合就是嵌套，因此数据处理过程就是对每条记录进行同样的嵌套处理直接得出所需结果，而没有中间计算结果，同时也要注意：依赖过长将导致嵌套过深，从而可能导致栈溢出。</p>
<p>转换算子序列一碰上shuffle类操作，宽依赖就发生了，流水线优化终止。在具体实现 中，DAGScheduler从当前算子往前回溯依赖图，一碰到宽依赖，就生成一个stage来容纳已遍历的算子序列。在这个stage里，可以安全地实施流水线优化。然后，又从那个宽依赖开始继续回溯，生成下一个stage。</p>
<p>另外，宽窄依赖的划分对spark的容错也具有重要作用，参见本文容错机制部分。</p>
<h3 id="DAG到任务的划分"><a href="#DAG到任务的划分" class="headerlink" title="DAG到任务的划分"></a>DAG到任务的划分</h3><p>用户代码定义RDD的有向无环图，行动操作把DAG转译为执行计划，进一步生成任务在集群中调度执行。</p>
<p>具体地说，RDD的一系列转化操作形成RDD的DAG，在RDD上调用行动操作将触发一个Job(作业)的运行，Job根据DAG中RDD之间的依赖关系(宽依赖/窄依赖，也即是否发生shuffle)的不同将DAG划分为多个Stage(阶段)，一个Stage对应DAG中的一个或多个RDD，一个Stage对应多个RDD是因为发生了流水线执行(pipelining)，一旦Stage划分出来，Task(任务)就会被创建出来并发给内部的调度器，进而分发到各个executor执行，一个Stage会启动很多Task，每个Task都是在不同的数据分区上做同样的事情(即执行同样的代码段)，Stage是按照依赖顺序处理的，而Task则是独立地启动来计算出RDD的一部分，一旦Job的最后一个Stage结束，一个行动操作也就执行完毕了。</p>
<p>Stage分为两种：<strong>ShuffleMapStage</strong>和<strong>ResultStage</strong>。<strong>ShuffleMapStage</strong>是非最终stage，后面还有其他的stage，所以它的输出一定是需要shuffle并作为后续stage的输入。ShuffleMapStage的最后Task就是<strong>ShuffleMapTask</strong>。<strong>ResultStage</strong>是一个Job的最后一个Stage，直接生成结果或存储。ResultStage的最后Task就是<strong>ResultTask</strong>。一个Job含有一个或多个Stage，最后一个为ResultTask，其他都为ShuffleMapStage。</p>
<h2 id="RDD不能嵌套"><a href="#RDD不能嵌套" class="headerlink" title="RDD不能嵌套"></a>RDD不能嵌套</h2><p>RDD嵌套是不被支持的，也即不能在一个RDD操作的内部再使用RDD。如果在一个RDD的操作中，需要访问另一个RDD的内容，你可以尝试join操作，或者将数据量较小的那个RDD广播(broadcast)出去。</p>
<p>你同时也应该注意到：join操作可能是低效的，将其中一个较小的RDD广播出去然后再join可以避免不必要的shuffle，俗称“小表广播”。</p>
<h2 id="使用其他分区数据"><a href="#使用其他分区数据" class="headerlink" title="使用其他分区数据"></a>使用其他分区数据</h2><p>由于RDD不能嵌套，这使得“在计算一个分区时，访问另一个分区的数据”成为一件困难的事情。那么有什么好的解决办法吗？请继续看。</p>
<p>spark依赖于RDD这种抽象模型进行粗粒度的并行计算，一般情况下每个节点的每次计算都是针对单一记录，当然也可以使用 RDD.mapPartition 来对分区进行处理，但都限制在一个分区内(当然更是一个节点内)。</p>
<p>spark的worker节点相互之间不能直接进行通信，如果在一个节点的计算中需要使用到另一个分区的数据，那么还是有一定的困难的。</p>
<p>你可以将整个RDD的数据全部广播(如果数据集很大，这可不是好办法)，或者广播一些其他辅助信息；也可以从所有节点均可以访问到的文件(hdfs文件)或者数据库(关系型数据库或者hbase)中读取；更进一步或许你应该修改你的并行方案，使之满足“可针对拆分得到的小数据块进行并行的独立的计算，然后归并得到大数据块的计算结果”的MapReduce准则，在“划分大的数据，并行独立计算，归并得到结果”的过程中可能存在数据冗余之类的，但它可以解决一次性没法计算的大数据，并最终提高计算效率，hadoop和spark都依赖于MapReduce准则。</p>
<h2 id="对RDD进行分区"><a href="#对RDD进行分区" class="headerlink" title="对RDD进行分区"></a>对RDD进行分区</h2><h3 id="何时进行分区？"><a href="#何时进行分区？" class="headerlink" title="何时进行分区？"></a>何时进行分区？</h3><p>spark程序可以通过控制RDD分区方式来减少通信开销。分区并不是对所有应用都是有好处的，如果给定RDD只需要被扫描一次，我们完全没有必要对其预先进行分区处理。只有当数据集多次在诸如连接这种基于键的操作中使用时，分区才会有帮助，同时记得将分区得到的新RDD持久化哦。</p>
<p>更多的分区意味着更多的并行任务(Task)数。对于shuffle过程，如果分区中数据量过大可能会引起OOM，这时可以将RDD划分为更多的分区，这同时也将导致更多的并行任务。spark通过线程池的方式复用executor JVM进程，每个Task作为一个线程存在于线程池中，这样就减少了线程的启动开销，可以高效地支持单个executor内的多任务执行，这样你就可以放心地将任务数量设置成比该应用分配到的CPU cores还要多的数量了。</p>
<h3 id="如何分区与分区信息"><a href="#如何分区与分区信息" class="headerlink" title="如何分区与分区信息"></a>如何分区与分区信息</h3><p>在创建RDD的时候，可以指定分区的个数，如果没有指定，则分区个数是系统默认值，即该程序所分配到的CPU核心数。在Java/Scala中，你可以使用<code>rdd.getNumPartitions</code>(1.6.0+)或<code>rdd.partitions.size()</code>来获取分区个数。</p>
<p>对基本类型RDD进行重新分区，可以通过<code>repartition()</code>函数，只需要指定重分区的分区数即可。repartition操作会引起shuffle，因此spark提供了一个优化版的repartition，叫做<code>coalesce()</code>，它允许你指定是否需要shuffle。在使用coalesce时，需要注意以下几个问题：</p>
<ul>
<li>coalesce默认shuffle为false，这将形成窄依赖，例如我们将1000个分区重新分到100个中时，并不会引起shuffle，而是原来的10个分区合并形成1个分区。</li>
</ul>
<ul>
<li>但是对于从很多个(比如1000个)分区重新分到很少的(比如1个)分区这种极端情况，数据将会分布到很少节点(对于从1000到1的重新分区，则是1个节点)上运行，完全无法开掘集群的并行能力，为了规避这个问题，可以设置shuffle为true。由于shuffle可以分隔stage，这就保证了上一阶段stage中的任务仍是很多个分区在并行计算，不这样设置的话，则两个上下游的任务将合并成一个stage进行计算，这个stage便会在很少的分区中进行计算。</li>
</ul>
<ul>
<li>如果当前每个分区的数据量过大，需要将分区数量增加，以利于充分利用并行，这时我们可以设置shuffle为true。对于数据分布不均而需要重分区的情况也是如此。spark默认使用hash分区器将数据重新分区。</li>
</ul>
<p>对RDD进行预置的hash分区，需将RDD转换为RDD[(key,value)]类型，然后就可以通过隐式转换为PairRDDFunctions，进而可以通过如下形式将RDD哈希分区，<code>HashPartitioner</code>会根据RDD中每个(key,value)中的key得出该记录对应的新的分区号：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PairRDDFunctions.partitionBy(new HashPartitioner(n))</span><br></pre></td></tr></table></figure>

<p>另外，spark还提供了一个范围分区器，叫做<code>RangePartitioner</code>。范围分区器争取将所有的分区尽可能分配得到相同多的数据，并且所有分区内数据的上界是有序的。</p>
<p>一个RDD可能存在分区器也可能没有，我们可以通过RDD的<code>partitioner</code>属性来获取其分区器，它返回一个Option对象。</p>
<h3 id="如何进行自定义分区"><a href="#如何进行自定义分区" class="headerlink" title="如何进行自定义分区"></a>如何进行自定义分区</h3><p>spark允许你通过提供一个自定义的Partitioner对象来控制RDD的分区方式，这可以让你利用领域知识进一步减少通信开销。</p>
<p>要实现自定义的分区器，你需要继承<code>Partitioner</code>类，并实现下面三个方法即可：</p>
<ul>
<li><strong>numPartitions</strong>: 返回创建出来的分区数。</li>
<li><strong>getPartition</strong>: 返回给定键的分区编号(0到numPartitions-1)。</li>
<li><strong>equals</strong>: Java判断相等性的标准方法。这个方法的实现非常重要，spark需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样spark才可以判断两个RDD的分区方式是否相同。</li>
</ul>
<h3 id="影响分区方式的操作"><a href="#影响分区方式的操作" class="headerlink" title="影响分区方式的操作"></a>影响分区方式的操作</h3><p>spark内部知道各操作会如何影响分区方式，并将会对数据进行分区的操作的结果RDD自动设置为对应的分区器。</p>
<p>不过转化操作的结果并不一定会按照已知的分区方式分区，这时输出的RDD可能就会丢失分区信息。例如，由于<code>map()</code>或<code>flatMap()</code>函数理论上可以改变元素的键，因此当你对一个哈希分区的键值对RDD调用map/flatMap时，结果RDD就不会再有分区方式信息。不过，spark提供了另外两个操作<code>mapValues()</code>和<code>flatMapValues()</code>作为替代方法，它们可以保证每个二元组的键保持不变。</p>
<p>这里列出了所有会为生成的结果RDD设好分区方式的操作：<code>cogroup()</code>、 <code>join()</code>、 <code>leftOuterJoin()</code>、 <code>rightOuterJoin()</code>、 <code>fullOuterJoin()</code>、<code>groupWith()</code>、 <code>groupByKey()</code>、 <code>reduceByKey()</code>、 <code>combineByKey()</code>、 <code>partitionBy()</code>、 <code>sortBy()</code>、 <code>sortByKey()</code>、 <code>mapValues()</code>(如果父RDD有分区方式的话)、 <code>flatMapValues()</code>(如果父RDD有分区方式的话)、 <code>filter()</code>(如果父RDD有分区方式的话) 等。其他所有操作生成的结果都不会存在特定的分区方式。</p>
<p>最后，对于二元操作，输出数据的分区方式取决于父RDD的分区方式。默认情况下，结果会采用哈希分区，分区的数量和操作的并行度一样。不过，如果其中一个父RDD已经设置过分区方式，那么结果就会采用那种分区方式；如果两个父RDD都设置过分区方式，结果RDD会采用第一个父RDD的分区方式。</p>
<h3 id="从分区中获益的操作"><a href="#从分区中获益的操作" class="headerlink" title="从分区中获益的操作"></a>从分区中获益的操作</h3><p>spark的许多操作都引入了将数据根据键跨节点进行shuffle的过程。所有这些操作都会从数据分区中获益。这些操作主要有：<code>cogroup()</code>、 <code>join()</code>、 <code>leftOuterJoin()</code>、 <code>rightOuterJoin()</code>、 <code>fullOuterJoin()</code>、 <code>groupWith()</code>、 <code>groupByKey()</code>、 <code>reduceByKey()</code>、 <code>combineByKey()</code>、 <code>lookup()</code> 等。</p>
<h2 id="RDD分区优先位置"><a href="#RDD分区优先位置" class="headerlink" title="RDD分区优先位置"></a>RDD分区优先位置</h2><p>RDD分区优先位置与spark的调度有关，在spark进行任务调度的时候，会尽可能将任务分配到数据块所存储的节点。我们可以通过RDD的<code>preferredLocations()</code>来获取指定分区的优先位置，返回值是该分区的优先位置列表。</p>
<h1 id="数据加载与保存"><a href="#数据加载与保存" class="headerlink" title="数据加载与保存"></a>数据加载与保存</h1><h2 id="从程序中的集合生成"><a href="#从程序中的集合生成" class="headerlink" title="从程序中的集合生成"></a>从程序中的集合生成</h2><p><code>sc.parallelize()</code>可用于从程序中的集合产生RDD。<code>sc.makeRDD()</code>也是在程序中生成RDD，不过其还允许指定每一个RDD分区的优先位置。</p>
<p>以上这些方式一般用于原型开发和测试，因为它们需要把你的整个数据集先放在一台机器(driver节点)的内存中，从而限制了只能用较小的数据量。</p>
<h2 id="从文本文件加载数据"><a href="#从文本文件加载数据" class="headerlink" title="从文本文件加载数据"></a>从文本文件加载数据</h2><p><code>sc.textFile()</code>默认从hdfs中读取文件，在路径前面加上<code>hdfs://</code>可显式表示从hdfs中读取文件，在路径前面加上<code>file://</code>表示从本地文件系统读。给sc.textFile()传递的文件路径可以是如下几种情形：</p>
<ul>
<li>一个文件路径，这时候只装载指定的文件。</li>
<li>一个目录路径，这时候只装载指定目录下面的所有文件(不包括子目录下面的文件)。</li>
<li>通过通配符的形式加载多个文件或者加载多个目录下面的所有文件。</li>
</ul>
<p>如果想一次性读取一个目录下面的多个文件并想知道数据来自哪个文件，可以使用<code>sc.wholeTextFiles</code>。它会返回一个键值对RDD，其中键是输入文件的文件名。由于该函数会将一个文件作为RDD的一个元素进行读取，因此所读取的文件不能太大，以便其可以在一个机器上装得下。</p>
<p>同其他transform算子一样，文本读取操作也是惰性的并由action算子触发，如果发生重新计算，那么读取数据的操作也可能会被再次执行。另外，在spark中超出内存大小的文件同样是可以被处理的，因为spark并不是将数据一次性全部装入内存，而是边装入边计算。</p>
<h2 id="从数据库加载数据"><a href="#从数据库加载数据" class="headerlink" title="从数据库加载数据"></a>从数据库加载数据</h2><p>spark中可以使用<code>JdbcRDD</code>从数据库中加载数据。spark会将数据从数据库中拷贝到集群各个节点，因此使用JdbcRDD会有初始的拷贝数据的开销。也可以考虑使用sqoop将数据从数据库中迁移到hdfs中，然后从hdfs中读取数据。</p>
<h2 id="将结果写入文本文件"><a href="#将结果写入文本文件" class="headerlink" title="将结果写入文本文件"></a>将结果写入文本文件</h2><p><code>rdd.saveAsTextFile()</code>用于将RDD写入文本文件。spark会将传入该函数的路径参数作为目录对待，默认情况下会在对应目录输出多个文件，这取决于并行度。如果要将结果写入hdfs的<strong>一个</strong>文件中，可以这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.coalesce(1).saveAsTextFile(&quot;filename&quot;)</span><br></pre></td></tr></table></figure>

<p>而不要使用repartition，因为repartition会引起shuffle，而coalesce在默认情况下会避免shuffle。</p>
<h2 id="关于文件系统"><a href="#关于文件系统" class="headerlink" title="关于文件系统"></a>关于文件系统</h2><p>spark支持读写很多文件系统，包括本地文件系统、HDFS、Amazon S3等等很多。</p>
<p>spark在本地文件系统中读取文件时，它要求文件在集群中所有节点的相同路径下都可以找到。我们可以通过<code>sc.addFile()</code>来将文件弄到所有节点同路径下面，并在各计算节点中通过<code>SparkFiles.get()</code>来获取对应文件在该节点上的绝对路径。</p>
<blockquote>
<p><code>sc.addFile()</code>的输入文件路径不仅可以是本地文件系统的，还可以是HDFS等spark所支持的所有文件系统，甚至还可以是来自网络的，如HTTP、HTTPS、FTP。</p>
</blockquote>
<h1 id="关于并行"><a href="#关于并行" class="headerlink" title="关于并行"></a>关于并行</h1><h2 id="慎用可变数据"><a href="#慎用可变数据" class="headerlink" title="慎用可变数据"></a>慎用可变数据</h2><p>当可变数据用于并发/并行/分布式程序时，都有可能出现问题，因此对于会并发执行的代码段不要使用可变数据。</p>
<p>尤其要注意不要在scala的object中使用var变量！其实scala的object单例对象只是对java中静态的一种封装而已，在class文件层面，object单例对象就是用java中静态(static)来实现的，而java静态成员变量不会被序列化！在编写并行计算程序时，不要在scala的object中使用var变量，如果确实需要使用var变量，请写在class中。</p>
<p>另外，在分布式执行的spark代码段中使用可变的闭包变量也可能会出现不同步问题，因此请谨慎使用。</p>
<h2 id="闭包-vs-广播变量"><a href="#闭包-vs-广播变量" class="headerlink" title="闭包 vs 广播变量"></a>闭包 vs 广播变量</h2><p>有两种方式将你的数据从driver节点发送到worker节点：通过<strong>闭包</strong>和通过<strong>广播变量</strong>。闭包是随着task的组装和分发自动进行的，而广播变量则是需要程序猿手动操作的，具体地可以通过如下方式操作广播变量(假设<code>sc</code>为<code>SparkContext</code>类型的对象，<code>bc</code>为<code>Broadcast</code>类型的对象)：</p>
<ul>
<li>可通过<code>sc.broadcast(xxx)</code>创建广播变量。</li>
<li>可在各计算节点中(闭包代码中)通过<code>bc.value</code>来引用广播的数据。</li>
<li><code>bc.unpersist()</code>可将各executor中缓存的广播变量删除，后续再使用时数据将被重新发送。</li>
<li><code>bc.destroy()</code>可将广播变量的数据和元数据一同销毁，销毁之后就不能再使用了。</li>
</ul>
<p>任务闭包包含了任务所需要的代码和数据，如果一个executor数量小于RDD partition的数量，那么每个executor就会得到多个同样的任务闭包，这通常是低效的。而广播变量则只会将数据发送到每个executor一次，并且可以在多个计算操作中共享该广播变量，而且广播变量使用了类似于p2p形式的非常高效的广播算法，大大提高了效率。另外，广播变量由spark存储管理模块进行管理，并以MEMORY_AND_DISK级别进行持久化存储。</p>
<p><strong>什么时候用闭包自动分发数据？</strong>情况有几种：</p>
<ul>
<li>数据比较小的时候。</li>
<li>数据已在driver程序中可用。典型用例是常量或者配置参数。</li>
</ul>
<p><strong>什么时候用广播变量分发数据？</strong>情况有几种：</p>
<ul>
<li>数据比较大的时候(实际上，spark支持非常大的广播变量，甚至广播变量中的元素数超过java/scala中Array的最大长度限制(2G，约21.5亿)都是可以的)。</li>
<li>数据是某种分布式计算结果。典型用例是训练模型等中间计算结果。</li>
</ul>
<p>当数据或者变量很小的时候，我们可以在Spark程序中直接使用它们，而无需使用广播变量。</p>
<p>对于大的广播变量，序列化优化可以大大提高网络传输效率，参见本文序列化优化部分。</p>
<h2 id="巧用累加器"><a href="#巧用累加器" class="headerlink" title="巧用累加器"></a>巧用累加器</h2><p>累加器提供了将工作节点中的值聚合到驱动器程序中的简单语法。累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。可以通过<code>sc.accumulator(xxx)</code>来创建一个累加器，并在各计算节点中(闭包代码中)直接写该累加器。</p>
<p>累加器只能在驱动程序中被读取，对于计算节点(闭包代码)是只写的，这大大精简了累加器的设计。</p>
<p>使用累加器时，我们要注意的是：对于在RDD转化操作中使用的累加器，如果发生了重新计算(这可能在很多种情况下发生)，那么累加器就会被重复更新，这会导致问题。而在行动操作(如foreach)中使用累加器却不会出现这种情况。因此，在转化操作中，累加器通常只用于调试目的。尽管将来版本的spark可能会改善这一问题，但在spark 1.2.0中确实存在这个问题。</p>
<h2 id="关于shuffle"><a href="#关于shuffle" class="headerlink" title="关于shuffle"></a>关于shuffle</h2><p>在经典的MapReduce中，shuffle(混洗)是连接map阶段和reduce阶段的桥梁(注意这里的术语跟spark的map和reduce操作没有直接关系)，它是将各个map的输出结果重新组合作为下阶段各个reduce的输入这样的一个过程，由于这一过程涉及各个节点相互之间的数据传输，故此而名“混洗”。下面这幅图清晰地描述了MapReduce算法的整个流程，其中shuffle阶段是介于map阶段和reduce阶段之间。<a href="http://smallx.me/2016/06/07/spark%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/mapreduce%E8%BF%87%E7%A8%8B.jpg" target="_blank" rel="noopener"><img src="http://smallx.me/2016/06/07/spark%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/mapreduce%E8%BF%87%E7%A8%8B.jpg" alt="mapreduce过程"></a></p>
<p>Spark的shuffle过程类似于经典的MapReduce，但是有所改进。spark中的shuffle在实现上主要分为<strong>shuffle write</strong>和<strong>shuffle fetch</strong>这两个大的阶段。如下图所示，shuffle过程大致可以描述为：</p>
<p><a href="http://smallx.me/2016/06/07/spark%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/spark-shuffle%E8%BF%87%E7%A8%8B.png" target="_blank" rel="noopener"><img src="http://smallx.me/2016/06/07/spark%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/spark-shuffle%E8%BF%87%E7%A8%8B.png" alt="spark-shuffle过程"></a></p>
<ul>
<li>首先每一个Mapper会根据Reducer的数量创建出相应的bucket，bucket的数量是M×R，其中M是Map的个数，R是Reduce的个数。</li>
<li>其次Mapper产生的结果会根据设置的partition算法填充到每个bucket中去。这里的partition算法是可以自定义的，当然默认的算法是根据key哈希到不同的bucket中去。</li>
<li>当Reducer启动时，它会根据自己task的id和所依赖的Mapper的id从远端或是本地的block manager中取得相应的bucket作为Reducer的输入进行处理。</li>
</ul>
<p>spark的shuffle实现随着spark版本的迭代正在逐步完善和成熟，这中间曾出现过多种优化实现，关于spark shuffle的演进过程和具体实现参见后面的参考链接。</p>
<p>shuffle(具体地是shuffle write阶段)会引起数据缓存到本地磁盘文件，从spark 1.3开始，这些缓存的shuffle文件只有在相应RDD不再被使用时才会被清除，这样在lineage重算的时候shuffle文件就不需要重新创建了，从而加快了重算效率(请注意这里的缓存并保留shuffle数据这一行为与RDD持久化和检查点机制是不同的，缓存并保留shuffle数据只是省去了重算时重建shuffle文件的开销，因此我们才有理由在shuffle(宽依赖)之后对形成的RDD进行持久化)。在standalone模式下，我们可以在<code>spark-env.sh</code>中通过环境变量<code>SPARK_LOCAL_DIRS</code>来设置shuffle数据的本地磁盘缓存目录。为了优化效率，本地shuffle缓存目录的设置都应该使用由单个逗号隔开的目录列表，并且这些目录分布在不同的磁盘上，写操作会被均衡地分配到所有提供的目录中，磁盘越多，可以提供的总吞吐量就越高。另外，<code>SPARK_LOCAL_DIRS</code>也是RDD持久化到磁盘的目录。</p>
<blockquote>
<p>参考链接及进一步阅读：</p>
<ul>
<li><a href="http://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation/" target="_blank" rel="noopener">详细探究Spark的shuffle实现</a></li>
<li><a href="http://spark.apache.org/docs/latest/programming-guide.html#shuffle-operations" target="_blank" rel="noopener">Spark Shuffle operations</a></li>
</ul>
</blockquote>
<h2 id="序列化优化"><a href="#序列化优化" class="headerlink" title="序列化优化"></a>序列化优化</h2><p>在spark中，序列化通常出现在跨节点的数据传输(如广播变量、shuffle等)和数据持久化过程中。序列化和反序列化的速度、序列化之后数据大小等都影响着集群的计算效率。</p>
<p>spark默认使用Java序列化库，它对于除基本类型的数组以外的任何对象都比较低效。为了优化序列化效率，你可以在spark配置文件中通过<code>spark.serializer</code>属性来设置你想使用的序列化库，一般情况下，你可以使用这个序列化库：<code>org.apache.spark.serializer.KryoSerializer</code>。</p>
<p>为了获得最佳性能，你还应该向Kryo注册你想要序列化的类，注册类可以让Kryo避免把每个对象的完整类名写下来，成千上万条记录累计节省的空间相当可观。如果你想强制要求这种注册，可以把<code>spark.kryo.registrationRequired</code>设置为true，这样Kryo会在遇到未注册的类时抛出错误。使用Kryo序列化库并注册所需类的示例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val conf &#x3D; new SparkConf()</span><br><span class="line">conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">conf.set(&quot;spark.kryo.registrationRequired&quot;, &quot;true&quot;)</span><br><span class="line">conf.registerKryoClasses(Array(classOf[MyClass], classOf[MyOtherClass]))</span><br></pre></td></tr></table></figure>

<h1 id="Spark调度"><a href="#Spark调度" class="headerlink" title="Spark调度"></a>Spark调度</h1><h2 id="应用调度"><a href="#应用调度" class="headerlink" title="应用调度"></a>应用调度</h2><p>应用是指用户提交的spark应用程序。spark应用程序之间的调度关系，不一定由spark所管理。</p>
<p>在YARN和Mesos模式下，底层资源的调度策略由YARN和Mesos集群资源管理器所决定。</p>
<p>只有在standalone模式下，spark master按照当前集群资源是否满足等待列表中的spark应用对资源的需求，而决定是否创建一个SparkContext对应的driver，进而完成spark应用的启动过程，这个过程可以粗略地认为是一种粗颗粒度的有条件的<strong>FIFO</strong>(先进先出)调度策略。</p>
<h2 id="作业调度"><a href="#作业调度" class="headerlink" title="作业调度"></a>作业调度</h2><p>作业是指spark应用程序内部的由action算子触发并提交的Job。在给定的spark应用中，不同线程的多个job可以并发执行，并且这个调度是线程安全的，这使得一个spark应用可以处理多个请求。</p>
<p>默认地，spark作业调度是<strong>FIFO</strong>的，在多线程的情况下，某些线程提交的job可能被大大推迟执行。</p>
<p>不过我们可以通过配置<strong>FAIR</strong>(公平)调度器来使spark在作业之间轮询调度，这样所有的作业都能得到一个大致公平的共享的集群资源。这就意味着即使有一个很长的作业在运行，较短的作业在提交之后也能够得到不错的响应。要启用一个FAIR作业调度，需在创建SparkContext之前配置一下<code>spark.scheduler.mode</code>为<code>FAIR</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 假设conf是你的SparkConf变量</span><br><span class="line">conf.set(&quot;spark.scheduler.mode&quot;, &quot;FAIR&quot;)</span><br></pre></td></tr></table></figure>

<p>公平调度还支持在池中将工作分组(这样就形成两级调度池)，而不同的池可以设置不同的调度选项(如权重)。这种方式允许更重要的job配置在高优先级池中优先调度。如果没有设置，新提交的job将进入<strong>默认池</strong>中，我们可以通过在对应线程中给SparkContext设置本地属性<code>spark.scheduler.pool</code>来设置该线程对应的pool：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 假设sc是你的SparkContext变量</span><br><span class="line">sc.setLocalProperty(&quot;spark.scheduler.pool&quot;, &quot;pool1&quot;)</span><br></pre></td></tr></table></figure>

<p>在设置了本地属性之后，所有在这个线程中提交的job都将会使用这个调度池的名字。如果你想清除该线程相关的pool，只需调用如下代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.setLocalProperty(&quot;spark.scheduler.pool&quot;, null)</span><br></pre></td></tr></table></figure>

<p>在默认情况下，每个调度池拥有相同的优先级来共享整个应用所分得的集群资源。同样的，<strong>默认池中的每个job也拥有同样的调度优先级，但是在用户创建的每个池中，job是通过FIFO方式进行调度的</strong>。</p>
<p>关于公平调度池的详细配置，请参见官方文档：<a href="http://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="noopener">Spark Job Scheduling</a>。</p>
<p>如果你想阅读相关实现代码，可以观看<code>Schedulable.scala</code>、<code>SchedulingAlgorithm.scala</code>以及<code>SchedulableBuilder.scala</code>等相关文件。</p>
<blockquote>
<p>参考链接及进一步阅读：</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="noopener">Spark Job Scheduling</a></li>
<li><a href="http://www.aboutyun.com/thread-7600-1-1.html" target="_blank" rel="noopener">Spark 作业调度–job执行方式介绍</a></li>
<li><a href="http://blog.csdn.net/colorant/article/details/24010035" target="_blank" rel="noopener">spark internal - 作业调度</a></li>
</ul>
</blockquote>
<h1 id="容错机制与检查点"><a href="#容错机制与检查点" class="headerlink" title="容错机制与检查点"></a>容错机制与检查点</h1><p>spark容错机制是粗粒度并且是轻量级的，主要依赖于RDD的依赖链(<strong>lineage</strong>)。spark能够通过lineage获取足够的信息来重新计算和恢复丢失的数据分区。这样的基于lineage的容错机制可以理解为粗粒度的重做日志(redo log)。</p>
<p>鉴于spark的基于lineage的容错机制，RDD DAG中宽窄依赖的划分对容错也有很重要的作用。如果一个节点宕机了，而且运算是窄依赖，那只要把丢失的父RDD分区重算即可，跟其他节点没有依赖。而宽依赖需要父RDD的所有分区都存在，重算代价就很高了。可以这样理解为什么窄依赖开销小而宽依赖开销大：在窄依赖中，在子RDD分区丢失、重算父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，并不存在冗余计算；而在宽依赖中，丢失一个子RDD分区将导致其每个父RDD的多个甚至所有分区的重算，而重算的结果并不都是给当前丢失的子RDD分区用的，这样就存在了冗余计算。</p>
<p>不过我们可以通过<strong>检查点</strong>(<strong>checkpoint</strong>)机制解决上述问题，通过在RDD上做检查点可以将物理RDD数据存储到持久层(HDFS、S3等)中。在RDD上做检查点的方法是在调用action算子之前调用<code>checkpoint()</code>，并且RDD最好是缓存在内存中的，否则可能导致重算(参见API注释)。示例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 假设rdd是你的RDD变量</span><br><span class="line">rdd.persist(StorageLevel.MEMORY_AND_DISK_SER)</span><br><span class="line">rdd.checkpoint()</span><br><span class="line">val count &#x3D; rdd.count()</span><br></pre></td></tr></table></figure>

<p>在RDD上做检查点会切断RDD依赖，具体地spark会清空该RDD的父RDD依赖列表。并且由于检查点机制是将RDD存储在外部存储系统上，所以它可以被其他应用重用。</p>
<p>过长的lineage(如在pagerank、spark streaming等中)也将导致过大的重算代价，而且还会占用很多系统资源。因此，<strong>在遇到宽依赖或者lineage足够长时，我们都应该考虑做检查点</strong>。</p>
<h1 id="集群监控与运行日志"><a href="#集群监控与运行日志" class="headerlink" title="集群监控与运行日志"></a>集群监控与运行日志</h1><p>spark在应用执行时记录详细的进度信息和性能指标。这些内容可以在两个地方找到：spark的网页用户界面以及driver进程和executor进程生成的日志文件中。</p>
<h2 id="网页用户界面"><a href="#网页用户界面" class="headerlink" title="网页用户界面"></a>网页用户界面</h2><p>在浏览器中打开 <a href="http://master:8080/" target="_blank" rel="noopener">http://master:8080</a> 页面，你可以看到集群概况，包括：集群节点、可用的和已用的资源、已运行的和正在运行的应用等。</p>
<p><a href="http://master:4040/" target="_blank" rel="noopener">http://master:4040</a> 页面用来监控正在运行的应用(默认端口为4040，如果有多个应用在运行，那么端口顺延，如4041、4042)，包括其执行进度、构成Job的Stage的执行情况、Stage详情、已缓存RDD的信息、各executor的信息、spark配置项以及应用依赖信息等，该页面经常用来发现应用的效率瓶颈并辅助优化，不过该页面只有在有spark应用运行时才可以被访问到。</p>
<p>上述404x端口可用于查看正在运行的应用的执行详情，但是应用运行结束之后该页面就不可以访问了。要想查看已经执行结束的应用的执行详情，则需开启事件日志机制，具体地设置如下两个选项：</p>
<ul>
<li><p>spark.eventLog.enabled: 设置为true时开启事件日志机制。这样已完成的spark作业就可以通过历史服务器查看。</p>
</li>
<li><p>spark.eventLog.dir: 开启事件日志机制时的事件日志文件存储位置。如果要在历史服务器中查看事件日志，需要将该值设置为一个全局可见的文件系统路径，比如HDFS中。最后，请确保目录以 ‘/‘ 结束，否则可能会出现如下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Application history not found ... No event logs found for application ...</span><br><span class="line">Did you specify the correct logging directory?</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>在配置好上述选项之后，我们就可以查看新提交的应用的详细执行信息了。在不同的部署模式中，查看的方式不同。在standalone模式中，可以直接在master节点的UI界面(上述8080端口对应的页面)中直接单击已完成应用以查看详细执行信息。在YARN/Mesos模式中，就要开启历史服务器了，此处略去。</p>
<h2 id="Metrics系统"><a href="#Metrics系统" class="headerlink" title="Metrics系统"></a>Metrics系统</h2><p>spark在其内部拥有一个可配置的度量系统(Metrics)，它能够将spark的内部状态通过HTTP、JMX、CSV等多种不同形式呈现给用户。同时，用户也可以定义自己的数据源(Metrics Source)和数据输出方式(Metrics Sink)，从而获取自己所需的数据。此处略去详情，可参考下面的链接进一步阅读。</p>
<blockquote>
<p>参考链接及进一步阅读：</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/monitoring.html#metrics" target="_blank" rel="noopener">Spark Monitoring and Instrumentation: Metrics</a></li>
</ul>
</blockquote>
<h2 id="查看日志文件"><a href="#查看日志文件" class="headerlink" title="查看日志文件"></a>查看日志文件</h2><p>spark日志文件的具体位置取决于具体的部署模式。在standalone模式中，日志默认存储于各个工作节点的spark目录下的<code>work</code>目录中，此时所有日志还可以直接通过主节点的网页用户界面进行查看。</p>
<p>默认情况下，spark输出的日志包含的信息量比较合适。我们可以自定义日志行为，改变日志等级或存储位置。spark日志系统使用<code>log4j</code>实现，我们只需将conf目录下的log4j.properties.template复制一个并命名为log4j.properties，然后自定义修改即可。</p>
<h1 id="SparkConf与配置"><a href="#SparkConf与配置" class="headerlink" title="SparkConf与配置"></a>SparkConf与配置</h1><p>spark中最主要的配置机制是通过<code>SparkConf</code>类对spark进行配置。当创建出一个SparkContext时，就需要创建出一个SparkConf的实例作为参数。</p>
<p>SparkConf实例包含用户要重载的配置选项的键值对，spark中的每个配置选项都是基于字符串形式的键值对。你可以调用SparkConf的<code>set()</code>或者<code>setXxx()</code>来设置对应选项。</p>
<p>另外，spark-submit脚本可以动态设置配置项。当应用被spark-submit脚本启动时，脚本会把这些配置项设置到运行环境中。当一个新的SparkConf被创建出来时，这些环境变量会被检测出来并且自动配到SparkConf中。这样在使用spark-submit时，用户应用通常只需创建一个“空”的SparkConf，并直接传递给SparkContext的构造方法即可。</p>
<p>spark-submit为常用的spark配置选项提供了专用的标记，还有一个通用标记<code>--conf</code>来接收任意spark配置项的值，形如<code>--conf 属性名=属性值</code>。</p>
<p>spark-submit也支持从文件中读取配置项的值。默认情况下，spark-submit会在spark安装目录中找到<code>conf/spark-defaults.conf</code>文件，读取该文件中以空格隔开的键值对数据。你也可以通过spark-submit的<code>--properties-File</code>选项来自定义该文件的路径。</p>
<blockquote>
<p>spark-defaults.conf的作用范围要搞清楚，编辑driver所在机器上的spark-defaults.conf，该文件会影响到driver所提交运行的application，及专门为该application提供计算资源的executor的启动参数。</p>
</blockquote>
<p>spark有特定的优先级顺序来选择实际配置。优先级最高的是在用户代码中显式调用set()方法设置的选项。其次是通过spark-submit传递的参数。再次是写在配置文件中的值。最后是系统默认值。如果你想知道应用中实际生效的配置，可以在应用的网页用户界面中查看。</p>
<p>下面列出一些常用的配置项，完整的配置项列表可以参见<a href="http://spark.apache.org/docs/latest/configuration.html" target="_blank" rel="noopener">官方配置文档</a>。</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>默认值</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>spark.master</td>
<td>(none)</td>
<td>表示要连接的集群管理器。</td>
</tr>
<tr>
<td>spark.app.name</td>
<td>(none)</td>
<td>应用名，将出现在UI和日志中。</td>
</tr>
<tr>
<td>spark.driver.memory</td>
<td>1g</td>
<td>为driver进程分配的内存。注意：在客户端模式中，不能在SparkConf中直接配置该项，因为driver JVM进程已经启动了。</td>
</tr>
<tr>
<td>spark.executor.memory</td>
<td>1g</td>
<td>为每个executor进程分配的内存。</td>
</tr>
<tr>
<td>spark.executor.cores</td>
<td>all/1</td>
<td>每个executor可用的核心数。针对standalone和YARN模式。更多参见官方文档。</td>
</tr>
<tr>
<td>spark.cores.max</td>
<td>(not set)</td>
<td>设置standalone和Mesos模式下应用程序的核心数上限。</td>
</tr>
<tr>
<td>spark.speculation</td>
<td>false</td>
<td>设置为true时开启任务预测执行机制。当出现比较慢的任务时，这种机制会在另外的节点上也尝试执行该任务的一个副本。打开此选项会帮助减少大规模集群中个别较慢的任务带来的影响。</td>
</tr>
<tr>
<td>spark.driver.extraJavaOptions</td>
<td>(none)</td>
<td>设置driver节点的JVM启动参数。</td>
</tr>
<tr>
<td>spark.executor.extraJavaOptions</td>
<td>(none)</td>
<td>设置executor节点的JVM启动参数。</td>
</tr>
<tr>
<td>spark.serializer</td>
<td>JavaSerializer</td>
<td>指定用来进行序列化的类库，包括通过网络传输数据或缓存数据时的序列化。为了速度，推荐使用KryoSerializer。</td>
</tr>
<tr>
<td>spark.eventLog.enabled</td>
<td>false</td>
<td>设置为true时开启事件日志机制。这样已完成的spark作业就可以通过历史服务器查看。</td>
</tr>
<tr>
<td>spark.eventLog.dir</td>
<td>file:///tmp/spark-events</td>
<td>开启事件日志机制时的事件日志文件存储位置。如果要在历史服务器中查看事件日志，需要将该值设置为一个全局可见的文件系统路径，比如HDFS中。最后，请确保目录以 ‘/‘ 结束，否则可能会出现错误，参见本文集群监控部分。</td>
</tr>
</tbody></table>
<h1 id="一些问题的解决办法"><a href="#一些问题的解决办法" class="headerlink" title="一些问题的解决办法"></a>一些问题的解决办法</h1><h2 id="tmp目录写满"><a href="#tmp目录写满" class="headerlink" title="/tmp目录写满"></a>/tmp目录写满</h2><p>由于Spark在计算的时候会将中间结果存储到/tmp目录，而目前linux又都支持tmpfs，其实说白了就是将/tmp目录挂载到内存当中。那么这里就存在一个问题，中间结果过多导致/tmp目录写满而出现如下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No Space Left on the device</span><br></pre></td></tr></table></figure>

<p>解决办法就是针对tmp目录不启用tmpfs，修改/etc/fstab。</p>
<h2 id="无法创建进程"><a href="#无法创建进程" class="headerlink" title="无法创建进程"></a>无法创建进程</h2><p>有时可能会遇到如下错误，即无法创建进程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.OutOfMemory, unable to create new native thread</span><br></pre></td></tr></table></figure>

<p>导致这种错误的原因比较多。有一种情况并非真的是内存不足引起的，而是由于超出了允许的最大文件句柄数或最大进程数。</p>
<p>排查的步骤就是查看一下允许打开的文件句柄数和最大进程数，如果数值过低，使用ulimit将其调高之后，再试试问题是否已经解决。</p>
<h2 id="不可序列化"><a href="#不可序列化" class="headerlink" title="不可序列化"></a>不可序列化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Task not serializable: java.io.NotSerializableException</span><br></pre></td></tr></table></figure>

<p>作为RDD操作算子参数的匿名函数使用外部变量从而形成闭包。为了效率，spark并不是将所有东东都序列化以分发到各个executor。spark会先对该匿名函数进行ClosureCleaner.clean()处理(将该匿名函数涉及到的$outer中的与闭包无关的变量移除)，然后将该匿名函数对象及闭包涉及到的对象序列化并包装成task分发到各个executor。</p>
<p>看到这里，你或许就发现了一个问题，那就是不管怎样，spark需要序列化的对象必须都可以被序列化！<code>Task not serializable: java.io.NotSerializableException</code>错误就是由于相应的对象不能被序列化造成的！</p>
<p>为了解决这个问题，首先你可以使用 <code>-Dsun.io.serialization.extendedDebugInfo=true</code> java选项来让jvm打印出更多的关于序列化的信息，以便了解哪些对象不可以被序列化。然后就是使这些对象对应的类可序列化，或者将这些对象定义在RDD操作算子的参数(匿名函数)中以取消闭包。</p>
<h2 id="缺少winutils-exe"><a href="#缺少winutils-exe" class="headerlink" title="缺少winutils.exe"></a>缺少winutils.exe</h2><p>在windows上进行spark程序测试时，你可能会碰到如下几个问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1010)</span><br></pre></td></tr></table></figure>

<p>原因就是缺少 hadoop 的 <code>winutils.exe</code> 这个文件。解决方法是：下载一个(注意是32位还是64位)，新建一个文件夹 D:\hadoop\bin\ 并将 winutils.exe 放入其中，并保证winutils.exe双击运行没有报*.dll缺失的错误，然后在程序中设置一下hadoop目录即可，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(&quot;hadoop.home.dir&quot;, &quot;D:\hadoop\&quot;)</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/%E8%BF%90%E7%BB%B4/top/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/%E8%BF%90%E7%BB%B4/top/" class="post-title-link" itemprop="url">top工具</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/linux/" itemprop="url" rel="index">
                    <span itemprop="name">linux</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/linux/%E5%91%BD%E4%BB%A4/" itemprop="url" rel="index">
                    <span itemprop="name">命令</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/%E8%BF%90%E7%BB%B4/top/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/运维/top/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>top命令简单介绍</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2017/06/04/%E8%BF%90%E7%BB%B4/top/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/Spark_ML/gdbt%E5%88%86%E7%B1%BB%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/Spark_ML/gdbt%E5%88%86%E7%B1%BB%E5%99%A8/" class="post-title-link" itemprop="url">gdbt分类器</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/sparkml/" itemprop="url" rel="index">
                    <span itemprop="name">sparkml</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/Spark_ML/gdbt%E5%88%86%E7%B1%BB%E5%99%A8/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/Spark_ML/gdbt分类器/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="gdbt分类器"><a href="#gdbt分类器" class="headerlink" title="gdbt分类器"></a>gdbt分类器</h1><p>[TOC]</p>
<p>GBTClassifier文件中包含有两个class文件：GBTClassifier 和 GBTClassificationModel</p>
<p>##1、GBTClassifier</p>
<p>class GBTClassifier 继承自 Estimator ，由此可见，GBTClassifier完成的工作是模型的评估/训练，实现样本数据到模型的过程。</p>
<p>class GBTClassifier 主要方法有相关参数的设置方法、一个train方法和copy方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GBTClassifier</span> <span class="title">@Since</span>(<span class="params">"1.4.0"</span>) (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    @<span class="type">Since</span>("1.4.0"</span>) <span class="title">override</span> <span class="title">val</span> <span class="title">uid</span></span>: <span class="type">String</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Predictor</span>[<span class="type">Vector</span>, <span class="type">GBTClassifier</span>, <span class="type">GBTClassificationModel</span>]</span><br><span class="line">  <span class="keyword">with</span> <span class="type">GBTClassifierParams</span> <span class="keyword">with</span> <span class="type">DefaultParamsWritable</span> <span class="keyword">with</span> <span class="type">Logging</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setMaxDepth</span></span>(value: <span class="type">Int</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(maxDepth, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setMaxBins</span></span>(value: <span class="type">Int</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(maxBins, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setMinInstancesPerNode</span></span>(value: <span class="type">Int</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(minInstancesPerNode, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setMinInfoGain</span></span>(value: <span class="type">Double</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(minInfoGain, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setMaxMemoryInMB</span></span>(value: <span class="type">Int</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(maxMemoryInMB, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setCacheNodeIds</span></span>(value: <span class="type">Boolean</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(cacheNodeIds, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setCheckpointInterval</span></span>(value: <span class="type">Int</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(checkpointInterval, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setImpurity</span></span>(value: <span class="type">String</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= &#123;</span><br><span class="line">    logWarning(<span class="string">"GBTClassifier.setImpurity should NOT be used"</span>)</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setSubsamplingRate</span></span>(value: <span class="type">Double</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(subsamplingRate, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setSeed</span></span>(value: <span class="type">Long</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(seed, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setMaxIter</span></span>(value: <span class="type">Int</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(maxIter, value)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setStepSize</span></span>(value: <span class="type">Double</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(stepSize, value)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">setLossType</span></span>(value: <span class="type">String</span>): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= set(lossType, value)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">train</span></span>(dataset: <span class="type">Dataset</span>[_]): <span class="type">GBTClassificationModel</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> categoricalFeatures: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Int</span>] =</span><br><span class="line">      <span class="type">MetadataUtils</span>.getCategoricalFeatures(dataset.schema($(featuresCol)))</span><br><span class="line">    <span class="keyword">val</span> oldDataset: <span class="type">RDD</span>[<span class="type">LabeledPoint</span>] =</span><br><span class="line">      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Row</span>(label: <span class="type">Double</span>, features: <span class="type">Vector</span>) =&gt;</span><br><span class="line">          require(label == <span class="number">0</span> || label == <span class="number">1</span>, <span class="string">s"GBTClassifier was given"</span> +</span><br><span class="line">            <span class="string">s" dataset with invalid label <span class="subst">$label</span>.  Labels must be in &#123;0,1&#125;; note that"</span> +</span><br><span class="line">            <span class="string">s" GBTClassifier currently only supports binary classification."</span>)</span><br><span class="line">          <span class="type">LabeledPoint</span>(label, features)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">val</span> numFeatures = oldDataset.first().features.size</span><br><span class="line">    <span class="keyword">val</span> boostingStrategy = <span class="keyword">super</span>.getOldBoostingStrategy(categoricalFeatures, <span class="type">OldAlgo</span>.<span class="type">Classification</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> instr = <span class="type">Instrumentation</span>.create(<span class="keyword">this</span>, oldDataset)</span><br><span class="line">    instr.logParams(params: _*)</span><br><span class="line">    instr.logNumFeatures(numFeatures)</span><br><span class="line">    instr.logNumClasses(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> (baseLearners, learnerWeights) = <span class="type">GradientBoostedTrees</span>.run(oldDataset, boostingStrategy,</span><br><span class="line">      $(seed))</span><br><span class="line">    <span class="keyword">val</span> m = <span class="keyword">new</span> <span class="type">GBTClassificationModel</span>(uid, baseLearners, learnerWeights, numFeatures)</span><br><span class="line">    instr.logSuccess(m)</span><br><span class="line">    m</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Since</span>(<span class="string">"1.4.1"</span>)</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(extra: <span class="type">ParamMap</span>): <span class="type">GBTClassifier</span> = defaultCopy(extra)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="二、GBTClassificationModel"><a href="#二、GBTClassificationModel" class="headerlink" title="二、GBTClassificationModel"></a>二、GBTClassificationModel</h2><p>GBTClassificationModel 继承自 PredictionModel，一个 Transformer，完成 DataFrame 到 DataFrame 的转换。</p>
<p>GBTClassificationModel中比较重要的方法是 transformImpl() 和 predict()。</p>
<p>transformImpl方法主要完成的功能是将 featuresCol数据 进行相关计算，得到 predict值，并将该值储存为新列。</p>
<p>需要注意的是，gdbt中 最终得到的 predict值 会是 1 或 0，无法得到 预测分数值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">transformImpl</span></span>(dataset: <span class="type">Dataset</span>[_]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> bcastModel = dataset.sparkSession.sparkContext.broadcast(<span class="keyword">this</span>)</span><br><span class="line">  <span class="keyword">val</span> predictUDF = udf &#123; (features: <span class="type">Any</span>) =&gt;</span><br><span class="line">    bcastModel.value.predict(features.asInstanceOf[<span class="type">Vector</span>])</span><br><span class="line">  &#125;</span><br><span class="line">  dataset.withColumn($(predictionCol), predictUDF(col($(featuresCol))))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">predict</span></span>(features: <span class="type">Vector</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> When we add a generic Boosting class, handle transform there?  SPARK-7129</span></span><br><span class="line">  <span class="comment">// Classifies by thresholding sum of weighted tree predictions</span></span><br><span class="line">  <span class="keyword">val</span> treePredictions = _trees.map(_.rootNode.predictImpl(features).prediction)</span><br><span class="line">  <span class="keyword">val</span> prediction = blas.ddot(numTrees, treePredictions, <span class="number">1</span>, _treeWeights, <span class="number">1</span>)</span><br><span class="line">  <span class="keyword">if</span> (prediction &gt; <span class="number">0.0</span>) <span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.0</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/Spark_ML/pipeline%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/Spark_ML/pipeline%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">MLLib Pipeline的实现分析(转)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/sparkml/" itemprop="url" rel="index">
                    <span itemprop="name">sparkml</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/Spark_ML/pipeline%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/Spark_ML/pipeline实现分析/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>文章来源：</strong><a href="https://github.com/ColZer/DigAndBuried/blob/master/spark/mllib-pipeline.md" target="_blank" rel="noopener">https://github.com/ColZer/DigAndBuried/blob/master/spark/mllib-pipeline.md</a></p>
<img src='data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" contentScriptType="application/ecmascript" contentStyleType="text/css" height="558px" preserveAspectRatio="none" style="width:398px;height:558px;" version="1.1" viewBox="0 0 398 558" width="398px" zoomAndPan="magnify"><defs><filter height="300%" id="f17g3x8pddy5c3" width="300%" x="-1" y="-1"><feGaussianBlur result="blurOut" stdDeviation="2.0"/><feColorMatrix in="blurOut" result="blurOut2" type="matrix" values="0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .4 0"/><feOffset dx="4.0" dy="4.0" in="blurOut2" result="blurOut3"/><feBlend in="SourceGraphic" in2="blurOut3" mode="normal"/></filter></defs><g><!--MD5=[012f44131d5f7dfe07c208d40db939fb]
class PipelineStage--><rect fill="#FEFECE" filter="url(#f17g3x8pddy5c3)" height="60.8047" id="PipelineStage" style="stroke: #A80036; stroke-width: 1.5;" width="202" x="99.5" y="8"/><ellipse cx="153.25" cy="24" fill="#ADD1B2" rx="11" ry="11" style="stroke: #A80036; stroke-width: 1.0;"/><path d="M155.5938,19.6719 L155.7656,19.75 C155.9844,19.4375 156.1875,19.3438 156.4844,19.3438 C156.7813,19.3438 157.0625,19.4844 157.2188,19.75 C157.3125,19.9063 157.3281,20.0313 157.3281,20.4688 L157.3281,21.8906 C157.3281,22.3125 157.2969,22.5 157.1875,22.6563 C157.0156,22.875 156.75,23.0156 156.4844,23.0156 C156.2656,23.0156 156.0313,22.9063 155.8906,22.7656 C155.75,22.6406 155.7188,22.5156 155.6563,22.1094 C155.5625,21.7031 155.3906,21.4844 154.9063,21.2031 C154.4375,20.9531 153.8281,20.7969 153.25,20.7969 C151.5156,20.7969 150.2656,22.1094 150.2656,23.8906 L150.2656,24.9844 C150.2656,26.6875 151.5625,27.7813 153.6094,27.7813 C154.375,27.7813 155.0625,27.6563 155.4844,27.3906 C155.6719,27.2969 155.6719,27.2969 156.125,26.8125 C156.3125,26.625 156.5156,26.5469 156.7344,26.5469 C157.2031,26.5469 157.5938,26.9375 157.5938,27.3906 C157.5938,27.7813 157.2656,28.2344 156.6875,28.6406 C155.9375,29.1875 154.7813,29.4844 153.5625,29.4844 C150.6719,29.4844 148.5625,27.5938 148.5625,25.0156 L148.5625,23.8906 C148.5625,21.1719 150.5625,19.0938 153.1875,19.0938 C154.0625,19.0938 154.6563,19.2344 155.5938,19.6719 Z "/><text fill="#000000" font-family="sans-serif" font-size="12" lengthAdjust="spacingAndGlyphs" textLength="86" x="173.75" y="28.1543">PipelineStage</text><line style="stroke: #A80036; stroke-width: 1.5;" x1="100.5" x2="300.5" y1="40" y2="40"/><line style="stroke: #A80036; stroke-width: 1.5;" x1="100.5" x2="300.5" y1="48" y2="48"/><ellipse cx="110.5" cy="59" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="176" x="119.5" y="62.2104">StructType transformSchema()</text><!--MD5=[238dbd9944093bcdc066e7d3f25fa0ed]
class Estimator--><rect fill="#FEFECE" filter="url(#f17g3x8pddy5c3)" height="60.8047" id="Estimator" style="stroke: #A80036; stroke-width: 1.5;" width="92" x="72.5" y="129"/><ellipse cx="87.5" cy="145" fill="#ADD1B2" rx="11" ry="11" style="stroke: #A80036; stroke-width: 1.0;"/><path d="M89.8438,140.6719 L90.0156,140.75 C90.2344,140.4375 90.4375,140.3438 90.7344,140.3438 C91.0313,140.3438 91.3125,140.4844 91.4688,140.75 C91.5625,140.9063 91.5781,141.0313 91.5781,141.4688 L91.5781,142.8906 C91.5781,143.3125 91.5469,143.5 91.4375,143.6563 C91.2656,143.875 91,144.0156 90.7344,144.0156 C90.5156,144.0156 90.2813,143.9063 90.1406,143.7656 C90,143.6406 89.9688,143.5156 89.9063,143.1094 C89.8125,142.7031 89.6406,142.4844 89.1563,142.2031 C88.6875,141.9531 88.0781,141.7969 87.5,141.7969 C85.7656,141.7969 84.5156,143.1094 84.5156,144.8906 L84.5156,145.9844 C84.5156,147.6875 85.8125,148.7813 87.8594,148.7813 C88.625,148.7813 89.3125,148.6563 89.7344,148.3906 C89.9219,148.2969 89.9219,148.2969 90.375,147.8125 C90.5625,147.625 90.7656,147.5469 90.9844,147.5469 C91.4531,147.5469 91.8438,147.9375 91.8438,148.3906 C91.8438,148.7813 91.5156,149.2344 90.9375,149.6406 C90.1875,150.1875 89.0313,150.4844 87.8125,150.4844 C84.9219,150.4844 82.8125,148.5938 82.8125,146.0156 L82.8125,144.8906 C82.8125,142.1719 84.8125,140.0938 87.4375,140.0938 C88.3125,140.0938 88.9063,140.2344 89.8438,140.6719 Z "/><text fill="#000000" font-family="sans-serif" font-size="12" lengthAdjust="spacingAndGlyphs" textLength="60" x="101.5" y="149.1543">Estimator</text><line style="stroke: #A80036; stroke-width: 1.5;" x1="73.5" x2="163.5" y1="161" y2="161"/><line style="stroke: #A80036; stroke-width: 1.5;" x1="73.5" x2="163.5" y1="169" y2="169"/><ellipse cx="83.5" cy="180" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="56" x="92.5" y="183.2104">Model fit()</text><!--MD5=[71c385664aa3d34c75f719e0a7af1215]
class Transformer--><rect fill="#FEFECE" filter="url(#f17g3x8pddy5c3)" height="60.8047" id="Transformer" style="stroke: #A80036; stroke-width: 1.5;" width="157" x="204" y="129"/><ellipse cx="241.05" cy="145" fill="#ADD1B2" rx="11" ry="11" style="stroke: #A80036; stroke-width: 1.0;"/><path d="M243.3938,140.6719 L243.5656,140.75 C243.7844,140.4375 243.9875,140.3438 244.2844,140.3438 C244.5813,140.3438 244.8625,140.4844 245.0188,140.75 C245.1125,140.9063 245.1281,141.0313 245.1281,141.4688 L245.1281,142.8906 C245.1281,143.3125 245.0969,143.5 244.9875,143.6563 C244.8156,143.875 244.55,144.0156 244.2844,144.0156 C244.0656,144.0156 243.8313,143.9063 243.6906,143.7656 C243.55,143.6406 243.5188,143.5156 243.4563,143.1094 C243.3625,142.7031 243.1906,142.4844 242.7063,142.2031 C242.2375,141.9531 241.6281,141.7969 241.05,141.7969 C239.3156,141.7969 238.0656,143.1094 238.0656,144.8906 L238.0656,145.9844 C238.0656,147.6875 239.3625,148.7813 241.4094,148.7813 C242.175,148.7813 242.8625,148.6563 243.2844,148.3906 C243.4719,148.2969 243.4719,148.2969 243.925,147.8125 C244.1125,147.625 244.3156,147.5469 244.5344,147.5469 C245.0031,147.5469 245.3938,147.9375 245.3938,148.3906 C245.3938,148.7813 245.0656,149.2344 244.4875,149.6406 C243.7375,150.1875 242.5813,150.4844 241.3625,150.4844 C238.4719,150.4844 236.3625,148.5938 236.3625,146.0156 L236.3625,144.8906 C236.3625,142.1719 238.3625,140.0938 240.9875,140.0938 C241.8625,140.0938 242.4563,140.2344 243.3938,140.6719 Z "/><text fill="#000000" font-family="sans-serif" font-size="12" lengthAdjust="spacingAndGlyphs" textLength="76" x="259.95" y="149.1543">Transformer</text><line style="stroke: #A80036; stroke-width: 1.5;" x1="205" x2="360" y1="161" y2="161"/><line style="stroke: #A80036; stroke-width: 1.5;" x1="205" x2="360" y1="169" y2="169"/><ellipse cx="215" cy="180" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="131" x="224" y="183.2104">DataFrame transform()</text><!--MD5=[02ca4528c2e69c2b329eaf51e1453b5a]
class PipelineModel--><rect fill="#FEFECE" filter="url(#f17g3x8pddy5c3)" height="99.2188" id="PipelineModel" style="stroke: #A80036; stroke-width: 1.5;" width="202" x="185.5" y="448"/><ellipse cx="239.25" cy="464" fill="#ADD1B2" rx="11" ry="11" style="stroke: #A80036; stroke-width: 1.0;"/><path d="M241.5938,459.6719 L241.7656,459.75 C241.9844,459.4375 242.1875,459.3438 242.4844,459.3438 C242.7813,459.3438 243.0625,459.4844 243.2188,459.75 C243.3125,459.9063 243.3281,460.0313 243.3281,460.4688 L243.3281,461.8906 C243.3281,462.3125 243.2969,462.5 243.1875,462.6563 C243.0156,462.875 242.75,463.0156 242.4844,463.0156 C242.2656,463.0156 242.0313,462.9063 241.8906,462.7656 C241.75,462.6406 241.7188,462.5156 241.6563,462.1094 C241.5625,461.7031 241.3906,461.4844 240.9063,461.2031 C240.4375,460.9531 239.8281,460.7969 239.25,460.7969 C237.5156,460.7969 236.2656,462.1094 236.2656,463.8906 L236.2656,464.9844 C236.2656,466.6875 237.5625,467.7813 239.6094,467.7813 C240.375,467.7813 241.0625,467.6563 241.4844,467.3906 C241.6719,467.2969 241.6719,467.2969 242.125,466.8125 C242.3125,466.625 242.5156,466.5469 242.7344,466.5469 C243.2031,466.5469 243.5938,466.9375 243.5938,467.3906 C243.5938,467.7813 243.2656,468.2344 242.6875,468.6406 C241.9375,469.1875 240.7813,469.4844 239.5625,469.4844 C236.6719,469.4844 234.5625,467.5938 234.5625,465.0156 L234.5625,463.8906 C234.5625,461.1719 236.5625,459.0938 239.1875,459.0938 C240.0625,459.0938 240.6563,459.2344 241.5938,459.6719 Z "/><text fill="#000000" font-family="sans-serif" font-size="12" lengthAdjust="spacingAndGlyphs" textLength="86" x="259.75" y="468.1543">PipelineModel</text><line style="stroke: #A80036; stroke-width: 1.5;" x1="186.5" x2="386.5" y1="480" y2="480"/><line style="stroke: #A80036; stroke-width: 1.5;" x1="186.5" x2="386.5" y1="488" y2="488"/><ellipse cx="196.5" cy="499" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="131" x="205.5" y="502.2104">DataFrame transform()</text><ellipse cx="196.5" cy="511.8047" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="176" x="205.5" y="515.0151">StructType transformSchema()</text><ellipse cx="196.5" cy="524.6094" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="115" x="205.5" y="527.8198">PipelineModel copy()</text><ellipse cx="196.5" cy="537.4141" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="88" x="205.5" y="540.6245">MLWriter write()</text><!--MD5=[5d7f87ad7be124bcf8ad2c9ead1692a4]
class Pipeline--><rect fill="#FEFECE" filter="url(#f17g3x8pddy5c3)" height="137.6328" id="Pipeline" style="stroke: #A80036; stroke-width: 1.5;" width="211" x="6" y="250"/><ellipse cx="82.75" cy="266" fill="#ADD1B2" rx="11" ry="11" style="stroke: #A80036; stroke-width: 1.0;"/><path d="M85.0938,261.6719 L85.2656,261.75 C85.4844,261.4375 85.6875,261.3438 85.9844,261.3438 C86.2813,261.3438 86.5625,261.4844 86.7188,261.75 C86.8125,261.9063 86.8281,262.0313 86.8281,262.4688 L86.8281,263.8906 C86.8281,264.3125 86.7969,264.5 86.6875,264.6563 C86.5156,264.875 86.25,265.0156 85.9844,265.0156 C85.7656,265.0156 85.5313,264.9063 85.3906,264.7656 C85.25,264.6406 85.2188,264.5156 85.1563,264.1094 C85.0625,263.7031 84.8906,263.4844 84.4063,263.2031 C83.9375,262.9531 83.3281,262.7969 82.75,262.7969 C81.0156,262.7969 79.7656,264.1094 79.7656,265.8906 L79.7656,266.9844 C79.7656,268.6875 81.0625,269.7813 83.1094,269.7813 C83.875,269.7813 84.5625,269.6563 84.9844,269.3906 C85.1719,269.2969 85.1719,269.2969 85.625,268.8125 C85.8125,268.625 86.0156,268.5469 86.2344,268.5469 C86.7031,268.5469 87.0938,268.9375 87.0938,269.3906 C87.0938,269.7813 86.7656,270.2344 86.1875,270.6406 C85.4375,271.1875 84.2813,271.4844 83.0625,271.4844 C80.1719,271.4844 78.0625,269.5938 78.0625,267.0156 L78.0625,265.8906 C78.0625,263.1719 80.0625,261.0938 82.6875,261.0938 C83.5625,261.0938 84.1563,261.2344 85.0938,261.6719 Z "/><text fill="#000000" font-family="sans-serif" font-size="12" lengthAdjust="spacingAndGlyphs" textLength="49" x="103.25" y="270.1543">Pipeline</text><line style="stroke: #A80036; stroke-width: 1.5;" x1="7" x2="216" y1="282" y2="282"/><ellipse cx="17" cy="293" fill="none" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="39" x="26" y="296.2104">stages</text><line style="stroke: #A80036; stroke-width: 1.5;" x1="7" x2="216" y1="302.8047" y2="302.8047"/><ellipse cx="17" cy="313.8047" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="185" x="26" y="317.0151">Array[PipelineStage] getStages()</text><ellipse cx="17" cy="326.6094" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="91" x="26" y="329.8198">this setStages()</text><ellipse cx="17" cy="339.4141" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="100" x="26" y="342.6245">PipelineModel fit()</text><ellipse cx="17" cy="352.2188" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="82" x="26" y="355.4292">Pipeline copy()</text><ellipse cx="17" cy="365.0234" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="176" x="26" y="368.2339">StructType transformSchema()</text><ellipse cx="17" cy="377.8281" fill="#84BE84" rx="3" ry="3" style="stroke: #038048; stroke-width: 1.0;"/><text fill="#000000" font-family="sans-serif" font-size="11" lengthAdjust="spacingAndGlyphs" textLength="88" x="26" y="381.0386">MLWriter write()</text><!--MD5=[7e20e022571e30ec1001cf6eabd7f3e2]
class Model--><rect fill="#FEFECE" filter="url(#f17g3x8pddy5c3)" height="48" id="Model" style="stroke: #A80036; stroke-width: 1.5;" width="69" x="252" y="295"/><ellipse cx="267" cy="311" fill="#ADD1B2" rx="11" ry="11" style="stroke: #A80036; stroke-width: 1.0;"/><path d="M269.3438,306.6719 L269.5156,306.75 C269.7344,306.4375 269.9375,306.3438 270.2344,306.3438 C270.5313,306.3438 270.8125,306.4844 270.9688,306.75 C271.0625,306.9063 271.0781,307.0313 271.0781,307.4688 L271.0781,308.8906 C271.0781,309.3125 271.0469,309.5 270.9375,309.6563 C270.7656,309.875 270.5,310.0156 270.2344,310.0156 C270.0156,310.0156 269.7813,309.9063 269.6406,309.7656 C269.5,309.6406 269.4688,309.5156 269.4063,309.1094 C269.3125,308.7031 269.1406,308.4844 268.6563,308.2031 C268.1875,307.9531 267.5781,307.7969 267,307.7969 C265.2656,307.7969 264.0156,309.1094 264.0156,310.8906 L264.0156,311.9844 C264.0156,313.6875 265.3125,314.7813 267.3594,314.7813 C268.125,314.7813 268.8125,314.6563 269.2344,314.3906 C269.4219,314.2969 269.4219,314.2969 269.875,313.8125 C270.0625,313.625 270.2656,313.5469 270.4844,313.5469 C270.9531,313.5469 271.3438,313.9375 271.3438,314.3906 C271.3438,314.7813 271.0156,315.2344 270.4375,315.6406 C269.6875,316.1875 268.5313,316.4844 267.3125,316.4844 C264.4219,316.4844 262.3125,314.5938 262.3125,312.0156 L262.3125,310.8906 C262.3125,308.1719 264.3125,306.0938 266.9375,306.0938 C267.8125,306.0938 268.4063,306.2344 269.3438,306.6719 Z "/><text fill="#000000" font-family="sans-serif" font-size="12" lengthAdjust="spacingAndGlyphs" textLength="37" x="281" y="315.1543">Model</text><line style="stroke: #A80036; stroke-width: 1.5;" x1="253" x2="320" y1="327" y2="327"/><line style="stroke: #A80036; stroke-width: 1.5;" x1="253" x2="320" y1="335" y2="335"/><!--MD5=[6a7fb0614c6412133269ef24af130046]
reverse link PipelineStage to Estimator--><path d="M168.463,85.993 C158.554,100.373 147.91,115.82 139.004,128.745 " fill="none" id="PipelineStage&lt;-Estimator" style="stroke: #A80036; stroke-width: 1.0;"/><polygon fill="none" points="162.907,81.719,180.019,69.222,174.435,89.662,162.907,81.719" style="stroke: #A80036; stroke-width: 1.0;"/><!--MD5=[5f04479e994c710a603e4205e4808e72]
reverse link PipelineStage to Transformer--><path d="M232.537,85.993 C242.446,100.373 253.09,115.82 261.996,128.745 " fill="none" id="PipelineStage&lt;-Transformer" style="stroke: #A80036; stroke-width: 1.0;"/><polygon fill="none" points="226.565,89.662,220.981,69.222,238.093,81.719,226.565,89.662" style="stroke: #A80036; stroke-width: 1.0;"/><!--MD5=[46e8da132d9c6dfd97c8bc4e302ad6bc]
reverse link Transformer to Model--><path d="M283.765,210.294 C284.482,238.522 285.345,272.515 285.907,294.66 " fill="none" id="Transformer&lt;-Model" style="stroke: #A80036; stroke-width: 1.0;"/><polygon fill="none" points="276.762,210.296,283.252,190.125,290.758,209.94,276.762,210.296" style="stroke: #A80036; stroke-width: 1.0;"/><!--MD5=[71bbf4c10f71bc859c1881ada18cc43c]
reverse link Model to PipelineModel--><path d="M286.5,363.444 C286.5,388.984 286.5,421.339 286.5,447.7485 " fill="none" id="Model&lt;-PipelineModel" style="stroke: #A80036; stroke-width: 1.0;"/><polygon fill="none" points="279.5,363.074,286.5,343.074,293.5,363.074,279.5,363.074" style="stroke: #A80036; stroke-width: 1.0;"/><!--MD5=[682e5639eb8e01a70eb7de860ed55956]
reverse link Estimator to Pipeline--><path d="M116.277,210.514 C115.72,223.054 115.112,236.72 114.523,249.974 " fill="none" id="Estimator&lt;-Pipeline" style="stroke: #A80036; stroke-width: 1.0;"/><polygon fill="none" points="109.302,209.794,117.183,190.125,123.288,210.416,109.302,209.794" style="stroke: #A80036; stroke-width: 1.0;"/><!--MD5=[8ac852fbd2d3fab8332da25cd99477c8]
@startuml
class PipelineStage{
  + StructType transformSchema()
}

class Estimator{
  + Model fit()
}

class Transformer{
  + DataFrame transform()
}

class PipelineModel{
  + DataFrame transform()
  + StructType transformSchema()
  + PipelineModel copy()
  + MLWriter write()
}

class Pipeline{
  + stages
  + Array[PipelineStage] getStages()
  + this setStages()
  + PipelineModel fit()
  + Pipeline copy()
  + StructType transformSchema()
  + MLWriter write()
}

PipelineStage <|- - Estimator
PipelineStage <|- - Transformer
Transformer <|- - Model
Model <|- - PipelineModel
Estimator <|- - Pipeline
@enduml

PlantUML version 1.2020.02beta5(Unknown compile time)
(GPL source distribution)
Java Runtime: Java(TM) SE Runtime Environment
JVM: Java HotSpot(TM) 64-Bit Server VM
Java Version: 1.7.0_25-b15
Operating System: Linux
Default Encoding: UTF-8
Language: en
Country: US
--></g></svg>'>

<p>以下为原文:</p>
<p>在2014年11月,他就在Spark MLLib代码中CI了一个全新的package:”org.apache.spark.ml”, 和传统的”org.apache.spark.mllib”独立, 这个包即Spark MLLib的<br><a href="https://docs.google.com/document/d/1rVwXRjWKfIb-7PI6b86ipytwbUH7irSNLF1_6dLmh8o/edit#heading=h.kaihowy4sg6c" target="_blank" rel="noopener">Pipeline and Parameters</a></p>
<p>pipeline即机器学习流水线, 在实际的机器学习应用过程中,一个任务(Job)由很多过程(Stage)组成, 比如日志的收集,清理,到字段/属性/feature的提取, 多算法的组合而成的模型,<br>模型的离线训练和评估,到最后的模型在线服务和在线评估,所进行的每一个stage都可以概况为数据到数据的转换以及数据到模型的转换. </p>
<p>后面我们会看到,mllib pipeline中的stage也做了相应的划分.</p>
<p>Parameters即参数化,机器学习大部分的过程是一个参数调优的过程,一个模型应该很显示的告诉使用者,模型有那些参数可以进行设置,以及这些参数的默认值是怎么样的;在现有的机器学习算法中,<br>模型的参数可能是以模型类字段或通过一些函数进行设置,不是很直观的展现当前模型有哪些可以调优的参数;</p>
<p>针对这个问题,在这个版本中,提出了Parameters功能,通过trait的方式显式地指定stage拥有那些参数.</p>
<p>下面我们就针对这两个部分进行分析, pipeline到我今天学习为止,还只有三个PR,很多东西应该还在实验中,这里做一个分析仅供学习和备忘.</p>
<p>##一、Parameters</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span> (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    override val weights: <span class="type">Vector</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    override val intercept: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">GeneralizedLinearModel</span>(<span class="params">weights, intercept</span>) <span class="keyword">with</span> <span class="title">ClassificationModel</span> <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> threshold: <span class="type">Option</span>[<span class="type">Double</span>] = <span class="type">Some</span>(<span class="number">0.5</span>)</span><br><span class="line">  <span class="comment">//...</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>上面是传统的org.apache.spark.mllib包中一个分类器:LogisticRegressionModel, 如果理解logistics分类器,那么我知道其中的threshold为模型一个很重要的参数.<br>但是从对于一般的用户来说,我们只知道这个模型类中有一个threshold字段,并不能很清楚了该字段是否是模型可调优的参数,还是只是类的一个”全局变量”而已;</p>
<p>针对这个问题, 就有了Parameters参数化的概念,先直接看结果:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegressionModel</span> <span class="title">private</span>[ml] (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    override val parent: <span class="type">LogisticRegression</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    override val fittingParamMap: <span class="type">ParamMap</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    weights: <span class="type">Vector</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Model</span>[<span class="type">LogisticRegressionModel</span>] <span class="keyword">with</span> <span class="title">LogisticRegressionParams</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[classification] <span class="class"><span class="keyword">trait</span> <span class="title">LogisticRegressionParams</span> <span class="keyword">extends</span> <span class="title">Params</span></span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">HasRegParam</span> <span class="keyword">with</span> <span class="title">HasMaxIter</span> <span class="keyword">with</span> <span class="title">HasLabelCol</span> <span class="keyword">with</span> <span class="title">HasThreshold</span> <span class="keyword">with</span> <span class="title">HasFeaturesCol</span></span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">HasScoreCol</span> <span class="keyword">with</span> <span class="title">HasPredictionCol</span> </span>&#123;</span><br></pre></td></tr></table></figure>

<p>我们看到这里的LogisticRegressionModel实现了LogisticRegressionParams,而LogisticRegressionParams继承了Params类,并且”拥有”一组Param,即HasMaxIter, HasRegParam之类.<br>相比传统的LogisticRegressionModel, 这个版本我们可以清楚的看到,该模型有RegParam, MaxIter等7个可控参数,其中包括我们上面谈到的threshold参数, 即HasThreshold.</p>
<p>即Parameters 将mllib中的组件参数进行标准化和可视化,下面我们继续针对Parameters进行分析.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Param</span>[<span class="type">T</span>] (<span class="params">val parent: <span class="type">Params</span>,val name: <span class="type">String</span>,val doc: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val defaultValue: <span class="type">Option</span>[<span class="type">T</span>] = <span class="type">None</span></span>)<span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br></pre></td></tr></table></figure>

<p>Param表示一个参数,从概念上来说,一个参数有下面几个属性:</p>
<ul>
<li>param的类型:即上面的[T], 它表示param值是何种类型</li>
<li>param的名称:即name</li>
<li>param的描述信息,和name相比, 它可以更长更详细, 即doc</li>
<li>param的默认值, 即defaultValue</li>
</ul>
<p>针对param的类型,ml提供了一组默认的子类, 如IntParam,FloatParam之类的.就不详细展开</p>
<p>另外针对Param, 提供了接口来设置Param的值</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">w</span></span>(value: <span class="type">T</span>): <span class="type">ParamPair</span>[<span class="type">T</span>] = <span class="keyword">this</span> -&gt; value</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">-&gt;</span></span>(value: <span class="type">T</span>): <span class="type">ParamPair</span>[<span class="type">T</span>] = <span class="type">ParamPair</span>(<span class="keyword">this</span>, value)</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ParamPair</span>[<span class="type">T</span>](<span class="params">param: <span class="type">Param</span>[<span class="type">T</span>], value: <span class="type">T</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>即将Param和value封装为paramPair类, paramPair是一个case类,没有其他的方法, 仅仅封装了Param和Value对. 因此我们可以通过param-&gt;value来创建一个ParamPair, 后面我们会看到怎么对ParamPair<br>进行管理.</p>
<p>上面谈到Param其中一个参数我们没有进行描述, 即”parent: Params”. 如果站在模型, 特征提取器等组件角度来, 它们应该是一个param的容器,它们的逻辑代码可以使用自身的容器中的param.<br>换句话说,如果一个组件继承自Params类,那么该组件就是一个被参数化的组件.</p>
<p>参考上面的LogisticRegressionModel, 它继承了LogisticRegressionParams, 而LogisticRegressionParams实现了Params, 此时LogisticRegressionModel就是一个被参数化的模型. 即自身是一个param容器</p>
<p>现在问题来了, 一个组件继承自Params, 然而我们没有通过add等方式来将相应的param添加到这个容器里面, 而是通过”with HasRegParam”方式来进行设置的,到底是怎么实现的呢?看一个rf现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[ml] <span class="class"><span class="keyword">trait</span> <span class="title">HasRegParam</span> <span class="keyword">extends</span> <span class="title">Params</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> regParam: <span class="type">DoubleParam</span> = <span class="keyword">new</span> <span class="type">DoubleParam</span>(<span class="keyword">this</span>, <span class="string">"regParam"</span>, <span class="string">"regularization parameter"</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getRegParam</span></span>: <span class="type">Double</span> = get(regParam)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[ml] <span class="class"><span class="keyword">trait</span> <span class="title">HasMaxIter</span> <span class="keyword">extends</span> <span class="title">Params</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> maxIter: <span class="type">IntParam</span> = <span class="keyword">new</span> <span class="type">IntParam</span>(<span class="keyword">this</span>, <span class="string">"maxIter"</span>, <span class="string">"max number of iterations"</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getMaxIter</span></span>: <span class="type">Int</span> = get(maxIter)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们看到每个具体的RegParam都是继承自Params, 这个继承感觉意义不大,所有这里就不纠结它的继承机制, 核心是它的<code>val regParam: DoubleParam</code>常量的定义,这里的常量会被编译为一个函数,<br>其中函数为public, 返回值为DoubleParam, 参数为空. 为什么要强调这个呢?这是规范. 一个具体的Param只有这样的实现, 它才会被组件的Params容器捕获. 怎么捕获呢? 在Params中有这样一个代码:<br>​    </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 java 反射的方法列出所有的(没有入参和返回值为Param)的方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">params</span></span>: <span class="type">Array</span>[<span class="type">Param</span>[_]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> methods = <span class="keyword">this</span>.getClass.getMethods</span><br><span class="line">    methods.filter &#123; m =&gt;</span><br><span class="line">        <span class="type">Modifier</span>.isPublic(m.getModifiers) &amp;&amp;</span><br><span class="line">          classOf[<span class="type">Param</span>[_]].isAssignableFrom(m.getReturnType) &amp;&amp;</span><br><span class="line">          m.getParameterTypes.isEmpty</span><br><span class="line">      &#125;.sortBy(_.getName)</span><br><span class="line">      .map(m =&gt; m.invoke(<span class="keyword">this</span>).asInstanceOf[<span class="type">Param</span>[_]])</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里就清晰了,一个组件,  继承Params类, 成为一个可参数化的组件, 该组件就有一个params方法可以返回该组件所有配置信息,即 Array[Param[_]]. 因为我们组件使用With方式继承了符合上面规范的”常量定义”,<br>这些param会被这里的def params所捕获, 从而返回该组件所有的Params;</p>
<p>不过还有一个问题,我们一直在说, 组件继承Params类, 成为一个可参数化的组件,但是我这里def params只是返回 Array[Param[_]], 而一个属性应该有Param和value组成, 即上面我们谈到ParamPair,<br>因此Params类中应该还有一个容器,维护Array[ParamPair], 或者还有一个Map,维护param-&gt;value对.</p>
<p>没错,这就是ParamMap的功能, 它维护了维护param-&gt;value对.并且每个实现Params的组件都有一个paramMap字段. 如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Params</span> <span class="keyword">extends</span> <span class="title">Identifiable</span> <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">val</span> paramMap: <span class="type">ParamMap</span> = <span class="type">ParamMap</span>.empty</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>具体的ParamMap实现这里就不分析, 就是一个Map的封装, 这里我们总结一下Parameters的功能: </p>
<blockquote>
<p>  Parameters即组件的可参数化, 一个组件,可以是模型,可以是特征选择器, 如果继承自Params, 那么它就是被参数化的组件, 将具体参数按照HasMaxIter类的规范进行定义, 然后通过With的<br>  方式追加到组件中,从而表示该组件有相应的参数, 并通过调用Params中的getParam,set等方法来操作相应的param.</p>
</blockquote>
<p>整体来说Parameters的功能就是组件参数标准化</p>
<h2 id="二、Pipeline"><a href="#二、Pipeline" class="headerlink" title="二、Pipeline"></a>二、Pipeline</h2><p>模型训练是整个数据挖掘和机器学习的目标, 如果把整个过程看着为一个黑盒, 内部可以包含了很多很多的特征提取, 模型训练等子stage,<br>但是站在黑盒的外面, 整个黑盒的输出就是一个模型(Model), 我们目标就是训练出一个完美的模型, 然后再利于该模型去做服务. </p>
<p>这句话就是pipeline的核心, 首先pipeline是一个黑盒生成的过程, 它对外提供fit接口, 完成黑盒的训练, 生成一个黑盒模型, 即PipelineModel</p>
<p>如果站在白盒的角度来看, pipeline的黑盒中肯定维护了一组stage, 这些stage可以是原子的stage,也可能继续是一个黑盒模型, 在fit接口调用时候,<br>会按照流水线的顺序依次调用每个stage的fit/transform函数,最后输出PipelineModel. </p>
<h3 id="2-1-PipelineStage"><a href="#2-1-PipelineStage" class="headerlink" title="2.1 PipelineStage"></a>2.1 PipelineStage</h3><p>如上所言, mllib pipeline是基于Spark SQL中的schemeRDD来实现, pipeline中每一次处理操作都表现为对schemeRDD的scheme或数据进行处理, 这里的操作步骤被抽象为Stage, 即PipelineStage</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">PipelineStage</span> <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span>[ml] <span class="function"><span class="keyword">def</span> <span class="title">transformSchema</span></span>(schema: <span class="type">StructType</span>, paramMap: <span class="type">ParamMap</span>): <span class="type">StructType</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>抽象的PipelineStage的实现很简单, 只提供了transformSchema虚函数, 由具体的stage进行实现,从而在一定参数paramMap作用下,对scheme进行修改(transform). </p>
<p>上面我们也谈到, 具体的stage是在PipelineStage基础上划分为两大类, 即数据到数据的转换(transform)以及数据到模型的转换(fit).</p>
<ul>
<li>Transformer:  数据到数据的转换</li>
<li>Estimator:    数据到模型的转换</li>
</ul>
<h4 id="2-1-1-Transformer"><a href="#2-1-1-Transformer" class="headerlink" title="2.1.1 Transformer"></a>2.1.1 Transformer</h4><p>我们首先来看Transformer, 数据预处理, 特征选择与提取都表现为Transformer, 它对提供的SchemaRDD进行转换生成新的SchemaRDD, 如下所示:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Transformer</span> <span class="keyword">extends</span> <span class="title">PipelineStage</span> <span class="keyword">with</span> <span class="title">Params</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>(dataset: <span class="type">SchemaRDD</span>, paramMap: <span class="type">ParamMap</span>): <span class="type">SchemaRDD</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>特殊的Transformer：模型</strong>：</p>
<p>在mllib中, 有一种特殊的Transformer, 即<strong>模型(Model)</strong>, 下面我们会看到模型是Estimator stage的产物,但是model本身是一个Transformer,<br>模型是经过训练和挖掘以后的一个对象, 它的一个主要功能就是预测/推荐服务, 即它可以对传入的dataset:SchemaRDD进行预测, 填充dataset中残缺的决策字段或评分字段, 返回更新后的SchemaRDD</p>
<h4 id="2-1-2-Estimator"><a href="#2-1-2-Estimator" class="headerlink" title="2.1.2 Estimator"></a>2.1.2 Estimator</h4><p>Estimator stage的功能是模型的估计/训练, 即它是一个SchemaRDD到Model的fit过程. 如下所示fit接口. </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Estimator</span>[<span class="type">M</span> &lt;: <span class="type">Model</span>[<span class="type">M</span>]] <span class="keyword">extends</span> <span class="title">PipelineStage</span> <span class="keyword">with</span> <span class="title">Params</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(dataset: <span class="type">SchemaRDD</span>, paramMap: <span class="type">ParamMap</span>): <span class="type">M</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-2-PipelineModel"><a href="#2-2-PipelineModel" class="headerlink" title="2.2 PipelineModel"></a>2.2 PipelineModel</h3><p>PipelineModel是由一组Transformer组成, 在对dataset进行预测(transform)时,  是按照Transformer的有序性(Array)逐步的对dataset进行处理. </p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PipelineModel</span> <span class="title">private</span>[ml] (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    override val parent: <span class="type">Pipeline</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    override val fittingParamMap: <span class="type">ParamMap</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private[ml] val stages: <span class="type">Array</span>[<span class="type">Transformer</span>]</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Model</span>[<span class="type">PipelineModel</span>] <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">      </span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">transform</span></span>(dataset: <span class="type">SchemaRDD</span>, paramMap: <span class="type">ParamMap</span>): <span class="type">SchemaRDD</span> = &#123;</span><br><span class="line">    <span class="comment">// Precedence of ParamMaps: paramMap &gt; this.paramMap &gt; fittingParamMap</span></span><br><span class="line">    <span class="keyword">val</span> map = (fittingParamMap ++ <span class="keyword">this</span>.paramMap) ++ paramMap</span><br><span class="line">    transformSchema(dataset.schema, map, logging = <span class="literal">true</span>)</span><br><span class="line">    stages.foldLeft(dataset)((cur, transformer) =&gt; transformer.transform(cur, map))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="2-3-Pipeline"><a href="#2-3-Pipeline" class="headerlink" title="2.3 Pipeline"></a>2.3 Pipeline</h3><p>Pipeline首先是一个Estimator, fit输出的模型为 PipelineModel,  其次Pipeline也继承Params类, 即被参数化,<br> 其中有一个参数, 即stages, 它的值为Array[PipelineStage], 该参数存储了Pipeline拥有的所有的stage;</p>
<p>Pipeline提供了<code>fit</code>和<code>transformSchema</code>两个接口: </p>
<ul>
<li>transformSchema 接口: 使用foldLeft函数, 对schema进行转换. </li>
<li>fit 接口: 遍历 stages列表。将 Estimator 类型的stage 转换为 model(model其实就是一种Transformer)</li>
</ul>
<pre><code class="scala"><span class="class"><span class="keyword">class</span> <span class="title">Pipeline</span> <span class="keyword">extends</span> <span class="title">Estimator</span>[<span class="type">PipelineModel</span>] </span>{
 <span class="keyword">val</span> stages: <span class="type">Param</span>[<span class="type">Array</span>[<span class="type">PipelineStage</span>]] = <span class="keyword">new</span> <span class="type">Param</span>(<span class="keyword">this</span>, <span class="string">"stages"</span>, <span class="string">"stages of the pipeline"</span>)
 <span class="function"><span class="keyword">def</span> <span class="title">setStages</span></span>(value: <span class="type">Array</span>[<span class="type">PipelineStage</span>]): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= { set(stages, value); <span class="keyword">this</span> }
 <span class="function"><span class="keyword">def</span> <span class="title">getStages</span></span>: <span class="type">Array</span>[<span class="type">PipelineStage</span>] = get(stages)

 <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">fit</span></span>(dataset: <span class="type">SchemaRDD</span>, paramMap: <span class="type">ParamMap</span>): <span class="type">PipelineModel</span> = {
     transformSchema(dataset.schema, paramMap, logging = <span class="literal">true</span>)
     <span class="keyword">val</span> map = <span class="keyword">this</span>.paramMap ++ paramMap
     <span class="keyword">val</span> theStages = map(stages)
     <span class="comment">// 记录最后一个 Estimator 的位置</span>
     <span class="keyword">var</span> indexOfLastEstimator = <span class="number">-1</span>
     theStages.view.zipWithIndex.foreach { <span class="keyword">case</span> (stage, index) =&gt;
       stage <span class="keyword">match</span> {
         <span class="keyword">case</span> _: <span class="type">Estimator</span>[_] =&gt;
           indexOfLastEstimator = index
         <span class="keyword">case</span> _ =&gt;
       }
     }
     <span class="keyword">var</span> curDataset = dataset
     <span class="keyword">val</span> transformers = <span class="type">ListBuffer</span>.empty[<span class="type">Transformer</span>]
     <span class="comment">//遍历所有的stages</span>
     theStages.view.zipWithIndex.foreach { <span class="keyword">case</span> (stage, index) =&gt;
       <span class="comment">// 遍历在 indexOfLastEstimator 之前的stage</span>
       <span class="keyword">if</span> (index &lt;= indexOfLastEstimator) {
         <span class="keyword">val</span> transformer = stage <span class="keyword">match</span> {
           <span class="comment">// 若 stage 为 Estimator，则将 estimator.fit的结果返回 </span>
           <span class="keyword">case</span> estimator: <span class="type">Estimator</span>[_] =&gt;
             estimator.fit(curDataset, paramMap)
           <span class="comment">// 若 stage 为 Transformer，直接返回</span>
           <span class="keyword">case</span> t: <span class="type">Transformer</span> =&gt;
             t
           <span class="keyword">case</span> _ =&gt;
             <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(
               <span class="string">s"Do not support stage <span class="subst">$stage</span> of type <span class="subst">${stage.getClass}</span>"</span>)
         }
         curDataset = transformer.transform(curDataset, paramMap)
         <span class="comment">// 将返回的 transformer 加入 transformers列表</span>
         transformers += transformer
       } <span class="keyword">else</span> {
         <span class="comment">//直接将 stage 转为 Transformer</span>
         transformers += stage.asInstanceOf[<span class="type">Transformer</span>]
       }
     }

     <span class="keyword">new</span> <span class="type">PipelineModel</span>(<span class="keyword">this</span>, map, transformers.toArray)
   }

 <span class="keyword">private</span>[ml] <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">transformSchema</span></span>(schema: <span class="type">StructType</span>, paramMap: <span class="type">ParamMap</span>): <span class="type">StructType</span> = {
     <span class="keyword">val</span> map = <span class="keyword">this</span>.paramMap ++ paramMap
     <span class="keyword">val</span> theStages = map(stages)
     require(theStages.toSet.size == theStages.size,
            <span class="string">"Cannot have duplicate components in a pipeline."</span>)
     theStages.foldLeft(schema)((cur, stage) =&gt; stage.transformSchema(cur, paramMap))
        }
}</code></pre>
<h3 id="2-3-实例讲解"><a href="#2-3-实例讲解" class="headerlink" title="2.3 实例讲解"></a>2.3 实例讲解</h3><pre><code>Transformer1 ---&gt; Estimator1 --&gt; Transformer2 --&gt; Transformer3 --&gt; Estimator2 --&gt; Transformer4</code></pre><p>我们知道Estimator和Transformer的区别是, Estimator需要经过一次fit操作, 才会输出一个Transformer, 而Transformer直接就是Transformer;</p>
<p>对于训练过程中的Transformer,只有一个数据经过Transformer操作后会被后面的Estimator拿来做fit操作的前提下,该Transformer操作才是有意义的,否则训练数据不应该经过该Transformer.</p>
<p>拿上面的Transformer4来说,  它后面没有Estimator操作, 如果对训练数据进行Transformer操作没有任何意义,但是在预测数据上还是有作用的,所以它可以直接用来构建PipelineModel.</p>
<p>对于训练过程中Estimator(非最后一个Estimator), 即上面的Estimator1,非Estimator2, 它训练数据上训练出的模型以后, 需要利用该模型对训练数据进行transform操作,输出的数据<br>继续进行后面Estimator和Transformer操作. </p>
<p>拿缺失值填充的例子来说, 我们可以利用当前数据,训练出一个模型, 该模型计算出每个字段的平均值, 然后我们理解利用这个模型对训练数据进行缺失值填充. </p>
<blockquote>
<p>但是对于最后一个Estimator,其实是没有必要做这个”curDataset = transformer.transform(curDataset, paramMap)”操作的, 所以这里还是可以有优化的!!嘿嘿!!!</p>
</blockquote>
<p>##三、总结</p>
<p>到目前为止,已经将Pipeline讲解的比较清楚了, 利用Pipeline可以将数据挖掘中各个步骤进行流水线化, api方便还是很简介清晰的!</p>
<p>最后拿孟祥瑞CI的描述信息中一个例子做结尾,其中pipeline包含两个stage,顺序为StandardScaler和LogisticRegression</p>
<pre><code>val scaler = new StandardScaler() 
    .setInputCol(&quot;features&quot;) 
    .setOutputCol(&quot;scaledFeatures&quot;) 
val lr = new LogisticRegression() 
    .setFeaturesCol(scaler.getOutputCol) 

val pipeline = new Pipeline() 
    .setStages(Array(scaler, lr)) 
val model = pipeline.fit(dataset) 
val predictions = model.transform(dataset) 
    .select(&apos;label, &apos;score, &apos;prediction)</code></pre>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark-sql%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark-sql%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86/" class="post-title-link" itemprop="url">Date and Time Functions</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/" itemprop="url" rel="index">
                    <span itemprop="name">数据处理实践</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark-sql%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/数据处理实践/spark-sql时间处理/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Date-and-Time-Functions"><a href="#Date-and-Time-Functions" class="headerlink" title="Date and Time Functions"></a>Date and Time Functions</h2><p>[TOC]</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>[current_date]</td>
<td>Gives current date as a date column</td>
</tr>
<tr>
<td>[current_timestamp]</td>
<td></td>
</tr>
<tr>
<td>[date_format]</td>
<td></td>
</tr>
<tr>
<td>[to_date]</td>
<td>Converts column to date type (with an optional date format)</td>
</tr>
<tr>
<td>[to_timestamp]</td>
<td>Converts column to timestamp type (with an optional timestamp format)</td>
</tr>
<tr>
<td>[unix_timestamp]</td>
<td>Converts current or specified time to Unix timestamp (in seconds)</td>
</tr>
<tr>
<td>[window]</td>
<td>Generates time windows (i.e. tumbling, sliding and delayed windows)</td>
</tr>
</tbody></table>
<h3 id="current-date-Current-Date-As-Date-Column"><a href="#current-date-Current-Date-As-Date-Column" class="headerlink" title="current_date - Current Date As Date Column"></a>current_date - Current Date As Date Column</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">current_date(): Column</span><br></pre></td></tr></table></figure>

<p><code>current_date</code> function gives the current date as a [date] column.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val df &#x3D; spark.range(1).select(current_date)</span><br><span class="line">scala&gt; df.show</span><br><span class="line">+--------------+</span><br><span class="line">|current_date()|</span><br><span class="line">+--------------+</span><br><span class="line">|    2017-09-16|</span><br><span class="line">+--------------+</span><br><span class="line"></span><br><span class="line">scala&gt; df.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- current_date(): date (nullable &#x3D; false)</span><br></pre></td></tr></table></figure>

<p>Internally, <code>current_date</code> creates a <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Column.html" target="_blank" rel="noopener">Column</a> with <code>CurrentDate</code> Catalyst leaf expression.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val c &#x3D; current_date()</span><br><span class="line">import org.apache.spark.sql.catalyst.expressions.CurrentDate</span><br><span class="line">val cd &#x3D; c.expr.asInstanceOf[CurrentDate]</span><br><span class="line">scala&gt; println(cd.prettyName)</span><br><span class="line">current_date</span><br><span class="line"></span><br><span class="line">scala&gt; println(cd.numberedTreeString)</span><br><span class="line">00 current_date(None)</span><br></pre></td></tr></table></figure>

<h3 id="date-format"><a href="#date-format" class="headerlink" title="date_format"></a>date_format</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">date_format(dateExpr: Column, format: String): Column</span><br></pre></td></tr></table></figure>

<p>Internally, <code>date_format</code> creates a <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Column.html" target="_blank" rel="noopener">Column</a> with <code>DateFormatClass</code> binary expression. <code>DateFormatClass</code> takes the expression from <code>dateExpr</code> column and <code>format</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">val c &#x3D; date_format($&quot;date&quot;, &quot;dd&#x2F;MM&#x2F;yyyy&quot;)</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.catalyst.expressions.DateFormatClass</span><br><span class="line">val dfc &#x3D; c.expr.asInstanceOf[DateFormatClass]</span><br><span class="line">scala&gt; println(dfc.prettyName)</span><br><span class="line">date_format</span><br><span class="line"></span><br><span class="line">scala&gt; println(dfc.numberedTreeString)</span><br><span class="line">00 date_format(&#39;date, dd&#x2F;MM&#x2F;yyyy, None)</span><br><span class="line">01 :- &#39;date</span><br><span class="line">02 +- dd&#x2F;MM&#x2F;yyyy</span><br></pre></td></tr></table></figure>

<h3 id="current-timestamp"><a href="#current-timestamp" class="headerlink" title="current_timestamp"></a>current_timestamp</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">current_timestamp(): Column</span><br></pre></td></tr></table></figure>

<p>Note : <code>current_timestamp</code> is also <code>now</code> function in SQL. </p>
<h3 id="unix-timestamp-Converting-Current-or-Specified-Time-to-Unix-Timestamp"><a href="#unix-timestamp-Converting-Current-or-Specified-Time-to-Unix-Timestamp" class="headerlink" title="unix_timestamp - Converting Current or Specified Time to Unix Timestamp"></a>unix_timestamp - Converting Current or Specified Time to Unix Timestamp</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">unix_timestamp(): Column  (1)</span><br><span class="line">unix_timestamp(time: Column): Column (2)</span><br><span class="line">unix_timestamp(time: Column, format: String): Column</span><br></pre></td></tr></table></figure>

<ol>
<li>Gives current timestamp (in seconds)</li>
<li>Converts <code>time</code> string in format <code>yyyy-MM-dd HH:mm:ss</code> to Unix timestamp (in seconds)</li>
</ol>
<p><code>unix_timestamp</code> converts the current or specified <code>time</code> in the specified <code>format</code> to a Unix timestamp (in seconds).</p>
<p><code>unix_timestamp</code> supports a column of type <code>Date</code>, <code>Timestamp</code> or <code>String</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; no time and format &#x3D;&gt; current time</span><br><span class="line">scala&gt; spark.range(1).select(unix_timestamp as &quot;current_timestamp&quot;).show</span><br><span class="line">+-----------------+</span><br><span class="line">|current_timestamp|</span><br><span class="line">+-----------------+</span><br><span class="line">|       1493362850|</span><br><span class="line">+-----------------+</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; no format so yyyy-MM-dd HH:mm:ss assumed</span><br><span class="line">scala&gt; Seq(&quot;2017-01-01 00:00:00&quot;).toDF(&quot;time&quot;).withColumn(&quot;unix_timestamp&quot;, unix_timestamp($&quot;time&quot;)).show</span><br><span class="line">+-------------------+--------------+</span><br><span class="line">|               time|unix_timestamp|</span><br><span class="line">+-------------------+--------------+</span><br><span class="line">|2017-01-01 00:00:00|    1483225200|</span><br><span class="line">+-------------------+--------------+</span><br><span class="line"></span><br><span class="line">scala&gt; Seq(&quot;2017&#x2F;01&#x2F;01 00:00:00&quot;).toDF(&quot;time&quot;).withColumn(&quot;unix_timestamp&quot;, unix_timestamp($&quot;time&quot;, &quot;yyyy&#x2F;MM&#x2F;dd&quot;)).show</span><br><span class="line">+-------------------+--------------+</span><br><span class="line">|               time|unix_timestamp|</span><br><span class="line">+-------------------+--------------+</span><br><span class="line">|2017&#x2F;01&#x2F;01 00:00:00|    1483225200|</span><br><span class="line">+-------------------+--------------+</span><br></pre></td></tr></table></figure>

<p><code>unix_timestamp</code> returns <code>null</code> if conversion fails.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; note slashes as date separators</span><br><span class="line">scala&gt; Seq(&quot;2017&#x2F;01&#x2F;01 00:00:00&quot;).toDF(&quot;time&quot;).withColumn(&quot;unix_timestamp&quot;, unix_timestamp($&quot;time&quot;)).show</span><br><span class="line">+-------------------+--------------+</span><br><span class="line">|               time|unix_timestamp|</span><br><span class="line">+-------------------+--------------+</span><br><span class="line">|2017&#x2F;01&#x2F;01 00:00:00|          null|</span><br><span class="line">+-------------------+--------------+</span><br></pre></td></tr></table></figure>

<p>Note:  <code>unix_timestamp</code> is also supported in [SQL mode]</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; spark.sql(&quot;SELECT unix_timestamp() as unix_timestamp&quot;).show </span><br><span class="line">unix_timestamp</span><br><span class="line">1493369225</span><br></pre></td></tr></table></figure>

<p>Internally, <code>unix_timestamp</code> creates a <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Column.html" target="_blank" rel="noopener">Column</a> with <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Expression-UnixTimestamp.html" target="_blank" rel="noopener">UnixTimestamp</a> binary expression (possibly with <code>CurrentTimestamp</code>).</p>
<h3 id="window-Generating-Time-Windows"><a href="#window-Generating-Time-Windows" class="headerlink" title="window - Generating Time Windows "></a>window - Generating Time Windows </h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">window(</span><br><span class="line">  timeColumn: Column,</span><br><span class="line">  windowDuration: String): Column  (1)</span><br><span class="line">window(</span><br><span class="line">  timeColumn: Column,</span><br><span class="line">  windowDuration: String,</span><br><span class="line">  slideDuration: String): Column   (2)</span><br><span class="line">window(</span><br><span class="line">  timeColumn: Column,</span><br><span class="line">  windowDuration: String,</span><br><span class="line">  slideDuration: String,</span><br><span class="line">  startTime: String): Column       (3)</span><br></pre></td></tr></table></figure>

<ol>
<li>Creates a tumbling time window with <code>slideDuration</code> as <code>windowDuration</code> and <code>0 second</code> for <code>startTime</code></li>
<li>Creates a sliding time window with <code>0 second</code> for <code>startTime</code></li>
<li>Creates a delayed time window</li>
</ol>
<p><code>window</code> generates <strong>tumbling</strong>, <strong>sliding</strong> or <strong>delayed</strong> time windows of <code>windowDuration</code> duration given a <code>timeColumn</code> timestamp specifying column.</p>
<p>Note:  From <a href="https://msdn.microsoft.com/en-us/library/azure/dn835055.aspx" target="_blank" rel="noopener">Tumbling Window (Azure Stream Analytics)</a>:<strong>Tumbling windows</strong> are a series of fixed-sized, non-overlapping and contiguous time intervals.                                                       |</p>
<p>Note:  From <a href="https://flink.apache.org/news/2015/12/04/Introducing-windows.html" target="_blank" rel="noopener">Introducing Stream Windows in Apache Flink</a>:<strong>Tumbling windows</strong> group elements of a stream into finite sets where each set corresponds to an interval.<strong>Tumbling windows</strong> discretize a stream into non-overlapping windows. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val timeColumn &#x3D; window(&#39;time, &quot;5 seconds&quot;)</span><br><span class="line">timeColumn: org.apache.spark.sql.Column &#x3D; timewindow(time, 5000000, 5000000, 0) AS &#96;window&#96;</span><br></pre></td></tr></table></figure>

<p><code>timeColumn</code> should be of <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataType.html#TimestampType" target="_blank" rel="noopener">TimestampType</a>, i.e. with <a href="https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html" target="_blank" rel="noopener">java.sql.Timestamp</a> values.</p>
<table>
<thead>
<tr>
<th>Tip</th>
<th>Use <a href="https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#from-java.time.Instant-" target="_blank" rel="noopener">java.sql.Timestamp.from</a> or <a href="https://docs.oracle.com/javase/8/docs/api/java/sql/Timestamp.html#valueOf-java.time.LocalDateTime-" target="_blank" rel="noopener">java.sql.Timestamp.valueOf</a> factory methods to create <code>Timestamp</code> instances.</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;api&#x2F;java&#x2F;time&#x2F;LocalDateTime.html</span><br><span class="line">import java.time.LocalDateTime</span><br><span class="line">&#x2F;&#x2F; https:&#x2F;&#x2F;docs.oracle.com&#x2F;javase&#x2F;8&#x2F;docs&#x2F;api&#x2F;java&#x2F;sql&#x2F;Timestamp.html</span><br><span class="line">import java.sql.Timestamp</span><br><span class="line">val levels &#x3D; Seq(</span><br><span class="line">  &#x2F;&#x2F; (year, month, dayOfMonth, hour, minute, second)</span><br><span class="line">  ((2012, 12, 12, 12, 12, 12), 5),</span><br><span class="line">  ((2012, 12, 12, 12, 12, 14), 9),</span><br><span class="line">  ((2012, 12, 12, 13, 13, 14), 4),</span><br><span class="line">  ((2016, 8,  13, 0, 0, 0), 10),</span><br><span class="line">  ((2017, 5,  27, 0, 0, 0), 15)).</span><br><span class="line">  map &#123; case ((yy, mm, dd, h, m, s), a) &#x3D;&gt; (LocalDateTime.of(yy, mm, dd, h, m, s), a) &#125;.</span><br><span class="line">  map &#123; case (ts, a) &#x3D;&gt; (Timestamp.valueOf(ts), a) &#125;.</span><br><span class="line">  toDF(&quot;time&quot;, &quot;level&quot;)</span><br><span class="line">scala&gt; levels.show</span><br><span class="line">+-------------------+-----+</span><br><span class="line">|               time|level|</span><br><span class="line">+-------------------+-----+</span><br><span class="line">|2012-12-12 12:12:12|    5|</span><br><span class="line">|2012-12-12 12:12:14|    9|</span><br><span class="line">|2012-12-12 13:13:14|    4|</span><br><span class="line">|2016-08-13 00:00:00|   10|</span><br><span class="line">|2017-05-27 00:00:00|   15|</span><br><span class="line">+-------------------+-----+</span><br><span class="line"></span><br><span class="line">val q &#x3D; levels.select(window($&quot;time&quot;, &quot;5 seconds&quot;), $&quot;level&quot;)</span><br><span class="line">scala&gt; q.show(truncate &#x3D; false)</span><br><span class="line">+---------------------------------------------+-----+</span><br><span class="line">|window                                       |level|</span><br><span class="line">+---------------------------------------------+-----+</span><br><span class="line">|[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|5    |</span><br><span class="line">|[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|9    |</span><br><span class="line">|[2012-12-12 13:13:10.0,2012-12-12 13:13:15.0]|4    |</span><br><span class="line">|[2016-08-13 00:00:00.0,2016-08-13 00:00:05.0]|10   |</span><br><span class="line">|[2017-05-27 00:00:00.0,2017-05-27 00:00:05.0]|15   |</span><br><span class="line">+---------------------------------------------+-----+</span><br><span class="line"></span><br><span class="line">scala&gt; q.printSchema</span><br><span class="line">root</span><br><span class="line"> |-- window: struct (nullable &#x3D; true)</span><br><span class="line"> |    |-- start: timestamp (nullable &#x3D; true)</span><br><span class="line"> |    |-- end: timestamp (nullable &#x3D; true)</span><br><span class="line"> |-- level: integer (nullable &#x3D; false)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; calculating the sum of levels every 5 seconds</span><br><span class="line">val sums &#x3D; levels.</span><br><span class="line">  groupBy(window($&quot;time&quot;, &quot;5 seconds&quot;)).</span><br><span class="line">  agg(sum(&quot;level&quot;) as &quot;level_sum&quot;).</span><br><span class="line">  select(&quot;window.start&quot;, &quot;window.end&quot;, &quot;level_sum&quot;)</span><br><span class="line">scala&gt; sums.show</span><br><span class="line">+-------------------+-------------------+---------+</span><br><span class="line">|              start|                end|level_sum|</span><br><span class="line">+-------------------+-------------------+---------+</span><br><span class="line">|2012-12-12 13:13:10|2012-12-12 13:13:15|        4|</span><br><span class="line">|2012-12-12 12:12:10|2012-12-12 12:12:15|       14|</span><br><span class="line">|2016-08-13 00:00:00|2016-08-13 00:00:05|       10|</span><br><span class="line">|2017-05-27 00:00:00|2017-05-27 00:00:05|       15|</span><br><span class="line">+-------------------+-------------------+---------+</span><br></pre></td></tr></table></figure>

<p><code>windowDuration</code> and <code>slideDuration</code> are strings specifying the width of the window for duration and sliding identifiers, respectively.</p>
<p>Use <code>CalendarInterval</code> for valid window identifiers.</p>
<p><code>window</code> is available as of Spark <strong>2.0.0</strong>. </p>
<p>Internally, <code>window</code> creates a <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Column.html" target="_blank" rel="noopener">Column</a> (with <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Expression-TimeWindow.html" target="_blank" rel="noopener">TimeWindow</a> expression) available as <code>window</code> alias.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; q is the query defined earlier</span><br><span class="line">scala&gt; q.show(truncate &#x3D; false)</span><br><span class="line">+---------------------------------------------+-----+</span><br><span class="line">|window                                       |level|</span><br><span class="line">+---------------------------------------------+-----+</span><br><span class="line">|[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|5    |</span><br><span class="line">|[2012-12-12 12:12:10.0,2012-12-12 12:12:15.0]|9    |</span><br><span class="line">|[2012-12-12 13:13:10.0,2012-12-12 13:13:15.0]|4    |</span><br><span class="line">|[2016-08-13 00:00:00.0,2016-08-13 00:00:05.0]|10   |</span><br><span class="line">|[2017-05-27 00:00:00.0,2017-05-27 00:00:05.0]|15   |</span><br><span class="line">+---------------------------------------------+-----+</span><br><span class="line"></span><br><span class="line">scala&gt; println(timeColumn.expr.numberedTreeString)</span><br><span class="line">00 timewindow(&#39;time, 5000000, 5000000, 0) AS window#22</span><br><span class="line">01 +- timewindow(&#39;time, 5000000, 5000000, 0)</span><br><span class="line">02    +- &#39;time</span><br></pre></td></tr></table></figure>

<h4 id="Example-—-Traffic-Sensor"><a href="#Example-—-Traffic-Sensor" class="headerlink" title="Example — Traffic Sensor"></a>Example — Traffic Sensor</h4><p>The example is borrowed from <a href="https://flink.apache.org/news/2015/12/04/Introducing-windows.html" target="_blank" rel="noopener">Introducing Stream Windows in Apache Flink</a>. </p>
<p>The example shows how to use <code>window</code> function to model a traffic sensor that counts every 15 seconds the number of vehicles passing a certain location.</p>
<h3 id="to-date-—-Converting-Column-To-DateType"><a href="#to-date-—-Converting-Column-To-DateType" class="headerlink" title="to_date — Converting Column To DateType "></a>to_date — Converting Column To DateType </h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">to_date(e: Column): Column</span><br><span class="line">to_date(e: Column, fmt: String): Column</span><br></pre></td></tr></table></figure>

<p><code>to_date</code> converts the column into <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataType.html#DateType" target="_blank" rel="noopener">DateType</a> (by casting to <code>DateType</code>).</p>
<p><code>fmt</code> follows <a href="http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html" target="_blank" rel="noopener">the formatting styles</a>.</p>
<p>Internally, <code>to_date</code> creates a <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Column.html#creating-instance" target="_blank" rel="noopener">Column</a> with <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Expression-ParseToDate.html" target="_blank" rel="noopener">ParseToDate</a> expression (and <code>Literal</code> expression for <code>fmt</code>).</p>
<p>Use <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Expression-ParseToDate.html" target="_blank" rel="noopener">ParseToDate</a> expression to use a column for the values of <code>fmt</code>. </p>
<h3 id="to-timestamp-—-Converting-Column-To-TimestampType"><a href="#to-timestamp-—-Converting-Column-To-TimestampType" class="headerlink" title="to_timestamp — Converting Column To TimestampType "></a>to_timestamp — Converting Column To TimestampType </h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">to_timestamp(s: Column): Column</span><br><span class="line">to_timestamp(s: Column, fmt: String): Column</span><br></pre></td></tr></table></figure>

<p><code>to_timestamp</code> converts the column into <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-DataType.html#TimestampType" target="_blank" rel="noopener">TimestampType</a> (by casting to <code>TimestampType</code>).</p>
<p><code>fmt</code> follows <a href="http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html" target="_blank" rel="noopener">the formatting styles</a>. </p>
<p>Internally, <code>to_timestamp</code> creates a <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Column.html#creating-instance" target="_blank" rel="noopener">Column</a> with <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Expression-ParseToTimestamp.html" target="_blank" rel="noopener">ParseToTimestamp</a> expression (and <code>Literal</code>expression for <code>fmt</code>).</p>
<p>Use <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Expression-ParseToTimestamp.html" target="_blank" rel="noopener">ParseToTimestamp</a> expression to use a column for the values of <code>fmt</code>. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark%E9%97%AE%E9%A2%98-rdd%E5%88%86%E5%8C%BA2GB%E9%99%90%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark%E9%97%AE%E9%A2%98-rdd%E5%88%86%E5%8C%BA2GB%E9%99%90%E5%88%B6/" class="post-title-link" itemprop="url">spark问题-rdd分区2GB限制</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/" itemprop="url" rel="index">
                    <span itemprop="name">数据处理实践</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark%E9%97%AE%E9%A2%98-rdd%E5%88%86%E5%8C%BA2GB%E9%99%90%E5%88%B6/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/数据处理实践/spark问题-rdd分区2GB限制/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>spark在处理较大数据时，会遇到 Shuffle block 小于 2GB 的限制。一旦 Shuffle block 大于2GB，就会出现<code>Size exceeds Integer.MAX_VALUE</code>异常</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark%E9%97%AE%E9%A2%98-rdd%E5%88%86%E5%8C%BA2GB%E9%99%90%E5%88%B6/#more" rel="contents">
                阅读全文 &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E5%9F%BA%E7%A1%80%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E5%9F%BA%E7%A1%80%E7%AF%87/" class="post-title-link" itemprop="url">Spark性能优化指南——基础篇（转）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/" itemprop="url" rel="index">
                    <span itemprop="name">数据处理实践</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96-%E5%9F%BA%E7%A1%80%E7%AF%87/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/数据处理实践/spark性能优化-基础篇/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Spark性能优化指南——基础篇（转）"><a href="#Spark性能优化指南——基础篇（转）" class="headerlink" title="Spark性能优化指南——基础篇（转）"></a>Spark性能优化指南——基础篇（转）</h1><p>来源: <a href="https://tech.meituan.com/spark-tuning-basic.html" target="_blank" rel="noopener">Spark性能优化指南——基础篇</a></p>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在大数据计算领域，Spark已经成为了越来越流行、越来越受欢迎的计算平台之一。Spark的功能涵盖了大数据领域的离线批处理、SQL类处理、流式/实时计算、机器学习、图计算等各种不同类型的计算操作，应用范围与前景非常广泛。在美团•大众点评，已经有很多同学在各种项目中尝试使用Spark。大多数同学（包括笔者在内），最初开始尝试使用Spark的原因很简单，主要就是为了让大数据计算作业的执行速度更快、性能更高。</p>
<p>然而，通过Spark开发出高性能的大数据计算作业，并不是那么简单的。如果没有对Spark作业进行合理的调优，Spark作业的执行速度可能会很慢，这样就完全体现不出Spark作为一种快速大数据计算引擎的优势来。因此，想要用好Spark，就必须对其进行合理的性能优化。</p>
<p>Spark的性能调优实际上是由很多部分组成的，不是调节几个参数就可以立竿见影提升作业性能的。我们需要根据不同的业务场景以及数据情况，对Spark作业进行综合性的分析，然后进行多个方面的调节和优化，才能获得最佳性能。</p>
<p>笔者根据之前的Spark作业开发经验以及实践积累，总结出了一套Spark作业的性能优化方案。整套方案主要分为开发调优、资源调优、数据倾斜调优、shuffle调优几个部分。开发调优和资源调优是所有Spark作业都需要注意和遵循的一些基本原则，是高性能Spark作业的基础；数据倾斜调优，主要讲解了一套完整的用来解决Spark作业数据倾斜的解决方案；shuffle调优，面向的是对Spark的原理有较深层次掌握和研究的同学，主要讲解了如何对Spark作业的shuffle运行过程以及细节进行调优。</p>
<p>本文作为Spark性能优化指南的基础篇，主要讲解开发调优以及资源调优。</p>
<h1 id="开发调优"><a href="#开发调优" class="headerlink" title="开发调优"></a>开发调优</h1><h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p>
<h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p>通常来说，我们在开发一个Spark作业时，首先是基于某个数据源（比如Hive表或HDFS文件）创建一个初始的RDD；接着对这个RDD执行某个算子操作，然后得到下一个RDD；以此类推，循环往复，直到计算出最终我们需要的结果。在这个过程中，多个RDD会通过不同的算子操作（比如map、reduce等）串起来，这个“RDD串”，就是RDD lineage，也就是“RDD的血缘关系链”。</p>
<p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。</p>
<p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p>
<h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></span><br><span class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span></span><br><span class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></span><br><span class="line"><span class="comment">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span></span><br><span class="line"><span class="comment">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span></span><br><span class="line"><span class="comment">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure>

<h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p>
<h3 id="一个简单的例子-1"><a href="#一个简单的例子-1" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 错误的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span></span><br><span class="line"><span class="comment">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span></span><br><span class="line"><span class="comment">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 其实在这种情况下完全可以复用同一个RDD。</span></span><br><span class="line"><span class="comment">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span></span><br><span class="line"><span class="comment">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span></span><br><span class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span></span><br><span class="line"><span class="comment">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span></span><br><span class="line"><span class="comment">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span></span><br></pre></td></tr></table></figure>

<h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p>
<p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p>
<p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p>
<h3 id="对多次使用的RDD进行持久化的代码示例"><a href="#对多次使用的RDD进行持久化的代码示例" class="headerlink" title="对多次使用的RDD进行持久化的代码示例"></a>对多次使用的RDD进行持久化的代码示例</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure>

<p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p>
<h3 id="Spark的持久化级别"><a href="#Spark的持久化级别" class="headerlink" title="Spark的持久化级别"></a>Spark的持久化级别</h3><table>
<thead>
<tr>
<th>持久化级别</th>
<th>含义解释</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td>
</tr>
<tr>
<td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td>
<td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td>
</tr>
</tbody></table>
<h3 id="如何选择一种最合适的持久化策略"><a href="#如何选择一种最合适的持久化策略" class="headerlink" title="如何选择一种最合适的持久化策略"></a>如何选择一种最合适的持久化策略</h3><ul>
<li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li>
<li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li>
<li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li>
<li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li>
</ul>
<h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作。比如reduceByKey、join等算子，都会触发shuffle操作。</p>
<p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p>
<p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p>
<h3 id="Broadcast与map进行join代码示例"><a href="#Broadcast与map进行join代码示例" class="headerlink" title="Broadcast与map进行join代码示例"></a>Broadcast与map进行join代码示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 传统的join操作会导致shuffle操作。</span><br><span class="line">&#x2F;&#x2F; 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span><br><span class="line">val rdd3 &#x3D; rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Broadcast+map的join操作，不会导致shuffle操作。</span><br><span class="line">&#x2F;&#x2F; 使用Broadcast将一个数据量较小的RDD作为广播变量。</span><br><span class="line">val rdd2Data &#x3D; rdd2.collect()</span><br><span class="line">val rdd2DataBroadcast &#x3D; sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span><br><span class="line">&#x2F;&#x2F; 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span><br><span class="line">&#x2F;&#x2F; 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span><br><span class="line">val rdd3 &#x3D; rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span><br><span class="line">&#x2F;&#x2F; 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span><br></pre></td></tr></table></figure>

<h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p>
<p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p>
<p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p>
<p><img src="https://tech.meituan.com/img/spark-tuning/group-by-key-wordcount.png" alt="groupByKey实现wordcount原理"></p>
<p><img src="https://tech.meituan.com/img/spark-tuning/reduce-by-key-wordcount.png" alt="reduceByKey实现wordcount原理"></p>
<h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p>
<h3 id="使用reduceByKey-aggregateByKey替代groupByKey"><a href="#使用reduceByKey-aggregateByKey替代groupByKey" class="headerlink" title="使用reduceByKey/aggregateByKey替代groupByKey"></a>使用reduceByKey/aggregateByKey替代groupByKey</h3><p>详情见“原则五：使用map-side预聚合的shuffle操作”。</p>
<h3 id="使用mapPartitions替代普通map"><a href="#使用mapPartitions替代普通map" class="headerlink" title="使用mapPartitions替代普通map"></a>使用mapPartitions替代普通map</h3><p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p>
<h3 id="使用foreachPartitions替代foreach"><a href="#使用foreachPartitions替代foreach" class="headerlink" title="使用foreachPartitions替代foreach"></a>使用foreachPartitions替代foreach</h3><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p>
<h3 id="使用filter之后进行coalesce操作"><a href="#使用filter之后进行coalesce操作" class="headerlink" title="使用filter之后进行coalesce操作"></a>使用filter之后进行coalesce操作</h3><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p>
<h3 id="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"><a href="#使用repartitionAndSortWithinPartitions替代repartition与sort类操作" class="headerlink" title="使用repartitionAndSortWithinPartitions替代repartition与sort类操作"></a>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</h3><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p>
<h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p>
<p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p>
<p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p>
<h3 id="广播大变量的代码示例"><a href="#广播大变量的代码示例" class="headerlink" title="广播大变量的代码示例"></a>广播大变量的代码示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 以下代码在算子函数中，使用了外部的变量。</span><br><span class="line">&#x2F;&#x2F; 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span><br><span class="line">val list1 &#x3D; ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 以下代码将list1封装成了Broadcast类型的广播变量。</span><br><span class="line">&#x2F;&#x2F; 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span><br><span class="line">&#x2F;&#x2F; 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span><br><span class="line">&#x2F;&#x2F; 每个Executor内存中，就只会驻留一份广播变量副本。</span><br><span class="line">val list1 &#x3D; ...</span><br><span class="line">val list1Broadcast &#x3D; sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure>

<h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化：</p>
<ul>
<li>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</li>
<li>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</li>
<li>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</li>
</ul>
<p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p>
<p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 创建SparkConf对象。</span><br><span class="line">val conf &#x3D; new SparkConf().setMaster(...).setAppName(...)</span><br><span class="line">&#x2F;&#x2F; 设置序列化器为KryoSerializer。</span><br><span class="line">conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)</span><br><span class="line">&#x2F;&#x2F; 注册要序列化的自定义类型。</span><br><span class="line">conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))</span><br></pre></td></tr></table></figure>

<h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存：</p>
<ul>
<li>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</li>
<li>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</li>
<li>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</li>
</ul>
<p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p>
<p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p>
<h1 id="资源调优"><a href="#资源调优" class="headerlink" title="资源调优"></a>资源调优</h1><h2 id="调优概述-1"><a href="#调优概述-1" class="headerlink" title="调优概述"></a>调优概述</h2><p>在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p>
<h2 id="Spark作业基本运行原理"><a href="#Spark作业基本运行原理" class="headerlink" title="Spark作业基本运行原理"></a>Spark作业基本运行原理</h2><p><img src="https://tech.meituan.com/img/spark-tuning/spark-base-mech.png" alt="Spark基本运行原理"></p>
<p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p>
<p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p>
<p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p>
<p>当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p>
<p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p>
<p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p>
<p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p>
<h2 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a>资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p>
<h3 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h3><ul>
<li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li>
<li>参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li>
</ul>
<h3 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h3><ul>
<li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li>
<li>参数调优建议：每个Executor进程的内存设置4G<del>8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3</del>1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li>
</ul>
<h3 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h3><ul>
<li>参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li>
<li>参数调优建议：Executor的CPU core数量设置为2<del>4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3</del>1/2左右比较合适，也是避免影响其他同学的作业运行。</li>
</ul>
<h3 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h3><ul>
<li>参数说明：该参数用于设置Driver进程的内存。</li>
<li>参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</li>
</ul>
<h3 id="spark-default-parallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a>spark.default.parallelism</h3><ul>
<li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</li>
<li>参数调优建议：Spark作业的默认task数量为500<del>1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2</del>3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li>
</ul>
<h3 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.storage.memoryFraction</h3><ul>
<li>参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li>
<li>参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul>
<li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li>
<li>参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li>
</ul>
<p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p>
<h2 id="资源参数参考示例"><a href="#资源参数参考示例" class="headerlink" title="资源参数参考示例"></a>资源参数参考示例</h2><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;bin&#x2F;spark-submit \</span><br><span class="line">  --master yarn-cluster \</span><br><span class="line">  --num-executors 100 \</span><br><span class="line">  --executor-memory 6G \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --driver-memory 1G \</span><br><span class="line">  --conf spark.default.parallelism&#x3D;1000 \</span><br><span class="line">  --conf spark.storage.memoryFraction&#x3D;0.5 \</span><br><span class="line">  --conf spark.shuffle.memoryFraction&#x3D;0.3 \</span><br></pre></td></tr></table></figure>

<h1 id="写在最后的话"><a href="#写在最后的话" class="headerlink" title="写在最后的话"></a>写在最后的话</h1><p>根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。在后续的《Spark性能优化指南——高级篇》中，我们会详细讲解数据倾斜调优以及Shuffle调优。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/" class="post-title-link" itemprop="url">数据倾斜（转）</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/" itemprop="url" rel="index">
                    <span itemprop="name">数据处理实践</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/数据处理实践/数据倾斜/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据倾斜（转）"><a href="#数据倾斜（转）" class="headerlink" title="数据倾斜（转）"></a>数据倾斜（转）</h1><p>[TOC]</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文结合实例详细阐明了Spark数据倾斜的几种场景以及对应的解决方案，包括避免数据源倾斜，调整并行度，使用自定义Partitioner，使用Map侧Join代替Reduce侧Join，给倾斜Key加上随机前缀等。</p>
<h2 id="一、为何要处理数据倾斜（Data-Skew）"><a href="#一、为何要处理数据倾斜（Data-Skew）" class="headerlink" title="一、为何要处理数据倾斜（Data Skew）"></a>一、为何要处理数据倾斜（Data Skew）</h2><h3 id="1-1-什么是数据倾斜"><a href="#1-1-什么是数据倾斜" class="headerlink" title="1.1 什么是数据倾斜"></a>1.1 什么是数据倾斜</h3><p>何谓数据倾斜？数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。</p>
<p>对于分布式系统而言，理想情况下，随着系统规模（节点数量）的增加，应用整体耗时线性下降。如果一台机器处理一批大量数据需要120分钟，当机器数量增加到三时，理想的耗时为120 / 3 = 40分钟，如下图所示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">单机处理    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;</span><br><span class="line"></span><br><span class="line">分布处理    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;</span><br><span class="line">分布处理	   &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;</span><br><span class="line">分布处理    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;</span><br><span class="line">           -----------------------------</span><br><span class="line">           处理时间</span><br></pre></td></tr></table></figure>

<p>但是，上述情况只是理想情况，实际上将单机任务转换成分布式任务后，会有overhead，使得总的任务量较之单机时有所增加，所以每台机器的执行时间加起来比单台机器时更大。这里暂不考虑这些overhead，假设单机任务转换成分布式任务后，总任务量不变。<br>但即使如此，想做到分布式情况下每台机器执行时间是单机时的<code>1 / N</code>，就必须保证每台机器的任务量相等。不幸的是，很多时候，任务的分配是不均匀的，甚至不均匀到大部分任务被分配到个别机器上，其它大部分机器所分配的任务量只占总得的小部分。比如一台机器负责处理80%的任务，另外两台机器各处理10%的任务，如下图所示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">单机处理    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;</span><br><span class="line"></span><br><span class="line">分布处理    &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;</span><br><span class="line">分布处理	  &#x3D;&#x3D;&#x3D;&#x3D;&gt;</span><br><span class="line">分布处理    &#x3D;&#x3D;&#x3D;&#x3D;&gt;</span><br><span class="line">           -----------------------------</span><br><span class="line">           处理时间</span><br></pre></td></tr></table></figure>

<p>在上图中，机器数据增加为三倍，但执行时间只降为原来的80%，远低于理想值。 　　</p>
<h3 id="1-2-数据倾斜的危害"><a href="#1-2-数据倾斜的危害" class="headerlink" title="1.2 数据倾斜的危害"></a>1.2 数据倾斜的危害</h3><p>当出现数据倾斜时，个别任务耗时远高于其它任务，从而使得整体耗时过大，未能充分发挥分布式系统的并行计算优势。<br>另外，当发生数据倾斜时，部分任务处理的数据量过大，可能造成内存不足使得任务失败，并进而引进整个应用失败。 　　</p>
<h3 id="1-3-数据倾斜是如何造成的"><a href="#1-3-数据倾斜是如何造成的" class="headerlink" title="1.3 数据倾斜是如何造成的"></a>1.3 数据倾斜是如何造成的</h3><p>在Spark中，同一个Stage的不同Partition可以并行处理，而具有依赖关系的不同Stage之间是串行处理的。假设某个Spark Job分为Stage 0和Stage 1两个Stage，且Stage 1依赖于Stage 0，那Stage 0完全处理结束之前不会处理Stage 1。而Stage 0可能包含N个Task，这N个Task可以并行进行。如果其中N-1个Task都在10秒内完成，而另外一个Task却耗时1分钟，那该Stage的总时间至少为1分钟。换句话说，一个Stage所耗费的时间，主要由最慢的那个Task决定。</p>
<p>由于同一个Stage内的所有Task执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同Task之间耗时的差异主要由该Task所处理的数据量决定。</p>
<p>Stage的数据来源主要分为如下两类</p>
<ul>
<li>从数据源直接读取。如读取HDFS，Kafka</li>
<li>读取上一个Stage的Shuffle数据</li>
</ul>
<h2 id="二、如何缓解-消除数据倾斜"><a href="#二、如何缓解-消除数据倾斜" class="headerlink" title="二、如何缓解/消除数据倾斜"></a>二、如何缓解/消除数据倾斜</h2><h3 id="2-1-避免数据源的数据倾斜-—-读Kafka"><a href="#2-1-避免数据源的数据倾斜-—-读Kafka" class="headerlink" title="2.1 避免数据源的数据倾斜 — 读Kafka"></a>2.1 避免数据源的数据倾斜 — 读Kafka</h3><p>以Spark Stream通过DirectStream方式读取Kafka数据为例。由于Kafka的每一个Partition对应Spark的一个Task（Partition），所以Kafka内相关Topic的各Partition之间数据是否平衡，直接决定Spark处理该数据时是否会产生数据倾斜。</p>
<p>如《<a href="http://www.jasongj.com/2015/03/10/KafkaColumn1/#Producer%E6%B6%88%E6%81%AF%E8%B7%AF%E7%94%B1" target="_blank" rel="noopener">Kafka设计解析（一）- Kafka背景及架构介绍</a>》一文所述，Kafka某一Topic内消息在不同Partition之间的分布，主要由Producer端所使用的Partition实现类决定。如果使用随机Partitioner，则每条消息会随机发送到一个Partition中，从而从概率上来讲，各Partition间的数据会达到平衡。此时源Stage（直接读取Kafka数据的Stage）不会产生数据倾斜。</p>
<p>但很多时候，业务场景可能会要求将具备同一特征的数据顺序消费，此时就需要将具有相同特征的数据放于同一个Partition中。一个典型的场景是，需要将同一个用户相关的PV信息置于同一个Partition中。此时，如果产生了数据倾斜，则需要通过其它方式处理。</p>
<h3 id="2-2-避免数据源的数据倾斜-—-读文件"><a href="#2-2-避免数据源的数据倾斜-—-读文件" class="headerlink" title="2.2 避免数据源的数据倾斜 — 读文件"></a>2.2 避免数据源的数据倾斜 — 读文件</h3><h4 id="2-2-1-原理"><a href="#2-2-1-原理" class="headerlink" title="2.2.1 原理"></a>2.2.1 原理</h4><p>Spark以通过<code>textFile(path, minPartitions)</code>方法读取文件时，使用TextFileFormat。</p>
<p>对于不可切分的文件，每个文件对应一个Split从而对应一个Partition。此时各文件大小是否一致，很大程度上决定了是否存在数据源侧的数据倾斜。另外，对于不可切分的压缩文件，即使压缩后的文件大小一致，它所包含的实际数据量也可能差别很多，因为源文件数据重复度越高，压缩比越高。反过来，即使压缩文件大小接近，但由于压缩比可能差距很大，所需处理的数据量差距也可能很大。</p>
<p>此时可通过在数据生成端将不可切分文件存储为可切分文件，或者保证各文件包含数据量相同的方式避免数据倾斜。</p>
<p>对于可切分的文件，每个Split大小由如下算法决定。其中goalSize等于所有文件总大小除以minPartitions。而blockSize，如果是HDFS文件，由文件本身的block大小决定；如果是Linux本地文件，且使用本地模式，由<code>fs.local.block.size</code>决定。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">protected long computeSplitSize(long goalSize, long minSize, long blockSize) &#123;</span><br><span class="line">    return Math.max(minSize, Math.min(goalSize, blockSize));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>默认情况下各Split的大小不会太大，一般相当于一个Block大小（在Hadoop 2中，默认值为128MB），所以数据倾斜问题不明显。如果出现了严重的数据倾斜，可通过上述参数调整。</p>
<h4 id="2-2-2-案例"><a href="#2-2-2-案例" class="headerlink" title="2.2.2 案例"></a>2.2.2 案例</h4><p>现通过脚本生成一些文本文件，并通过如下代码进行简单的单词计数。为避免Shuffle，只计单词总个数，不须对单词进行分组计数。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SparkConf sparkConf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"ReadFileSkewDemo"</span>);</span><br><span class="line">JavaSparkContext javaSparkContext = <span class="keyword">new</span> JavaSparkContext(sparkConf);</span><br><span class="line"><span class="keyword">long</span> count = javaSparkContext.textFile(inputFile, minPartitions)</span><br><span class="line">    .flatMap((String line) -&gt; Arrays.asList(line.split(<span class="string">" "</span>)).iterator()).count();</span><br><span class="line">System.out.printf(<span class="string">"total words : %s"</span>, count);</span><br><span class="line">javaSparkContext.stop();</span><br></pre></td></tr></table></figure>

<p>总共生成如下11个csv文件，其中10个大小均为271.9MB，另外一个大小为8.5GB。</p>
<p><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/uncompressedfiles.png" alt="uncompressedfiles"></p>
<p>之后将8.5GB大小的文件使用gzip压缩，压缩后大小仅为25.3MB。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/compressedfiles.png" alt="compressedfiles"></p>
<p>使用如上代码对未压缩文件夹进行单词计数操作。Split大小为 max(minSize, min(goalSize, blockSize) = max(1 B, min((271.9*10+8.5 *1024) / 1 MB, 128 MB) = 128MB。无明显数据倾斜。</p>
<p><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/splitable_unskewed.png" alt="splitable_unskewed"></p>
<p>使用同样代码对包含压缩文件的文件夹进行同样的单词计数操作。未压缩文件的Split大小仍然为128MB，而压缩文件（gzip压缩）由于不可切分，且大小仅为25.3MB，因此该文件作为一个单独的Split/Partition。虽然该文件相对较小，但是它由8.5GB文件压缩而来，包含数据量是其它未压缩文件的32倍，因此处理该Split/Partition/文件的Task耗时为4.4分钟，远高于其它Task的10秒。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/compressedfileskew.png" alt="compressedfileskew"></p>
<p>由于上述gzip压缩文件大小为25.3MB，小于128MB的Split大小，不能证明gzip压缩文件不可切分。现将minPartitions从默认的1设置为229，从而目标Split大小为max(minSize, min(goalSize, blockSize) = max(1 B, min((271.9 * 10+25.3) / 229 MB, 128 MB) = 12 MB。如果gzip压缩文件可切分，则所有Split/Partition大小都不会远大于12。反之，如果仍然存在25.3MB的Partition，则说明gzip压缩文件确实不可切分，在生成不可切分文件时需要如上文所述保证各文件数量大大致相同。</p>
<p>如下图所示，gzip压缩文件对应的Split/Partition大小为25.3MB，其它Split大小均为12MB左右。而该Task耗时4.7分钟，远大于其它Task的4秒。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/unsplitable_skew.png" alt="unsplitable_skew"></p>
<h4 id="2-2-3总结"><a href="#2-2-3总结" class="headerlink" title="2.2.3总结"></a>2.2.3总结</h4><p>*<em>适用场景 *</em><br>数据源侧存在不可切分文件，且文件内包含的数据量相差较大。</p>
<p>*<em>解决方案 *</em><br>尽量使用可切分的格式代替不可切分的格式，或者保证各文件实际包含数据量大致相同。</p>
<p>*<em>优势 *</em><br>可撤底消除数据源侧数据倾斜，效果显著。</p>
<p>*<em>劣势 *</em><br>数据源一般来源于外部系统，需要外部系统的支持。</p>
<h3 id="2-3-调整并行度分散同一个Task的不同Key"><a href="#2-3-调整并行度分散同一个Task的不同Key" class="headerlink" title="2.3 调整并行度分散同一个Task的不同Key"></a>2.3 调整并行度分散同一个Task的不同Key</h3><h4 id="2-3-1-原理"><a href="#2-3-1-原理" class="headerlink" title="2.3.1 原理"></a>2.3.1 原理</h4><p>Spark在做Shuffle时，默认使用HashPartitioner（非Hash Shuffle）对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的Key对应的数据被分配到了同一个Task上，造成该Task所处理的数据远大于其它Task，从而造成数据倾斜。</p>
<p>如果调整Shuffle时的并行度，使得原本被分配到同一Task的不同Key发配到不同Task上处理，则可降低原Task所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/changeparallelism.png" alt="changeparallelism"></p>
<h4 id="2-3-2-案例"><a href="#2-3-2-案例" class="headerlink" title="2.3.2 案例"></a>2.3.2 案例</h4><p>现有一张测试表，名为student_external，内有10.5亿条数据，每条数据有一个唯一的id值。现从中取出id取值为9亿到10.5亿的共1.5亿条数据，并通过一些处理，使得id为9亿到9.4亿间的所有数据对12取模后余数为8（即在Shuffle并行度为12时该数据集全部被HashPartition分配到第8个Task），其它数据集对其id除以100取整，从而使得id大于9.4亿的数据在Shuffle时可被均匀分配到所有Task中，而id小于9.4亿的数据全部分配到同一个Task中。处理过程如下</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> <span class="keyword">test</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">CASE</span> <span class="keyword">WHEN</span> <span class="keyword">id</span> &lt; <span class="number">940000000</span> <span class="keyword">THEN</span> (<span class="number">9500000</span>  + (<span class="keyword">CAST</span> (<span class="keyword">RAND</span>() * <span class="number">8</span> <span class="keyword">AS</span> <span class="built_in">INTEGER</span>)) * <span class="number">12</span> )</span><br><span class="line">       <span class="keyword">ELSE</span> <span class="keyword">CAST</span>(<span class="keyword">id</span>/<span class="number">100</span> <span class="keyword">AS</span> <span class="built_in">INTEGER</span>)</span><br><span class="line">       <span class="keyword">END</span>,</span><br><span class="line">       <span class="keyword">name</span></span><br><span class="line"><span class="keyword">FROM</span> student_external</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">id</span> <span class="keyword">BETWEEN</span> <span class="number">900000000</span> <span class="keyword">AND</span> <span class="number">1050000000</span>;</span><br></pre></td></tr></table></figure>

<p>通过上述处理，一份可能造成后续数据倾斜的测试数据即以准备好。接下来，使用Spark读取该测试数据，并通过<code>groupByKey(12)</code>对id分组处理，且Shuffle并行度为12。代码如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkDataSkew</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    SparkSession sparkSession = SparkSession.builder()</span><br><span class="line">      .appName(<span class="string">"SparkDataSkewTunning"</span>)</span><br><span class="line">      .config(<span class="string">"hive.metastore.uris"</span>, <span class="string">"thrift://hadoop1:9083"</span>)</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .getOrCreate();</span><br><span class="line"></span><br><span class="line">    Dataset&lt;Row&gt; dataframe = sparkSession.sql( <span class="string">"select * from test"</span>);</span><br><span class="line">    dataframe.toJavaRDD()</span><br><span class="line">      .mapToPair((Row row) -&gt; <span class="keyword">new</span> Tuple2&lt;Integer, String&gt;(row.getInt(<span class="number">0</span>),row.getString(<span class="number">1</span>)))</span><br><span class="line">      .groupByKey(<span class="number">12</span>)</span><br><span class="line">      .mapToPair((Tuple2&lt;Integer, Iterable&lt;String&gt;&gt; tuple) -&gt; &#123;</span><br><span class="line">        <span class="keyword">int</span> id = tuple._1();</span><br><span class="line">        AtomicInteger atomicInteger = <span class="keyword">new</span> AtomicInteger(<span class="number">0</span>);</span><br><span class="line">        tuple._2().forEach((String name) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;Integer, Integer&gt;(id, atomicInteger.get());</span><br><span class="line">      &#125;).count();</span><br><span class="line"></span><br><span class="line">      sparkSession.stop();</span><br><span class="line">      sparkSession.close();</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>本次实验所使用集群节点数为4，每个节点可被Yarn使用的CPU核数为16，内存为16GB。使用如下方式提交上述应用，将启动4个Executor，每个Executor可使用核数为12（该配置并非生产环境下的最优配置，仅用于本文实验），可用内存为12GB。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --queue ambari --num-executors 4 --executor-cores 12 --executor-memory 12g --class com.jasongj.spark.driver.SparkDataSkew --master yarn --deploy-mode client SparkExample-with-dependencies-1.0.jar</span><br></pre></td></tr></table></figure>

<p>GroupBy Stage的Task状态如下图所示，Task 8处理的记录数为4500万，远大于（9倍于）其它11个Task处理的500万记录。而Task 8所耗费的时间为38秒，远高于其它11个Task的平均时间（16秒）。整个Stage的时间也为38秒，该时间主要由最慢的Task 8决定。</p>
<p><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/differentkeyskew12.png" alt="differentkeyskew12"></p>
<p>在这种情况下，可以通过调整Shuffle并行度，使得原来被分配到同一个Task（即该例中的Task 8）的不同Key分配到不同Task，从而降低Task 8所需处理的数据量，缓解数据倾斜。</p>
<p>通过<code>groupByKey(48)</code>将Shuffle并行度调整为48，重新提交到Spark。新的Job的GroupBy Stage所有Task状态如下图所示。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/differentkeyskew48.png" alt="differentkeyskew48"></p>
<p>从上图可知，记录数最多的Task 20处理的记录数约为1125万，相比于并行度为12时Task 8的4500万，降低了75%左右，而其耗时从原来Task 8的38秒降到了24秒。</p>
<p>在这种场景下，调整并行度，并不意味着一定要增加并行度，也可能是减小并行度。如果通过<code>groupByKey(11)</code>将Shuffle并行度调整为11，重新提交到Spark。新Job的GroupBy Stage的所有Task状态如下图所示。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/differentkeyskew11.png" alt="differentkeyskew11"></p>
<p>从上图可见，处理记录数最多的Task 6所处理的记录数约为1045万，耗时为23秒。处理记录数最少的Task 1处理的记录数约为545万，耗时12秒。</p>
<h4 id="2-3-3总结"><a href="#2-3-3总结" class="headerlink" title="2.3.3总结"></a>2.3.3总结</h4><p><strong>适用场景</strong><br>大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p>
<p><strong>解决方案</strong><br>调整并行度。一般是增大并行度，但有时如本例减小并行度也可达到效果。</p>
<p><strong>优势</strong><br>实现简单，可在需要Shuffle的操作算子上直接设置并行度或者使用<code>spark.default.parallelism</code>设置。如果是Spark SQL，还可通过<code>SET spark.sql.shuffle.partitions=[num_tasks]</code>设置并行度。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。</p>
<p><strong>劣势</strong><br>适用场景少，只能将分配到同一Task的不同Key分散开，但对于同一Key倾斜严重的情况该方法并不适用。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般。</p>
<h3 id="2-4-自定义Partitioner"><a href="#2-4-自定义Partitioner" class="headerlink" title="2.4 自定义Partitioner"></a>2.4 自定义Partitioner</h3><h4 id="2-4-1-原理"><a href="#2-4-1-原理" class="headerlink" title="2.4.1 原理"></a>2.4.1 原理</h4><p>使用自定义的Partitioner（默认为HashPartitioner），将原本被分配到同一个Task的不同Key分配到不同Task。</p>
<h4 id="2-4-2-案例"><a href="#2-4-2-案例" class="headerlink" title="2.4.2 案例"></a>2.4.2 案例</h4><p>以上述数据集为例，继续将并发度设置为12，但是在<code>groupByKey</code>算子上，使用自定义的<code>Partitioner</code>（实现如下）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">.groupByKey(<span class="keyword">new</span> Partitioner() &#123;</span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numPartitions</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">12</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> id = Integer.parseInt(key.toString());</span><br><span class="line">    <span class="keyword">if</span>(id &gt;= <span class="number">9500000</span> &amp;&amp; id &lt;= <span class="number">9500084</span> &amp;&amp; ((id - <span class="number">9500000</span>) % <span class="number">12</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> (id - <span class="number">9500000</span>) / <span class="number">12</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> id % <span class="number">12</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>由下图可见，使用自定义Partition后，耗时最长的Task 6处理约1000万条数据，用时15秒。并且各Task所处理的数据集大小相当。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/customizedpartition.png" alt="customizedpartition"></p>
<h4 id="2-4-3-总结"><a href="#2-4-3-总结" class="headerlink" title="2.4.3 总结"></a>2.4.3 总结</h4><p><strong>适用场景</strong><br>大量不同的Key被分配到了相同的Task造成该Task数据量过大。</p>
<p><strong>解决方案</strong><br>使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。</p>
<p><strong>优势</strong><br>不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。</p>
<p><strong>劣势</strong><br>适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。</p>
<h3 id="2-5-将Reduce-side-Join转变为Map-side-Join"><a href="#2-5-将Reduce-side-Join转变为Map-side-Join" class="headerlink" title="2.5 将Reduce side Join转变为Map side Join"></a>2.5 将Reduce side Join转变为Map side Join</h3><h4 id="2-5-1-原理"><a href="#2-5-1-原理" class="headerlink" title="2.5.1 原理"></a>2.5.1 原理</h4><p>通过Spark的Broadcast机制，将Reduce侧Join转化为Map侧Join，避免Shuffle从而完全消除Shuffle带来的数据倾斜。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/mapjoin.png" alt="mapjoin"></p>
<h4 id="2-5-2-案例"><a href="#2-5-2-案例" class="headerlink" title="2.5.2 案例"></a>2.5.2 案例</h4><p>通过如下SQL创建一张具有倾斜Key且总记录数为1.5亿的大表test。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> <span class="keyword">test</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">CAST</span>(<span class="keyword">CASE</span> <span class="keyword">WHEN</span> <span class="keyword">id</span> &lt; <span class="number">980000000</span> <span class="keyword">THEN</span> (<span class="number">95000000</span>  + (<span class="keyword">CAST</span> (<span class="keyword">RAND</span>() * <span class="number">4</span> <span class="keyword">AS</span> <span class="built_in">INT</span>) + <span class="number">1</span>) * <span class="number">48</span> )</span><br><span class="line">       <span class="keyword">ELSE</span> <span class="keyword">CAST</span>(<span class="keyword">id</span>/<span class="number">10</span> <span class="keyword">AS</span> <span class="built_in">INT</span>) <span class="keyword">END</span> <span class="keyword">AS</span> <span class="keyword">STRING</span>),</span><br><span class="line">       <span class="keyword">name</span></span><br><span class="line"><span class="keyword">FROM</span> student_external</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">id</span> <span class="keyword">BETWEEN</span> <span class="number">900000000</span> <span class="keyword">AND</span> <span class="number">1050000000</span>;</span><br></pre></td></tr></table></figure>

<p>使用如下SQL创建一张数据分布均匀且总记录数为50万的小表test_new。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test_new</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">CAST</span>(<span class="keyword">CAST</span>(<span class="keyword">id</span>/<span class="number">10</span> <span class="keyword">AS</span> <span class="built_in">INT</span>) <span class="keyword">AS</span> <span class="keyword">STRING</span>),</span><br><span class="line">       <span class="keyword">name</span></span><br><span class="line"><span class="keyword">FROM</span> student_delta_external</span><br><span class="line"><span class="keyword">WHERE</span> <span class="keyword">id</span> <span class="keyword">BETWEEN</span> <span class="number">950000000</span> <span class="keyword">AND</span> <span class="number">950500000</span>;</span><br></pre></td></tr></table></figure>

<p>直接通过Spark Thrift Server提交如下SQL将表test与表test_new进行Join并将Join结果存于表test_join中。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> test_join</span><br><span class="line"><span class="keyword">SELECT</span> test_new.id, test_new.name</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">test</span></span><br><span class="line"><span class="keyword">JOIN</span> test_new</span><br><span class="line"><span class="keyword">ON</span> test.id = test_new.id;</span><br></pre></td></tr></table></figure>

<p>该SQL对应的DAG如下图所示。从该图可见，该执行过程总共分为三个Stage，前两个用于从Hive中读取数据，同时二者进行Shuffle，通过最后一个Stage进行Join并将结果写入表test_join中。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/reducejoindag.png" alt="reducejoindag"></p>
<p>从下图可见，Join Stage各Task处理的数据倾斜严重，处理数据量最大的Task耗时7.1分钟，远高于其它无数据倾斜的Task约2秒的耗时。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/reducejoinlaststage.png" alt="reducejoinlaststage"></p>
<p>接下来，尝试通过Broadcast实现Map侧Join。实现Map侧Join的方法，并非直接通过<code>CACHE TABLE test_new</code>将小表test_new进行cache。现通过如下SQL进行Join。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CACHE TABLE test_new;</span><br><span class="line">INSERT OVERWRITE TABLE test_join</span><br><span class="line">SELECT test_new.id, test_new.name</span><br><span class="line">FROM test</span><br><span class="line">JOIN test_new</span><br><span class="line">ON test.id &#x3D; test_new.id;</span><br></pre></td></tr></table></figure>

<p>通过如下DAG图可见，该操作仍分为三个Stage，且仍然有Shuffle存在，唯一不同的是，小表的读取不再直接扫描Hive表，而是扫描内存中缓存的表。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/reducejoincachedag.png" alt="reducejoincachedag"></p>
<p>并且数据倾斜仍然存在。如下图所示，最慢的Task耗时为7.1分钟，远高于其它Task的约2秒。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/reducejoincachelaststage.png" alt="reducejoincachelaststage"></p>
<p>正确的使用Broadcast实现Map侧Join的方式是，通过<code>SET spark.sql.autoBroadcastJoinThreshold=104857600;</code>将Broadcast的阈值设置得足够大。</p>
<p>再次通过如下SQL进行Join。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SET spark.sql.autoBroadcastJoinThreshold&#x3D;104857600;</span><br><span class="line">INSERT OVERWRITE TABLE test_join</span><br><span class="line">SELECT test_new.id, test_new.name</span><br><span class="line">FROM test</span><br><span class="line">JOIN test_new</span><br><span class="line">ON test.id &#x3D; test_new.id;</span><br></pre></td></tr></table></figure>

<p>通过如下DAG图可见，该方案只包含一个Stage。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/mapjoindag.png" alt="mapjoindag"></p>
<p>并且从下图可见，各Task耗时相当，无明显数据倾斜现象。并且总耗时为1.5分钟，远低于Reduce侧Join的7.3分钟。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/mapjoinlaststage.png" alt="mapjoinlaststage"></p>
<h4 id="2-5-3-总结"><a href="#2-5-3-总结" class="headerlink" title="2.5.3 总结"></a>2.5.3 总结</h4><p><strong>适用场景</strong><br>参与Join的一边数据集足够小，可被加载进Driver并通过Broadcast方法广播到各个Executor中。</p>
<p><strong>解决方案</strong><br>在Java/Scala代码中将小数据集数据拉取到Driver，然后通过Broadcast方案将小数据集的数据广播到各Executor。或者在使用SQL前，将Broadcast的阈值调整得足够大，从而使用Broadcast生效。进而将Reduce侧Join替换为Map侧Join。</p>
<p><strong>优势</strong><br>避免了Shuffle，彻底消除了数据倾斜产生的条件，可极大提升性能。</p>
<p><strong>劣势</strong><br>要求参与Join的一侧数据集足够小，并且主要适用于Join的场景，不适合聚合的场景，适用条件有限。</p>
<h3 id="2-6-为skew的key增加随机前-后缀"><a href="#2-6-为skew的key增加随机前-后缀" class="headerlink" title="2.6 为skew的key增加随机前/后缀"></a>2.6 为skew的key增加随机前/后缀</h3><h4 id="2-6-1-原理"><a href="#2-6-1-原理" class="headerlink" title="2.6.1 原理"></a>2.6.1 原理</h4><p>为数据量特别大的Key增加随机前/后缀，使得原来Key相同的数据变为Key不相同的数据，从而使倾斜的数据集分散到不同的Task中，彻底解决数据倾斜问题。Join另一则的数据中，与倾斜Key对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜Key如何加前缀，都能与之正常Join。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/randomprefix.png" alt="randomprefix"></p>
<h4 id="2-6-2-案例"><a href="#2-6-2-案例" class="headerlink" title="2.6.2 案例"></a>2.6.2 案例</h4><p>通过如下SQL，将id为9亿到9.08亿共800万条数据的id转为9500048或者9500096，其它数据的id除以100取整。从而该数据集中，id为9500048和9500096的数据各400万，其它id对应的数据记录数均为100条。这些数据存于名为test的表中。</p>
<p>对于另外一张小表test_new，取出50万条数据，并将id（递增且唯一）除以100取整，使得所有id都对应100条数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE test</span><br><span class="line">SELECT CAST(CASE WHEN id &lt; 908000000 THEN (9500000  + (CAST (RAND() * 2 AS INT) + 1) * 48 )</span><br><span class="line">  ELSE CAST(id&#x2F;100 AS INT) END AS STRING),</span><br><span class="line">  name</span><br><span class="line">FROM student_external</span><br><span class="line">WHERE id BETWEEN 900000000 AND 1050000000;</span><br><span class="line"></span><br><span class="line">INSERT OVERWRITE TABLE test_new</span><br><span class="line">SELECT CAST(CAST(id&#x2F;100 AS INT) AS STRING),</span><br><span class="line">  name</span><br><span class="line">FROM student_delta_external</span><br><span class="line">WHERE id BETWEEN 950000000 AND 950500000;</span><br></pre></td></tr></table></figure>

<p>通过如下代码，读取test表对应的文件夹内的数据并转换为JavaPairRDD存于leftRDD中，同样读取test表对应的数据存于rightRDD中。通过RDD的join算子对leftRDD与rightRDD进行Join，并指定并行度为48。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">public class SparkDataSkew&#123;</span><br><span class="line">  public static void main(String[] args) &#123;</span><br><span class="line">    SparkConf sparkConf &#x3D; new SparkConf();</span><br><span class="line">    sparkConf.setAppName(&quot;DemoSparkDataFrameWithSkewedBigTableDirect&quot;);</span><br><span class="line">    sparkConf.set(&quot;spark.default.parallelism&quot;, String.valueOf(parallelism));</span><br><span class="line">    JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; leftRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">        return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; rightRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test_new&#x2F;&quot;)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">          return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    leftRDD.join(rightRDD, parallelism)</span><br><span class="line">      .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()))</span><br><span class="line">      .foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">        AtomicInteger atomicInteger &#x3D; new AtomicInteger();</span><br><span class="line">          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    javaSparkContext.stop();</span><br><span class="line">    javaSparkContext.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从下图可看出，整个Join耗时1分54秒，其中Join Stage耗时1.7分钟。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/fewskewkeyjoinallstage.png" alt="fewskewkeyjoinallstage"></p>
<p>通过分析Join Stage的所有Task可知，在其它Task所处理记录数为192.71万的同时Task 32的处理的记录数为992.72万，故它耗时为1.7分钟，远高于其它Task的约10秒。这与上文准备数据集时，将id为9500048为9500096对应的数据量设置非常大，其它id对应的数据集非常均匀相符合。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/fewskewkeyjoinlaststage.png" alt="fewskewkeyjoinlaststage"></p>
<p>现通过如下操作，实现倾斜Key的分散处理</p>
<ul>
<li>将leftRDD中倾斜的key（即9500048与9500096）对应的数据单独过滤出来，且加上1到24的随机前缀，并将前缀与原数据用逗号分隔（以方便之后去掉前缀）形成单独的leftSkewRDD</li>
<li>将rightRDD中倾斜key对应的数据抽取出来，并通过flatMap操作将该数据集中每条数据均转换为24条数据（每条分别加上1到24的随机前缀），形成单独的rightSkewRDD</li>
<li>将leftSkewRDD与rightSkewRDD进行Join，并将并行度设置为48，且在Join过程中将随机前缀去掉，得到倾斜数据集的Join结果skewedJoinRDD</li>
<li>将leftRDD中不包含倾斜Key的数据抽取出来作为单独的leftUnSkewRDD</li>
<li>对leftUnSkewRDD与原始的rightRDD进行Join，并行度也设置为48，得到Join结果unskewedJoinRDD</li>
<li>通过union算子将skewedJoinRDD与unskewedJoinRDD进行合并，从而得到完整的Join结果集</li>
</ul>
<p>具体实现代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">public class SparkDataSkew&#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">      int parallelism &#x3D; 48;</span><br><span class="line">      SparkConf sparkConf &#x3D; new SparkConf();</span><br><span class="line">      sparkConf.setAppName(&quot;SolveDataSkewWithRandomPrefix&quot;);</span><br><span class="line">      sparkConf.set(&quot;spark.default.parallelism&quot;, parallelism + &quot;&quot;);</span><br><span class="line">      JavaSparkContext javaSparkContext &#x3D; new JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">      JavaPairRDD&lt;String, String&gt; leftRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test&#x2F;&quot;)</span><br><span class="line">        .mapToPair((String row) -&gt; &#123;</span><br><span class="line">          String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">            return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; rightRDD &#x3D; javaSparkContext.textFile(&quot;hdfs:&#x2F;&#x2F;hadoop1:8020&#x2F;apps&#x2F;hive&#x2F;warehouse&#x2F;default&#x2F;test_new&#x2F;&quot;)</span><br><span class="line">          .mapToPair((String row) -&gt; &#123;</span><br><span class="line">            String[] str &#x3D; row.split(&quot;,&quot;);</span><br><span class="line">              return new Tuple2&lt;String, String&gt;(str[0], str[1]);</span><br><span class="line">          &#125;);</span><br><span class="line"></span><br><span class="line">        String[] skewedKeyArray &#x3D; new String[]&#123;&quot;9500048&quot;, &quot;9500096&quot;&#125;;</span><br><span class="line">        Set&lt;String&gt; skewedKeySet &#x3D; new HashSet&lt;String&gt;();</span><br><span class="line">        List&lt;String&gt; addList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">        for(int i &#x3D; 1; i &lt;&#x3D;24; i++) &#123;</span><br><span class="line">            addList.add(i + &quot;&quot;);</span><br><span class="line">        &#125;</span><br><span class="line">        for(String key : skewedKeyArray) &#123;</span><br><span class="line">            skewedKeySet.add(key);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Broadcast&lt;Set&lt;String&gt;&gt; skewedKeys &#x3D; javaSparkContext.broadcast(skewedKeySet);</span><br><span class="line">        Broadcast&lt;List&lt;String&gt;&gt; addListKeys &#x3D; javaSparkContext.broadcast(addList);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; leftSkewRDD &#x3D; leftRDD</span><br><span class="line">          .filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))</span><br><span class="line">          .mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;((new Random().nextInt(24) + 1) + &quot;,&quot; + tuple._1(), tuple._2()));</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; rightSkewRDD &#x3D; rightRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; skewedKeys.value().contains(tuple._1()))</span><br><span class="line">          .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream()</span><br><span class="line">          .map((String i) -&gt; new Tuple2&lt;String, String&gt;( i + &quot;,&quot; + tuple._1(), tuple._2()))</span><br><span class="line">          .collect(Collectors.toList())</span><br><span class="line">          .iterator()</span><br><span class="line">        );</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; skewedJoinRDD &#x3D; leftSkewRDD</span><br><span class="line">          .join(rightSkewRDD, parallelism)</span><br><span class="line">          .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1().split(&quot;,&quot;)[1], tuple._2()._2()));</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, String&gt; leftUnSkewRDD &#x3D; leftRDD.filter((Tuple2&lt;String, String&gt; tuple) -&gt; !skewedKeys.value().contains(tuple._1()));</span><br><span class="line">        JavaPairRDD&lt;String, String&gt; unskewedJoinRDD &#x3D; leftUnSkewRDD.join(rightRDD, parallelism).mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; new Tuple2&lt;String, String&gt;(tuple._1(), tuple._2()._2()));</span><br><span class="line"></span><br><span class="line">        skewedJoinRDD.union(unskewedJoinRDD).foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">          AtomicInteger atomicInteger &#x3D; new AtomicInteger();</span><br><span class="line">          iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        javaSparkContext.stop();</span><br><span class="line">        javaSparkContext.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从下图可看出，整个Join耗时58秒，其中Join Stage耗时33秒。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/fewskewkeyrandomjoinallstage.png" alt="fewskewkeyrandomjoinallstage"></p>
<p>通过分析Join Stage的所有Task可知</p>
<ul>
<li>由于Join分倾斜数据集Join和非倾斜数据集Join，而各Join的并行度均为48，故总的并行度为96</li>
<li>由于提交任务时，设置的Executor个数为4，每个Executor的core数为12，故可用Core数为48，所以前48个Task同时启动（其Launch时间相同），后48个Task的启动时间各不相同（等待前面的Task结束才开始）</li>
<li>由于倾斜Key被加上随机前缀，原本相同的Key变为不同的Key，被分散到不同的Task处理，故在所有Task中，未发现所处理数据集明显高于其它Task的情况</li>
</ul>
<p><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/fewskewkeyjoinrandomlaststage.png" alt="fewskewkeyjoinrandomlaststage"></p>
<p>实际上，由于倾斜Key与非倾斜Key的操作完全独立，可并行进行。而本实验受限于可用总核数为48，可同时运行的总Task数为48，故而该方案只是将总耗时减少一半（效率提升一倍）。如果资源充足，可并发执行Task数增多，该方案的优势将更为明显。在实际项目中，该方案往往可提升数倍至10倍的效率。</p>
<h4 id="2-6-3-总结"><a href="#2-6-3-总结" class="headerlink" title="2.6.3 总结"></a>2.6.3 总结</h4><p><strong>适用场景</strong><br>两张表都比较大，无法使用Map则Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。</p>
<p><strong>解决方案</strong><br>将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p>
<p><strong>优势</strong><br>相对于Map则Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p>
<p><strong>劣势</strong><br>如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。</p>
<h3 id="2-7-大表随机添加N种随机前缀，小表扩大N倍"><a href="#2-7-大表随机添加N种随机前缀，小表扩大N倍" class="headerlink" title="2.7 大表随机添加N种随机前缀，小表扩大N倍"></a>2.7 大表随机添加N种随机前缀，小表扩大N倍</h3><h4 id="2-7-1-原理"><a href="#2-7-1-原理" class="headerlink" title="2.7.1 原理"></a>2.7.1 原理</h4><p>如果出现数据倾斜的Key比较多，上一种方法将这些大量的倾斜Key分拆出来，意义不大。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大N倍）。<br><img src="/images/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/randomprefixandenlargesmalltable.png" alt="randomprefixandenlargesmalltable"></p>
<h4 id="2-7-2案例"><a href="#2-7-2案例" class="headerlink" title="2.7.2案例"></a>2.7.2案例</h4><p>这里给出示例代码，读者可参考上文中分拆出少数倾斜Key添加随机前缀的方法，自行测试。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkDataSkew</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    SparkConf sparkConf = <span class="keyword">new</span> SparkConf();</span><br><span class="line">    sparkConf.setAppName(<span class="string">"ResolveDataSkewWithNAndRandom"</span>);</span><br><span class="line">    sparkConf.set(<span class="string">"spark.default.parallelism"</span>, parallelism + <span class="string">""</span>);</span><br><span class="line">    JavaSparkContext javaSparkContext = <span class="keyword">new</span> JavaSparkContext(sparkConf);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; leftRDD = javaSparkContext.textFile(<span class="string">"hdfs://hadoop1:8020/apps/hive/warehouse/default/test/"</span>)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str = row.split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(str[<span class="number">0</span>], str[<span class="number">1</span>]);</span><br><span class="line">      &#125;);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; rightRDD = javaSparkContext.textFile(<span class="string">"hdfs://hadoop1:8020/apps/hive/warehouse/default/test_new/"</span>)</span><br><span class="line">      .mapToPair((String row) -&gt; &#123;</span><br><span class="line">        String[] str = row.split(<span class="string">","</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;String, String&gt;(str[<span class="number">0</span>], str[<span class="number">1</span>]);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    List&lt;String&gt; addList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;=<span class="number">48</span>; i++) &#123;</span><br><span class="line">      addList.add(i + <span class="string">""</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Broadcast&lt;List&lt;String&gt;&gt; addListKeys = javaSparkContext.broadcast(addList);</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; leftRandomRDD = leftRDD.mapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; <span class="keyword">new</span> Tuple2&lt;String, String&gt;(<span class="keyword">new</span> Random().nextInt(<span class="number">48</span>) + <span class="string">","</span> + tuple._1(), tuple._2()));</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; rightNewRDD = rightRDD</span><br><span class="line">      .flatMapToPair((Tuple2&lt;String, String&gt; tuple) -&gt; addListKeys.value().stream()</span><br><span class="line">      .map((String i) -&gt; <span class="keyword">new</span> Tuple2&lt;String, String&gt;( i + <span class="string">","</span> + tuple._1(), tuple._2()))</span><br><span class="line">      .collect(Collectors.toList())</span><br><span class="line">      .iterator()</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    JavaPairRDD&lt;String, String&gt; joinRDD = leftRandomRDD</span><br><span class="line">      .join(rightNewRDD, parallelism)</span><br><span class="line">      .mapToPair((Tuple2&lt;String, Tuple2&lt;String, String&gt;&gt; tuple) -&gt; <span class="keyword">new</span> Tuple2&lt;String, String&gt;(tuple._1().split(<span class="string">","</span>)[<span class="number">1</span>], tuple._2()._2()));</span><br><span class="line"></span><br><span class="line">    joinRDD.foreachPartition((Iterator&lt;Tuple2&lt;String, String&gt;&gt; iterator) -&gt; &#123;</span><br><span class="line">      AtomicInteger atomicInteger = <span class="keyword">new</span> AtomicInteger();</span><br><span class="line">      iterator.forEachRemaining((Tuple2&lt;String, String&gt; tuple) -&gt; atomicInteger.incrementAndGet());</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    javaSparkContext.stop();</span><br><span class="line">    javaSparkContext.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-7-3总结"><a href="#2-7-3总结" class="headerlink" title="2.7.3总结"></a>2.7.3总结</h4><p><strong>适用场景</strong><br>一个数据集存在的倾斜Key比较多，另外一个数据集数据分布比较均匀。</p>
<p><strong>优势</strong><br>对大部分场景都适用，效果不错。</p>
<p><strong>劣势</strong><br>需要将一个数据集整体扩大N倍，会增加资源消耗。</p>
<h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><p>对于数据倾斜，并无一个统一的一劳永逸的方法。更多的时候，是结合数据特点（数据集大小，倾斜Key的多少等）综合使用上文所述的多种方法。</p>
<h2 id="四、来源"><a href="#四、来源" class="headerlink" title="四、来源"></a>四、来源</h2><p><a href="http://www.jasongj.com/spark/skew/" target="_blank" rel="noopener">Spark性能优化之道——解决Spark数据倾斜（Data Skew）的N种姿势</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://longzl2015.github.io/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/%E9%97%AD%E5%8C%85%E4%B8%8E%E5%88%86%E5%8F%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhoul">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/%E9%97%AD%E5%8C%85%E4%B8%8E%E5%88%86%E5%8F%91/" class="post-title-link" itemprop="url">闭包分发</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2017-06-04 23:22:58" itemprop="dateCreated datePublished" datetime="2017-06-04T23:22:58+08:00">2017-06-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-27 17:16:07" itemprop="dateModified" datetime="2020-02-27T17:16:07+08:00">2020-02-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/" itemprop="url" rel="index">
                    <span itemprop="name">数据处理实践</span>
                  </a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus：</span>
    
    <a title="disqus" href="/2017/06/04/spark/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/%E9%97%AD%E5%8C%85%E4%B8%8E%E5%88%86%E5%8F%91/#comments" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/06/04/spark/数据处理实践/闭包与分发/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/27/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><span class="page-number current">28</span><a class="page-number" href="/page/29/">29</a><span class="space">&hellip;</span><a class="page-number" href="/page/36/">36</a><a class="extend next" rel="next" href="/page/29/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhoul</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">351</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">92</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">190</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/longzl2015" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;longzl2015" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:289570126@qq.com" title="E-Mail → mailto:289570126@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/5276366/egg" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;5276366&#x2F;egg" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhoul</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.7.0
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://long12356-gitee-io.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>

</body>
</html>
